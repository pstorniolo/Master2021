{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-11-02-Spark_MLlib.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMm/IQ8In9Bg0UPaaBrvzCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_11_02_Spark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcadi1WFa3CI"
      },
      "source": [
        "#MLlib\n",
        "\n",
        "**MLlib** è la libreria di machine learning (ML) di Spark. Il suo obiettivo è rendere l'apprendimento automatico pratico scalabile e facile. Fornisce strumenti ad alto livello come:\n",
        "\n",
        "*   Algoritmi ML: algoritmi di apprendimento comuni come *classification, regression, clustering, and collaborative filtering*;\n",
        "*   Pipeline: strumenti per la costruzione, la valutazione e l'ottimizzazione delle pipeline ML;\n",
        "*   Caratterizzazione: *feature extraction, transformation, dimensionality reduction, and selection*;\n",
        "*   Persistenza: salvataggio e caricamento di algoritmi, modelli e pipeline\n",
        "*   Utility: algebra lineare, statistica, gestione dati, ecc.\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zlGnx1Qaclc"
      },
      "source": [
        "!curl ipecho.net/plain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHq0fu7_YMeY"
      },
      "source": [
        "# Install Spark 3.2.0 - JDK11\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!curl -O https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "#!curl -O http://www.pa.icar.cnr.it/storniolo/files/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f *.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "!pip -q install findspark\n",
        "#!pip install -q dnspython\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "#from pymongo import MongoClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A4JEEB9Z3lz"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(spark.version)\n",
        "#spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP2Ma15fyOl-"
      },
      "source": [
        "Creo un collegamento con spark per semplificare il percorso dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_SdlfG4yCwP"
      },
      "source": [
        "!ln -s spark-3.2.0-bin-hadoop3.2 spark\n",
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ie2TMGbyhiQ"
      },
      "source": [
        "#Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TaAQLC4y0QY"
      },
      "source": [
        "##Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEbP_dIbccHj"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\").load(\"spark/data/mllib/sample_libsvm_data.txt\")\n",
        "training.printSchema()\n",
        "training.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrQV6yfBzgqB"
      },
      "source": [
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64lOc8j3zkbP"
      },
      "source": [
        "# Fit the model\n",
        "lrModel = lr.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wykHwoGFzrN0"
      },
      "source": [
        "# Print the coefficients and intercept for logistic regression\n",
        "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
        "print(\"Intercept: \" + str(lrModel.intercept))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfU3Y4mH05XC"
      },
      "source": [
        "##Multinomial logistic regression\n",
        "\n",
        "La *Multiclass classification* è supportata tramite regressione logistica multinomiale (softmax). Nella regressione logistica multinomiale, l'algoritmo produce ***K*** insiemi di coefficienti o una matrice di dimensione ***K×J*** dove ***K*** è il numero di classi di risultati e ***J*** è il numero di caratteristiche."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgaxEezGzuGq"
      },
      "source": [
        "# We can also use the multinomial family for binary classification\n",
        "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuxrwY0_zyRb"
      },
      "source": [
        "# Fit the model\n",
        "mlrModel = mlr.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVwTPmbqz2NV"
      },
      "source": [
        "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
        "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
        "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mkR9tm_4d2x"
      },
      "source": [
        "##Multilayer perceptron classifier\n",
        "\n",
        "Il MultiLayer Perceptron Classifier (MLPC) è un classificatore basato sulla rete neurale artificiale *feedforward*. MLPC è costituito da più livelli di nodi. Ogni livello è completamente connesso al livello successivo nella rete. I nodi nel livello di input rappresentano i dati di input. Tutti gli altri nodi mappano gli input agli output mediante una combinazione lineare degli input con i pesi ***w*** e bias ***b*** del nodo e applicando una **funzione di attivazione**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC-3TMI05DnM"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load training data\n",
        "data = spark.read.format(\"libsvm\").load(\"spark/data/mllib/sample_multiclass_classification_data.txt\")\n",
        "\n",
        "data.printSchema()\n",
        "data.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LabOggsn50-E"
      },
      "source": [
        "# Split the data into train and test\n",
        "splits = data.randomSplit([0.6, 0.4], 1234)\n",
        "train = splits[0]\n",
        "test = splits[1]\n",
        "print(data.count(),train.count(),test.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-po9zMO55-Z"
      },
      "source": [
        "# specify layers for the neural network:\n",
        "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
        "# and output of size 3 (classes)\n",
        "layers = [4, 6, 5, 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYmgCKUs6QLr"
      },
      "source": [
        "# create the trainer and set its parameters\n",
        "trainer = MultilayerPerceptronClassifier(maxIter=150, layers=layers, blockSize=20, seed=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzXjqmwq6XS8"
      },
      "source": [
        "# train the model\n",
        "model = trainer.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKciTcE06cKh"
      },
      "source": [
        "# compute accuracy on the test set\n",
        "result = model.transform(test)\n",
        "result.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdXpfLyX6hDG"
      },
      "source": [
        "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-VMPZrmZyad"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(metricName=\"hammingLoss\")\n",
        "print(\"Test set hammingLoss = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWG-MQiL63IQ"
      },
      "source": [
        "##Linear Support Vector Machine\n",
        "\n",
        "Una *support vector machine* costruisce un iperpiano o un insieme di iperpiani in uno spazio ad alta o infinita dimensione, che può essere utilizzato per la classificazione, la regressione o altre attività. Intuitivamente, una buona separazione è ottenuta dall'iperpiano che ha la distanza maggiore dai punti dati di addestramento più vicini di qualsiasi classe (cosiddetto margine funzionale), poiché in generale maggiore è il margine, minore è l'errore di generalizzazione del classificatore. **LinearSVC** in Spark ML supporta la classificazione binaria con SVM lineare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Dy8sNq7pHo"
      },
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\").load(\"spark/data/mllib/sample_libsvm_data.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKmWGdkf72os"
      },
      "source": [
        "lsvc = LinearSVC(maxIter=100, regParam=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy57P99h771y"
      },
      "source": [
        "# Fit the model\n",
        "lsvcModel = lsvc.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da9X-2gT7-y3"
      },
      "source": [
        "# Print the coefficients and intercept for linear SVC\n",
        "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
        "print(\"Intercept: \" + str(lsvcModel.intercept))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHIlZUZEz-vc"
      },
      "source": [
        "#!ls -la spark/examples/src/main/python/ml/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSR8YpE84G6I"
      },
      "source": [
        "#Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUz51l_c9lvd"
      },
      "source": [
        "##K-means\n",
        "\n",
        "**k-means** è uno degli algoritmi di clustering più comunemente usati che raggruppa i punti dati in un numero predefinito di cluster. L'implementazione di MLlib include una variante parallelizzata del metodo ***k-means++*** chiamato ***kmeans||***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w7ma8je-FGI"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Loads data.\n",
        "dataset = spark.read.format(\"libsvm\").load(\"spark/data/mllib/sample_kmeans_data.txt\")\n",
        "dataset.printSchema()\n",
        "dataset.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaDgX3Sh-PXi"
      },
      "source": [
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jSOYQkT-WAz"
      },
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(dataset)\n",
        "predictions.printSchema()\n",
        "predictions.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6HZVcpO-aKV"
      },
      "source": [
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJlOk4OS-eAU"
      },
      "source": [
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxtC5HBl_7Lj"
      },
      "source": [
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmzi7aLSeZH7"
      },
      "source": [
        "##RDD-based K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I8NXmCZehLz"
      },
      "source": [
        "from numpy import array\n",
        "from math import sqrt\n",
        "\n",
        "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
        "\n",
        "# Load and parse the data\n",
        "data = sc.textFile(\"spark/data/mllib/kmeans_data.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpCsmTTrdThH"
      },
      "source": [
        "!cat spark/data/mllib/kmeans_data.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhGJwf5Eeq5s"
      },
      "source": [
        "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmhjHpdkeqTc"
      },
      "source": [
        "# Build the model (cluster the data)\n",
        "clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZfrhTu2ezsT"
      },
      "source": [
        "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
        "def error(point):\n",
        "    center = clusters.centers[clusters.predict(point)]\n",
        "    return sqrt(sum([x**2 for x in (point - center)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPr8a27e8wI"
      },
      "source": [
        "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVnw0viBeNkS"
      },
      "source": [
        "!rm -rf KMeansModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D0o0C3ifD6r"
      },
      "source": [
        "# Save and load model\n",
        "clusters.save(sc, \"KMeansModel\")\n",
        "sameModel = KMeansModel.load(sc, \"KMeansModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_-SgJAsfMrp"
      },
      "source": [
        "!ls -la KMeansModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdXxyEVyASmO"
      },
      "source": [
        "#Frequent Pattern Mining\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UtXh9oOAyVh"
      },
      "source": [
        "##FP-Growth\n",
        "\n",
        "L'algoritmo **FP-growth** è descritto nel documento \"*Han et al., Mining frequent patterns without candidate generation*\", dove \"FP\" sta per Frequent Pattern. Dato un set di dati di transazioni, il primo passo della crescita del FP-growth è calcolare le frequenze degli articoli e identificare gli articoli frequenti.\n",
        "\n",
        "Diversamente dagli algoritmi *Apriori-like* progettati per lo stesso scopo, il secondo passaggio di FP-growth utilizza una struttura ad albero dei suffissi (FP-tree) per codificare le transazioni senza generare esplicitamente insiemi candidati, che di solito sono onerosi da generare. Dopo il secondo passaggio, gli insiemi di elementi frequenti possono essere estratti dall'albero FP.\n",
        "\n",
        "In spark.mllib, si è implementata una versione parallela di FP-growth chiamata PFP, come descritto in \"*Li et al., PFP: Parallel FP-growth for query recommendation*\". PFP distribuisce il lavoro di crescita di alberi FP in base ai suffissi delle transazioni e quindi è più scalabile di un'implementazione su una singola macchina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr9KXTagC7Ce"
      },
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (0, [1, 2, 5]),\n",
        "    (1, [1, 2, 3, 5]),\n",
        "    (2, [1, 2])\n",
        "], [\"id\", \"items\"])\n",
        "\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnqbPp3SAr8Z"
      },
      "source": [
        "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\n",
        "model = fpGrowth.fit(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj0lLXb0FCN_"
      },
      "source": [
        "# Display frequent itemsets.\n",
        "model.freqItemsets.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpAKxpUiFcre"
      },
      "source": [
        "# transform examines the input items against all the association rules and summarize the\n",
        "# consequents as prediction\n",
        "model.transform(df).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT1kJgdHFXJB"
      },
      "source": [
        "# Display generated association rules.\n",
        "model.associationRules.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGYO8uzscFaR"
      },
      "source": [
        "##RDD-based FP-growth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnm-b5sNar_n"
      },
      "source": [
        "from pyspark.mllib.fpm import FPGrowth\n",
        "\n",
        "data = sc.textFile(\"spark/data/mllib/sample_fpgrowth.txt\")\n",
        "print(data.collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLozaiZfcaci"
      },
      "source": [
        "transactions = data.map(lambda line: line.strip().split(' '))\n",
        "\n",
        "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
        "\n",
        "result = model.freqItemsets().collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of4-P-IMchYw"
      },
      "source": [
        "for fi in result:\n",
        "    print(fi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMc14oa5GoXW"
      },
      "source": [
        "#ML Tuning (model selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7NHL5_9HOSV"
      },
      "source": [
        "##Cross-Validation\n",
        "\n",
        "**CrossValidator** inizia suddividendo il set di dati in un insieme di *fold* che vengono utilizzate come set di dati di addestramento e test separati. Ad esempio, con ***k=3*** fold, CrossValidator genererà 3 coppie di set di dati (addestramento, test), ognuna delle quali utilizza 2/3 dei dati per l'addestramento e 1/3 per il test. Per valutare un particolare *ParamMap*, *CrossValidator* calcola la metrica di valutazione media per i 3 modelli prodotti adattando l'estimatore alle 3 diverse coppie di set di dati (addestramento, test).\n",
        "Dopo aver identificato la migliore *ParamMap*, *CrossValidator* infine riadatta l'estimatore utilizzando la migliore *ParamMap* e l'intero set di dati."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgcYasBcICsT"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Prepare training documents, which are labeled.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 2.0),\n",
        "    (4, \"b spark who\", 1.0),\n",
        "    (5, \"g d a y\", 0.0),\n",
        "    (6, \"spark fly\", 1.0),\n",
        "    (7, \"was mapreduce\", 0.0),\n",
        "    (8, \"e spark program\", 1.0),\n",
        "    (9, \"a e c l\", 0.0),\n",
        "    (10, \"spark compile\", 1.0),\n",
        "    (11, \"hadoop software\", 2.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "training.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e933kLjvIN1g"
      },
      "source": [
        "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSzu5QmdIwym"
      },
      "source": [
        "Ora usiamo *Pipeline* come un *Estimator*, facendone il *wrapping* in un'istanza *CrossValidator*. Questo ci consentirà di scegliere congiuntamente i parametri per tutte le fasi della pipeline. Un CrossValidator richiede un Estimator, un set di Estimator ParamMaps e un Evaluator. Usiamo il metodo *ParamGridBuilder* per costruire una griglia di parametri su cui cercare. Con 3 valori per *hashingTF.numFeatures* e 2 valori per *lr.regParam*, questa griglia avrà 3 x 2 = 6 impostazioni dei parametri tra cui *CrossValidator* potrà scegliere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owx4abzBITR4"
      },
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .build()\n",
        "\n",
        "for i in paramGrid:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSNGO60jIcwJ"
      },
      "source": [
        "#evaluator = BinaryClassificationEvaluator()\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=2)\n",
        "\n",
        "# Run cross-validation, and choose the best set of parameters.\n",
        "cvModel = crossval.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "361s-mLGKB9Q"
      },
      "source": [
        "# Prepare test documents, which are unlabeled.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"mapreduce spark\"),\n",
        "    (7, \"ape hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "test.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em7nf1uZKOvj"
      },
      "source": [
        "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
        "prediction = cvModel.transform(test)\n",
        "prediction.printSchema()\n",
        "\n",
        "prediction.select(\"id\", \"text\", \"probability\", \"prediction\").show(truncate=False)\n",
        "\n",
        "training.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsVPdkcqVlSN"
      },
      "source": [
        "##Train-Validation Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeUZFB1MVtIB"
      },
      "source": [
        "Oltre a *CrossValidator* Spark offre anche *TrainValidationSplit* per l'ottimizzazione degli iperparametri. *TrainValidationSplit* valuta ogni combinazione di parametri solo una volta, invece di k volte come nel caso di CrossValidator. È, quindi, meno oneroso, ma non produrrà risultati altrettanto affidabili quando il set di dati di addestramento non è sufficientemente grande.\n",
        "\n",
        "A differenza di *CrossValidator*, *TrainValidationSplit* crea una singola coppia di set di dati (addestramento, test). Divide il set di dati in queste due parti utilizzando il parametro *trainRatio*. Ad esempio con *trainRatio=0.75*, *TrainValidationSplit* genererà una coppia di set di dati di addestramento e test in cui il 75% dei dati viene utilizzato per l'addestramento e il 25% per la convalida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd7Hmt0pWPP7"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "# Prepare training and test data.\n",
        "data = spark.read.format(\"libsvm\").load(\"spark/data/mllib/sample_linear_regression_data.txt\")\n",
        "data.printSchema()\n",
        "print(data.count())\n",
        "data.show(5,truncate=False)\n",
        "\n",
        "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
        "print(train.count())\n",
        "train.show(2)\n",
        "print(test.count())\n",
        "test.show(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFKAwKZnW98F"
      },
      "source": [
        "lr = LinearRegression(maxIter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3VquzmXiou"
      },
      "source": [
        "paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
        "    .addGrid(lr.fitIntercept, [False, True])\\\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
        "    .build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DBtBflwXpf7"
      },
      "source": [
        "# In this case the estimator is simply the linear regression.\n",
        "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
        "tvs = TrainValidationSplit(estimator=lr,\n",
        "                           estimatorParamMaps=paramGrid,\n",
        "                           evaluator=RegressionEvaluator(),\n",
        "                           trainRatio=0.8) # 80% of the data will be used for training, 20% for validation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmndnuUbXucG"
      },
      "source": [
        "# Run TrainValidationSplit, and choose the best set of parameters.\n",
        "model = tvs.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGApRU7cXzHz"
      },
      "source": [
        "# Make predictions on test data. model is the model with combination of parameters\n",
        "# that performed best.\n",
        "result = model.transform(test)\n",
        "result.printSchema()\n",
        "result.select(\"features\", \"label\", \"prediction\").show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt62PtPyxWLx"
      },
      "source": [
        "#Stop Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvTL5-HLxyp9"
      },
      "source": [
        "#spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}