{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Install_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXT+RWMkEdVerLBb60Rthw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/03_Install_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w48z9Ca2RKhQ"
      },
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0BsDb12RXGT"
      },
      "source": [
        "#Install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBjM356_RdMT"
      },
      "source": [
        "#Unzip the spark file to the current folder\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr3VSxtf0oBg",
        "outputId": "71821e9f-3907-459d-9878-7dfafe48da37"
      },
      "source": [
        "!cd spark-3.1.2-bin-hadoop3.2; ls -la"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 164\n",
            "drwxr-xr-x 13 1000 1000  4096 May 24 04:45 .\n",
            "drwxr-xr-x  1 root root  4096 Oct 20 14:13 ..\n",
            "drwxr-xr-x  2 1000 1000  4096 May 24 04:45 bin\n",
            "drwxr-xr-x  2 1000 1000  4096 May 24 04:45 conf\n",
            "drwxr-xr-x  5 1000 1000  4096 May 24 04:45 data\n",
            "drwxr-xr-x  4 1000 1000  4096 May 24 04:45 examples\n",
            "drwxr-xr-x  2 1000 1000 20480 May 24 04:45 jars\n",
            "drwxr-xr-x  4 1000 1000  4096 May 24 04:45 kubernetes\n",
            "-rw-r--r--  1 1000 1000 23235 May 24 04:45 LICENSE\n",
            "drwxr-xr-x  2 1000 1000  4096 May 24 04:45 licenses\n",
            "-rw-r--r--  1 1000 1000 57677 May 24 04:45 NOTICE\n",
            "drwxr-xr-x  9 1000 1000  4096 May 24 04:45 python\n",
            "drwxr-xr-x  3 1000 1000  4096 May 24 04:45 R\n",
            "-rw-r--r--  1 1000 1000  4488 May 24 04:45 README.md\n",
            "-rw-r--r--  1 1000 1000   183 May 24 04:45 RELEASE\n",
            "drwxr-xr-x  2 1000 1000  4096 May 24 04:45 sbin\n",
            "drwxr-xr-x  2 1000 1000  4096 May 24 04:45 yarn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRB8s_BMENqq",
        "outputId": "963fc5cf-ed29-4f0d-f76f-063f92d39ee8"
      },
      "source": [
        "!cd spark-3.1.2-bin-hadoop3.2/bin; ls -la"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 124\n",
            "drwxr-xr-x  2 1000 1000  4096 May 24 04:45 .\n",
            "drwxr-xr-x 13 1000 1000  4096 May 24 04:45 ..\n",
            "-rwxr-xr-x  1 1000 1000  1089 May 24 04:45 beeline\n",
            "-rw-r--r--  1 1000 1000  1064 May 24 04:45 beeline.cmd\n",
            "-rwxr-xr-x  1 1000 1000 10965 May 24 04:45 docker-image-tool.sh\n",
            "-rwxr-xr-x  1 1000 1000  1935 May 24 04:45 find-spark-home\n",
            "-rw-r--r--  1 1000 1000  2685 May 24 04:45 find-spark-home.cmd\n",
            "-rw-r--r--  1 1000 1000  2337 May 24 04:45 load-spark-env.cmd\n",
            "-rw-r--r--  1 1000 1000  2435 May 24 04:45 load-spark-env.sh\n",
            "-rwxr-xr-x  1 1000 1000  2634 May 24 04:45 pyspark\n",
            "-rw-r--r--  1 1000 1000  1540 May 24 04:45 pyspark2.cmd\n",
            "-rw-r--r--  1 1000 1000  1170 May 24 04:45 pyspark.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1030 May 24 04:45 run-example\n",
            "-rw-r--r--  1 1000 1000  1223 May 24 04:45 run-example.cmd\n",
            "-rwxr-xr-x  1 1000 1000  3539 May 24 04:45 spark-class\n",
            "-rwxr-xr-x  1 1000 1000  2812 May 24 04:45 spark-class2.cmd\n",
            "-rw-r--r--  1 1000 1000  1180 May 24 04:45 spark-class.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1039 May 24 04:45 sparkR\n",
            "-rw-r--r--  1 1000 1000  1097 May 24 04:45 sparkR2.cmd\n",
            "-rw-r--r--  1 1000 1000  1168 May 24 04:45 sparkR.cmd\n",
            "-rwxr-xr-x  1 1000 1000  3122 May 24 04:45 spark-shell\n",
            "-rw-r--r--  1 1000 1000  1818 May 24 04:45 spark-shell2.cmd\n",
            "-rw-r--r--  1 1000 1000  1178 May 24 04:45 spark-shell.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1065 May 24 04:45 spark-sql\n",
            "-rw-r--r--  1 1000 1000  1118 May 24 04:45 spark-sql2.cmd\n",
            "-rw-r--r--  1 1000 1000  1173 May 24 04:45 spark-sql.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1040 May 24 04:45 spark-submit\n",
            "-rw-r--r--  1 1000 1000  1155 May 24 04:45 spark-submit2.cmd\n",
            "-rw-r--r--  1 1000 1000  1180 May 24 04:45 spark-submit.cmd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWT1FhhVRhJz"
      },
      "source": [
        "#Set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmpjec4lRpr0"
      },
      "source": [
        "#Install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWuXmFDqRwdU",
        "outputId": "7e316927-03a2-4fd0-c22e-afec6a3261d8"
      },
      "source": [
        "#Spark for Python (pyspark)\n",
        "!pip install -q pyspark==3.1.2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 69 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 23.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vUM_qYTR2vU"
      },
      "source": [
        "#importing pyspark\n",
        "import pyspark"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcR7zpmtR8nE"
      },
      "source": [
        "#importing sparksessio\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0iIxwXySB3c"
      },
      "source": [
        "#creating a sparksession object and providing appName \n",
        "spark=SparkSession.builder.appName(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dta5v8CSTZxv",
        "outputId": "a9bc28e6-eea0-49bb-9779-452be6954b4c"
      },
      "source": [
        "#printing the version of spark\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Spark version:  3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwgAP3VKT0tz",
        "outputId": "bf1c17a8-d953-4f95-cd49-8cdbc957b0c1"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 223492\n",
            "drwxr-xr-x  1 root root      4096 Oct 20 14:13 .\n",
            "drwxr-xr-x  1 root root      4096 Oct 20 14:12 ..\n",
            "drwxr-xr-x  4 root root      4096 Oct  8 13:44 .config\n",
            "drwxr-xr-x  1 root root      4096 Oct  8 13:45 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24 04:45 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24 05:01 spark-3.1.2-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pTsZKZhU3GU"
      },
      "source": [
        "from random import randint \n",
        "\n",
        "# create a list of random numbers between 10 to 1000\n",
        "my_large_list = [randint(10,1000) for x in range(0,20000000)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1-75OQgVPlI",
        "outputId": "1bd3a40e-f929-4ecd-f741-a02b8cadc74c"
      },
      "source": [
        "# create one partition of the list  \n",
        "my_large_list_one_partition = spark.sparkContext.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "%time\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.63 µs\n",
            "16166739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfhyUxLNXiAc",
        "outputId": "a6989f12-f7e8-4224-f367-4c43b032b552"
      },
      "source": [
        "# create five partitions of the list\n",
        "my_large_list_with_five_partition = spark.sparkContext.parallelize(my_large_list, numSlices=5)\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "%time \n",
        "\n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_five_partition.count())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.48 µs\n",
            "16166739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT9DbB28Ydyj",
        "outputId": "a444757e-9719-4db1-e7d5-a1fc12e8de5e"
      },
      "source": [
        "# create a sample list\n",
        "my_list = [i for i in range(1,10000000)]\n",
        "\n",
        "# parallelize the data\n",
        "rdd_0 = spark.sparkContext.parallelize(my_list,3)\n",
        "\n",
        "rdd_0\n",
        "\n",
        "# add value 4 to each number\n",
        "rdd_1 = rdd_0.map(lambda x : x+4)\n",
        "\n",
        "# RDD object\n",
        "print(rdd_1)\n",
        "\n",
        "# get the RDD Lineage \n",
        "print(rdd_1.toDebugString())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[5] at RDD at PythonRDD.scala:53\n",
            "b'(3) PythonRDD[5] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:274 []'\n"
          ]
        }
      ]
    }
  ]
}