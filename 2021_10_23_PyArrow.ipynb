{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-23-PyArrow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "a2S2Yvq8pb1q"
      ],
      "authorship_tag": "ABX9TyMEMJjqKSAkkDVZ86TWgkfW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_23_PyArrow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZPGsw6wLgGQ"
      },
      "source": [
        "#Apache Arrow in PySpark\n",
        "\n",
        "Apache Arrow è un formato di **dati a colonne in memoria** utilizzato in Spark per trasferire in modo efficiente i dati tra i processi *JVM* e *Python*. Questo è attualmente molto vantaggioso per gli utenti Python che lavorano con dati *Pandas/NumPy*. Il suo utilizzo non è automatico e potrebbe richiedere alcune modifiche minori alla configurazione o al codice per trarre il massimo vantaggio e garantire la compatibilità."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePujHliLIRyN"
      },
      "source": [
        "# Install Spark 3.2.0\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f *.tgz*\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark==3.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0W_Z-pPIaiY"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "from datetime import datetime, date\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "spark=SparkSession.builder.appName(\"Spark_Session\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-o56OhqMOMa"
      },
      "source": [
        "##Enabling for Conversion to/from Pandas\n",
        "\n",
        "Arrow è disponibile come ottimizzazione durante la conversione di Spark DataFrame in Pandas DataFrame usando la chiamata **DataFrame.toPandas()** e durante la creazione di Spark DataFrame da Pandas DataFrame con **SparkSession.createDataFrame()**. Per usare Arrow durante l'esecuzione di queste chiamate, si deve prima impostare la configurazione Spark *spark.sql.execution.arrow.pyspark.enabled* su *true*. Questa impostazione è disabilitata per default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekkYwCIwM0OQ"
      },
      "source": [
        "spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPQL_kabM9l_"
      },
      "source": [
        "L'utilizzo delle ottimizzazioni con Arrow produrrà gli stessi risultati di quando Arrow non è abilitato se non abilitato nell'ambiente Python. La conversione sarà meno efficiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvAnnkH1Yica"
      },
      "source": [
        "# Generate a Pandas DataFrame\n",
        "pdf = pd.DataFrame(np.random.rand(1000000, 10), columns=[\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"l\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRdtdCgoNPVZ"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
        "\n",
        "# Create a Spark DataFrame from a Pandas DataFrame\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Convert the Spark DataFrame back to a Pandas DataFrame\n",
        "result_pdf = df.select(\"*\").toPandas()\n",
        "result_pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-QnQl16YA7F"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
        "result_pdf = df.select(\"*\").toPandas()\n",
        "result_pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4WtS1Y2RGM8"
      },
      "source": [
        "##Pandas UDFs (a.k.a. Vectorized UDFs)\n",
        "\n",
        "Le UDF di Panda sono funzioni definite dall'utente che vengono eseguite da Spark utilizzando Arrow per trasferire dati e Panda per lavorare con i dati, il che consente operazioni vettorializzate. Una Pandas UDF viene definita utilizzando **pandas_udf()** come wrapper per la funzione e non è richiesta alcuna configurazione aggiuntiva. Una Pandas UDF si comporta come una normale API di PySpark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1M7m8ZOOYwS"
      },
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf(\"col1 string, col2 long\")\n",
        "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
        "    s3['col2'] = s1 + s2.str.len()\n",
        "    return s3\n",
        "\n",
        "# Create a Spark DataFrame that has three columns including a struct column.\n",
        "df = spark.createDataFrame(\n",
        "    [[1, \"a string\", (\"a nested string\",)]],\n",
        "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWtz2jLmt6oE"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkiEGEE1tj9J"
      },
      "source": [
        "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnJeSG0rSJkj"
      },
      "source": [
        "###Series to Series\n",
        "\n",
        "Usando **pandas_udf()** si crea una UDF Pandas in cui la funzione data prende uno o più *pandas.Series* e restituisce un *pandas.Series*. L'output della funzione deve essere sempre della stessa lunghezza dell'input. Internamente, PySpark eseguirà una UDF Pandas suddividendo le colonne in batch e chiamando la funzione per ogni batch come sottoinsieme dei dati, quindi concatenando i risultati insieme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RhtQpiJSTPw"
      },
      "source": [
        "from pyspark.sql.functions import col, pandas_udf\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
        "    return a * b\n",
        "\n",
        "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
        "\n",
        "# The function for a pandas_udf should be able to execute with local Pandas data\n",
        "x = pd.Series([1, 2, 3])\n",
        "print(multiply_func(x, x))\n",
        "\n",
        "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
        "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
        "df.show()\n",
        "\n",
        "# Execute function as a Spark vectorized UDF\n",
        "df.select(multiply(col(\"x\"), col(\"x\"))).show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBFcZX0NxKH5"
      },
      "source": [
        "df.select(multiply(df.x,df.x)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbqQCxIii5CV"
      },
      "source": [
        "###Iterator of Series to Iterator of Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXrfquBWjjiJ"
      },
      "source": [
        "from typing import Iterator\n",
        "\n",
        "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "@pandas_udf(\"long\")\n",
        "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
        "    for x in iterator:\n",
        "        yield x + 1\n",
        "\n",
        "df.select(plus_one(\"x\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86phDL00znGt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu2WYhHIkxoj"
      },
      "source": [
        "###Iterator of Multiple Series to Iterator of Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVufnXCMk2Bu"
      },
      "source": [
        "from typing import Iterator, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "@pandas_udf(\"long\")\n",
        "def multiply_two_cols(\n",
        "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
        "    for a, b in iterator:\n",
        "        yield a * b\n",
        "\n",
        "df.select(multiply_two_cols(\"x\", \"x\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-v1axZnlZ9u"
      },
      "source": [
        "###Series to Scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn_-_lT_ldtM"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql import Window\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
        "    (\"id\", \"v\"))\n",
        "df.show()\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "@pandas_udf(\"double\")\n",
        "def mean_udf(v: pd.Series) -> float:\n",
        "    return v.mean()\n",
        "\n",
        "df.select(mean_udf(df['v'])).show()\n",
        "\n",
        "df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
        "\n",
        "w = Window.partitionBy('id').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Mv76klnXJf"
      },
      "source": [
        "##Grouped Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvrH9qOBncJa"
      },
      "source": [
        "df = spark.createDataFrame(\n",
        "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
        "    (\"id\", \"v\"))\n",
        "\n",
        "def subtract_mean(pdf):\n",
        "    # pdf is a pandas.DataFrame\n",
        "    v = pdf.v\n",
        "    return pdf.assign(v=v - v.mean())\n",
        "\n",
        "df.groupby(\"id\").applyInPandas(subtract_mean, schema=\"id long, v double\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY02wdtgnvgW"
      },
      "source": [
        "##Map\n",
        "\n",
        "Le operazioni sono supportate da **DataFrame.mapInPandas()** che mappa un iteratore di *pandas.DataFrames* in un altro iteratore di *pandas.DataFrames* che rappresenta l'attuale PySpark DataFrame e restituisce il risultato come PySpark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVJKPBnQnxek"
      },
      "source": [
        "df = spark.createDataFrame([(1, 21), (2, 30), (3, 55)], (\"id\", \"age\"))\n",
        "df.show()\n",
        "\n",
        "def filter_func(iterator):\n",
        "    for pdf in iterator:\n",
        "        yield pdf[pdf.id == 1]\n",
        "\n",
        "df.mapInPandas(filter_func, schema=df.schema).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5OfuDHwo6RW"
      },
      "source": [
        "##Co-grouped Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JibaBRJIo8Ez"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = spark.createDataFrame(\n",
        "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
        "    (\"time\", \"id\", \"v1\"))\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame(\n",
        "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
        "    (\"time\", \"id\", \"v2\"))\n",
        "df2.show()\n",
        "\n",
        "def asof_join(l, r):\n",
        "    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
        "\n",
        "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
        "    asof_join, schema=\"time int, id int, v1 double, v2 string\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2S2Yvq8pb1q"
      },
      "source": [
        "##Usage Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UDsQiQq3En"
      },
      "source": [
        "###Setting Arrow **self_destruct** for memory savings\n",
        "\n",
        "Dalla versione Spark 3.2, la configurazione *spark.sql.execution.arrow.pyspark.selfDestruct.enabled* può essere utilizzata per abilitare la funzione *self_destruct* di PyArrow, che può risparmiare memoria durante la creazione di un DataFrame Panda tramite toPandas liberando la memoria allocata da Arrow durante la creazione del DataFrame stesso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5LBiTK0p6Oj"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\",\"true\")\n",
        "spark.conf.get(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}