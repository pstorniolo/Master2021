{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-23-PyArrow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOw35q/tlA+yXXYoc44t8NT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_23_PyArrow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZPGsw6wLgGQ"
      },
      "source": [
        "#Apache Arrow in PySpark\n",
        "\n",
        "Apache Arrow è un formato di dati a colonne in memoria utilizzato in Spark per trasferire in modo efficiente i dati tra i processi *JVM* e *Python*. Questo è attualmente molto vantaggioso per gli utenti Python che lavorano con dati *Pandas/NumPy*. Il suo utilizzo non è automatico e potrebbe richiedere alcune modifiche minori alla configurazione o al codice per trarre il massimo vantaggio e garantire la compatibilità."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePujHliLIRyN",
        "outputId": "95610b63-2d86-409d-b08a-ed602b191697"
      },
      "source": [
        "# Install Spark 3.2.0\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f *.tgz*\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark==3.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 37 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 14.5 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0W_Z-pPIaiY"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "from datetime import datetime, date\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow\n",
        "\n",
        "spark=SparkSession.builder.appName(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-o56OhqMOMa"
      },
      "source": [
        "##Enabling for Conversion to/from Pandas\n",
        "\n",
        "Arrow è disponibile come ottimizzazione durante la conversione di Spark DataFrame in Pandas DataFrame usando la chiamata **DataFrame.toPandas()** e durante la creazione di Spark DataFrame da Pandas DataFrame con **SparkSession.createDataFrame()**. Per usare Arrow durante l'esecuzione di queste chiamate, si deve prima impostare la configurazione Spark *spark.sql.execution.arrow.pyspark.enabled* su *true*. Questa impostazione è disabilitata per default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ekkYwCIwM0OQ",
        "outputId": "2282bdec-8439-4070-b9e3-6c82caf6b939"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'true'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPQL_kabM9l_"
      },
      "source": [
        "L'utilizzo delle ottimizzazioni con Arrow produrrà gli stessi risultati di quando Arrow non è abilitato se non abilitato nell'ambiente Python. La conversione sarà meno efficiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "dRdtdCgoNPVZ",
        "outputId": "bfb384fa-81ce-420d-994c-e065e1518918"
      },
      "source": [
        "# Generate a Pandas DataFrame\n",
        "pdf = pd.DataFrame(np.random.rand(1000, 3))\n",
        "\n",
        "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
        "result_pdf = df.select(\"*\").toPandas()\n",
        "\n",
        "%time\n",
        "result_pdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 8.58 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.292554</td>\n",
              "      <td>0.895397</td>\n",
              "      <td>0.918625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.071531</td>\n",
              "      <td>0.947871</td>\n",
              "      <td>0.334770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.119521</td>\n",
              "      <td>0.593665</td>\n",
              "      <td>0.055347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.775464</td>\n",
              "      <td>0.308434</td>\n",
              "      <td>0.807798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.787438</td>\n",
              "      <td>0.206828</td>\n",
              "      <td>0.729282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0.560107</td>\n",
              "      <td>0.684574</td>\n",
              "      <td>0.118798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.702867</td>\n",
              "      <td>0.945057</td>\n",
              "      <td>0.778028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.597309</td>\n",
              "      <td>0.823100</td>\n",
              "      <td>0.379332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0.380332</td>\n",
              "      <td>0.171202</td>\n",
              "      <td>0.754277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0.468934</td>\n",
              "      <td>0.189392</td>\n",
              "      <td>0.591102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2\n",
              "0    0.292554  0.895397  0.918625\n",
              "1    0.071531  0.947871  0.334770\n",
              "2    0.119521  0.593665  0.055347\n",
              "3    0.775464  0.308434  0.807798\n",
              "4    0.787438  0.206828  0.729282\n",
              "..        ...       ...       ...\n",
              "995  0.560107  0.684574  0.118798\n",
              "996  0.702867  0.945057  0.778028\n",
              "997  0.597309  0.823100  0.379332\n",
              "998  0.380332  0.171202  0.754277\n",
              "999  0.468934  0.189392  0.591102\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4WtS1Y2RGM8"
      },
      "source": [
        "##Pandas UDFs (a.k.a. Vectorized UDFs)\n",
        "\n",
        "Le UDF di Panda sono funzioni definite dall'utente che vengono eseguite da Spark utilizzando Arrow per trasferire dati e Panda per lavorare con i dati, il che consente operazioni vettorializzate. Una Pandas UDF viene definita utilizzando **pandas_udf()** come wrapper per la funzione e non è richiesta alcuna configurazione aggiuntiva. Una Pandas UDF si comporta come una normale API di PySpark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1M7m8ZOOYwS",
        "outputId": "3ebc69c1-9431-4209-f515-baefde8be5de"
      },
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf(\"col1 string, col2 long\")\n",
        "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
        "    s3['col2'] = s1 + s2.str.len()\n",
        "    return s3\n",
        "\n",
        "# Create a Spark DataFrame that has three columns including a struct column.\n",
        "df = spark.createDataFrame(\n",
        "    [[1, \"a string\", (\"a nested string\",)]],\n",
        "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- long_col: long (nullable = true)\n",
            " |-- string_col: string (nullable = true)\n",
            " |-- struct_col: struct (nullable = true)\n",
            " |    |-- col1: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
            " |    |-- col1: string (nullable = true)\n",
            " |    |-- col2: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnJeSG0rSJkj"
      },
      "source": [
        "###Series to Series\n",
        "\n",
        "Usando **pandas_udf()** si crea una UDF Pandas in cui la funzione data prende uno o più *pandas.Series* e restituisce un *pandas.Series*. L'output della funzione deve essere sempre della stessa lunghezza dell'input. Internamente, PySpark eseguirà una UDF Pandas suddividendo le colonne in batch e chiamando la funzione per ogni batch come sottoinsieme dei dati, quindi concatenando i risultati insieme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RhtQpiJSTPw",
        "outputId": "2fe003bc-376e-4850-e51d-8e2d206a502c"
      },
      "source": [
        "from pyspark.sql.functions import col, pandas_udf\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
        "    return a * b\n",
        "\n",
        "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
        "\n",
        "# The function for a pandas_udf should be able to execute with local Pandas data\n",
        "x = pd.Series([1, 2, 3])\n",
        "print(multiply_func(x, x))\n",
        "\n",
        "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
        "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
        "\n",
        "# Execute function as a Spark vectorized UDF\n",
        "df.select(multiply(col(\"x\"), col(\"x\"))).show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    1\n",
            "1    4\n",
            "2    9\n",
            "dtype: int64\n",
            "+-------------------+\n",
            "|multiply_func(x, x)|\n",
            "+-------------------+\n",
            "|                  1|\n",
            "|                  4|\n",
            "|                  9|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbqQCxIii5CV"
      },
      "source": [
        "##Iterator of Series to Iterator of Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXrfquBWjjiJ",
        "outputId": "6ad0a0a6-7118-4ab4-f86e-3d5232e9af8b"
      },
      "source": [
        "from typing import Iterator\n",
        "\n",
        "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "@pandas_udf(\"long\")\n",
        "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
        "    for x in iterator:\n",
        "        yield x + 1\n",
        "\n",
        "df.select(plus_one(\"x\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|plus_one(x)|\n",
            "+-----------+\n",
            "|          2|\n",
            "|          3|\n",
            "|          4|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu2WYhHIkxoj"
      },
      "source": [
        "##Iterator of Multiple Series to Iterator of Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVufnXCMk2Bu",
        "outputId": "05993e10-04a8-4b62-954f-a3815378aa7f"
      },
      "source": [
        "from typing import Iterator, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "@pandas_udf(\"long\")\n",
        "def multiply_two_cols(\n",
        "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
        "    for a, b in iterator:\n",
        "        yield a * b\n",
        "\n",
        "df.select(multiply_two_cols(\"x\", \"x\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|  x|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "+---+\n",
            "\n",
            "+-----------------------+\n",
            "|multiply_two_cols(x, x)|\n",
            "+-----------------------+\n",
            "|                      1|\n",
            "|                      4|\n",
            "|                      9|\n",
            "+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-v1axZnlZ9u"
      },
      "source": [
        "##Series to Scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn_-_lT_ldtM",
        "outputId": "261b4ed7-3e7e-41ec-ce17-4e14125cb616"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql import Window\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
        "    (\"id\", \"v\"))\n",
        "df.show()\n",
        "\n",
        "# Declare the function and create the UDF\n",
        "@pandas_udf(\"double\")\n",
        "def mean_udf(v: pd.Series) -> float:\n",
        "    return v.mean()\n",
        "\n",
        "df.select(mean_udf(df['v'])).show()\n",
        "\n",
        "df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
        "\n",
        "w = Window.partitionBy('id').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|   v|\n",
            "+---+----+\n",
            "|  1| 1.0|\n",
            "|  1| 2.0|\n",
            "|  2| 3.0|\n",
            "|  2| 5.0|\n",
            "|  2|10.0|\n",
            "+---+----+\n",
            "\n",
            "+-----------+\n",
            "|mean_udf(v)|\n",
            "+-----------+\n",
            "|        4.2|\n",
            "+-----------+\n",
            "\n",
            "+---+-----------+\n",
            "| id|mean_udf(v)|\n",
            "+---+-----------+\n",
            "|  1|        1.5|\n",
            "|  2|        6.0|\n",
            "+---+-----------+\n",
            "\n",
            "+---+----+------+\n",
            "| id|   v|mean_v|\n",
            "+---+----+------+\n",
            "|  1| 1.0|   1.5|\n",
            "|  1| 2.0|   1.5|\n",
            "|  2| 3.0|   6.0|\n",
            "|  2| 5.0|   6.0|\n",
            "|  2|10.0|   6.0|\n",
            "+---+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Mv76klnXJf"
      },
      "source": [
        "##Grouped Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvrH9qOBncJa",
        "outputId": "4e1cf6e4-1ef0-410e-aaf1-aac4d7edf9d5"
      },
      "source": [
        "df = spark.createDataFrame(\n",
        "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
        "    (\"id\", \"v\"))\n",
        "\n",
        "def subtract_mean(pdf):\n",
        "    # pdf is a pandas.DataFrame\n",
        "    v = pdf.v\n",
        "    return pdf.assign(v=v - v.mean())\n",
        "\n",
        "df.groupby(\"id\").applyInPandas(subtract_mean, schema=\"id long, v double\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|   v|\n",
            "+---+----+\n",
            "|  1|-0.5|\n",
            "|  1| 0.5|\n",
            "|  2|-3.0|\n",
            "|  2|-1.0|\n",
            "|  2| 4.0|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY02wdtgnvgW"
      },
      "source": [
        "##Map\n",
        "\n",
        "Le operazioni sono supportate da **DataFrame.mapInPandas()** che mappa un iteratore di *pandas.DataFrames* in un altro iteratore di *pandas.DataFrames* che rappresenta l'attuale PySpark DataFrame e restituisce il risultato come PySpark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVJKPBnQnxek",
        "outputId": "dc6a5c06-1425-471b-f0d2-440d82e43dd3"
      },
      "source": [
        "df = spark.createDataFrame([(1, 21), (2, 30), (3, 55)], (\"id\", \"age\"))\n",
        "df.show()\n",
        "\n",
        "def filter_func(iterator):\n",
        "    for pdf in iterator:\n",
        "        yield pdf[pdf.id == 1]\n",
        "\n",
        "df.mapInPandas(filter_func, schema=df.schema).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|  1| 21|\n",
            "|  2| 30|\n",
            "|  3| 55|\n",
            "+---+---+\n",
            "\n",
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|  1| 21|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5OfuDHwo6RW"
      },
      "source": [
        "##Co-grouped Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JibaBRJIo8Ez",
        "outputId": "579fc408-2e51-40b1-b8c5-76dcb20c33c3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = spark.createDataFrame(\n",
        "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
        "    (\"time\", \"id\", \"v1\"))\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame(\n",
        "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
        "    (\"time\", \"id\", \"v2\"))\n",
        "df2.show()\n",
        "\n",
        "def asof_join(l, r):\n",
        "    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
        "\n",
        "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
        "    asof_join, schema=\"time int, id int, v1 double, v2 string\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+---+\n",
            "|    time| id| v1|\n",
            "+--------+---+---+\n",
            "|20000101|  1|1.0|\n",
            "|20000101|  2|2.0|\n",
            "|20000102|  1|3.0|\n",
            "|20000102|  2|4.0|\n",
            "+--------+---+---+\n",
            "\n",
            "+--------+---+---+\n",
            "|    time| id| v2|\n",
            "+--------+---+---+\n",
            "|20000101|  1|  x|\n",
            "|20000101|  2|  y|\n",
            "+--------+---+---+\n",
            "\n",
            "+--------+---+---+---+\n",
            "|    time| id| v1| v2|\n",
            "+--------+---+---+---+\n",
            "|20000101|  1|1.0|  x|\n",
            "|20000102|  1|3.0|  x|\n",
            "|20000101|  2|2.0|  y|\n",
            "|20000102|  2|4.0|  y|\n",
            "+--------+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2S2Yvq8pb1q"
      },
      "source": [
        "##Usage Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UDsQiQq3En"
      },
      "source": [
        "###Setting Arrow **self_destruct** for memory savings\n",
        "\n",
        "Dalla versione Spark 3.2, la configurazione *spark.sql.execution.arrow.pyspark.selfDestruct.enabled* può essere utilizzata per abilitare la funzione *self_destruct* di PyArrow, che può risparmiare memoria durante la creazione di un DataFrame Panda tramite toPandas liberando la memoria allocata da Arrow durante la creazione del DataFrame stesso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r5LBiTK0p6Oj",
        "outputId": "706d3182-8d7f-4add-a149-ca7017abd832"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\",\"true\")\n",
        "spark.conf.get(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'true'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}