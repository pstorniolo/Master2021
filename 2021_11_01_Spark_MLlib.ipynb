{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-11-01-Spark_MLlib.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMUxgpQQXMNmaADzDfRRzHU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_11_01_Spark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcadi1WFa3CI"
      },
      "source": [
        "#MLlib\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zlGnx1Qaclc"
      },
      "source": [
        "!curl ipecho.net/plain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHq0fu7_YMeY"
      },
      "source": [
        "# Install Spark 3.2.0 - JDK11\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "#!curl -1 -O https://www.pa.icar.cnr.it/storniolo/files/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f *.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "!pip -q install findspark\n",
        "#!pip install -q dnspython\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "#from pymongo import MongoClient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A4JEEB9Z3lz"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print(spark.version)\n",
        "#spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKsz-xAvXTLn"
      },
      "source": [
        "##Correlation\n",
        "Il calcolo della correlazione tra due serie di dati è un'operazione comune in Statistica. \n",
        "\n",
        "**spark.ml** fornisce la flessibilità per calcolare le correlazioni a coppie tra molte serie. I metodi di correlazione supportati sono attualmente la correlazione di Pearson e di Spearman.\n",
        "\n",
        "La *Correlation* calcola la matrice di correlazione per il set di dati di vettori di input utilizzando il metodo specificato. L'output sarà un DataFrame che contiene la matrice di correlazione della colonna di vettori."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpLBIvPPm4NE"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
        "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
        "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
        "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9ir9yDzpmiw"
      },
      "source": [
        "r1 = Correlation.corr(df, \"features\").head()\n",
        "print()\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
        "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w5RtacxhCC-"
      },
      "source": [
        "##Summarizer\n",
        "\n",
        "Fornisce statistiche di riepilogo delle colonne vettoriali per Dataframe tramite il metodo *Summarizer*. Le metriche disponibili sono *max*, *min*, *mean*, *sum*, *variance*, *std*, *nonzeros* e *count*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu16t2bUhuiI"
      },
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
        "\n",
        "df.show()\n",
        "\n",
        "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
        "summarizer = Summarizer.metrics(\"mean\", \"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdPGYdiown"
      },
      "source": [
        "# compute statistics for multiple metrics with weight\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k_p7wipiuB-"
      },
      "source": [
        "# compute statistics for multiple metrics without weight\n",
        "df.select(summarizer.summary(df.features)).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNON0tOcivf6"
      },
      "source": [
        "# compute statistics for single metric \"mean\" with weight\n",
        "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH4iWrp-izAT"
      },
      "source": [
        "# compute statistics for single metric \"mean\" without weight\n",
        "df.select(Summarizer.mean(df.features)).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNLPT6XajiWy"
      },
      "source": [
        "##Hypothesis testing\n",
        "\n",
        "*Hypothesis testing* è un potente strumento in statistica per determinare se un risultato è statisticamente significativo, se questo risultato si è verificato per caso o meno. \n",
        "\n",
        "**spark.ml** attualmente supporta i test Chi-quadrato di Pearson (χ2) per l'indipendenza."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xg9ro0jkCzb"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "\n",
        "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        "        (0.0, Vectors.dense(1.5, 20.0)),\n",
        "        (1.0, Vectors.dense(1.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 30.0)),\n",
        "        (0.0, Vectors.dense(3.5, 40.0)),\n",
        "        (1.0, Vectors.dense(3.5, 40.0))]\n",
        "        \n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiVKbpCRkNfW"
      },
      "source": [
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "print()\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3tI8GyzlkXk"
      },
      "source": [
        "##Data sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP2Ma15fyOl-"
      },
      "source": [
        "Creo un collegamento con la cartella data di spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_SdlfG4yCwP"
      },
      "source": [
        "!ln -s spark-3.2.0-bin-hadoop3.2/data data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31bP8iFXxUXN"
      },
      "source": [
        "df = spark.read.format(\"image\").option(\"dropInvalid\", True) \\\n",
        "               .load(\"data/mllib/images/origin/kittens\")\n",
        "df.printSchema()\n",
        "df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcs2XNRtw8QV"
      },
      "source": [
        "df = spark.read.format(\"libsvm\").option(\"numFeatures\", \"780\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
        "df.printSchema()\n",
        "df.show(10,truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lailnv30zMm"
      },
      "source": [
        "##Pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unU2-sOf1i7y"
      },
      "source": [
        "###Estimator, Transformer, and Param"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmj-hDm102al"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Prepare training data from a list of (label, features) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
        "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
        "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
        "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
        "\n",
        "training.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBhy6NU80_iz"
      },
      "source": [
        "# Create a LogisticRegression instance. This instance is an Estimator.\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
        "\n",
        "# Print out the parameters, documentation, and any default values.\n",
        "#print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9K4elAb1MHA"
      },
      "source": [
        "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
        "model1 = lr.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtPNbeIs11sH"
      },
      "source": [
        "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
        "# we can view the parameters it used during fit().\n",
        "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
        "# LogisticRegression instance.\n",
        "print(\"Model 1 was fit using parameters: \")\n",
        "print(model1.extractParamMap())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7S65tAP1-r1"
      },
      "source": [
        "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
        "paramMap = {lr.maxIter: 20}\n",
        "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
        "# Specify multiple Params.\n",
        "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PnOcIJP2Fs3"
      },
      "source": [
        "# You can combine paramMaps, which are python dictionaries.\n",
        "# Change output column name\n",
        "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # type: ignore\n",
        "paramMapCombined = paramMap.copy()\n",
        "paramMapCombined.update(paramMap2)  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOHwDJxd2PPk"
      },
      "source": [
        "# Now learn a new model using the paramMapCombined parameters.\n",
        "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
        "model2 = lr.fit(training, paramMapCombined)\n",
        "print(\"Model 2 was fit using parameters: \")\n",
        "print(model2.extractParamMap())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cqCg8_n2ZLT"
      },
      "source": [
        "# Prepare test data\n",
        "test = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
        "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
        "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
        "test.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa0mGfc42hHC"
      },
      "source": [
        "# Make predictions on test data using the Transformer.transform() method.\n",
        "# LogisticRegression.transform will only use the 'features' column.\n",
        "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
        "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
        "prediction = model2.transform(test)\n",
        "prediction.printSchema()\n",
        "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r084kMrO2pmy"
      },
      "source": [
        "for row in result:\n",
        "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
        "          % (row.features, row.label, row.myProbability, row.prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stPQvNjF3VxF"
      },
      "source": [
        "###Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-ATLMyA3cNL"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "\n",
        "# Prepare training documents from a list of (id, text, label) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 2.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "training.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbxO-Mqv3poJ"
      },
      "source": [
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKkGXxvh3x7K"
      },
      "source": [
        "# Fit the pipeline to training documents.\n",
        "model = pipeline.fit(training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yexC73SB31eL"
      },
      "source": [
        "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"spark hadoop spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "test.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMwd3r6R3_W7"
      },
      "source": [
        "# Make predictions on test documents and print columns of interest.\n",
        "prediction = model.transform(test)\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    rid, text, prob, prediction = row  # type: ignore\n",
        "    print(\n",
        "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
        "            rid, text, str(prob), prediction   # type: ignore\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt62PtPyxWLx"
      },
      "source": [
        "#Stop Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvTL5-HLxyp9"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}