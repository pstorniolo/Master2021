{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-11-06-1_SparkNLP_SparkML_Pipelines.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_11_06_1_SparkNLP_SparkML_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1MOf0VrlEe"
      },
      "source": [
        "##**Colab** setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVycMjc1dLIe"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip -q install --ignore-installed pyspark==3.0.3\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip -q install --ignore-installed spark-nlp==3.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWM4Im4DMSSM"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "https://it.wikipedia.org/wiki/John_Snow_(medico)\n",
        "\n",
        "https://www.johnsnowlabs.com/spark-nlp/\n",
        "#Spark NLP\n",
        "\n",
        "**Spark ML** fornisce un set di applicazioni di Machine Learning che possono essere predisposte utilizzando due componenti principali: *Estimators* e *Transformers*. Gli *Estimators* hanno un metodo chiamato ***fit()*** che protegge e addestra una parte di dati a tale applicazione. Il *Transformer* è generalmente il risultato di un processo di adattamento e applica modifiche al set di dati di destinazione. Questi componenti sono stati incorporati per essere utilizzabili da **Spark NLP**.\n",
        "\n",
        "Le **Pipeline** sono un meccanismo per combinare più *Estimators* e *Transformers* in un unico flusso di lavoro. Consentono più trasformazioni concatenate lungo un'attività di Machine Learning.\n",
        "\n",
        "https://nlp.johnsnowlabs.com/api/python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plXDUA7kNlHO"
      },
      "source": [
        "##Annotation\n",
        "Il risultato di base di un'operazione **Spark NLP** è un'annotazione. La sua struttura comprende:\n",
        "*   *annotatorType*: il tipo di annotatore che ha generato l'annotazione corrente\n",
        "*   *begin*: l'inizio del contenuto corrispondente relativo al testo non elaborato\n",
        "*   *end*: la fine del contenuto corrispondente relativo al testo non elaborato\n",
        "*   *result*: l'output principale dell'annotazione\n",
        "*   *metadata*: contenuto del risultato abbinato e informazioni aggiuntive\n",
        "*   *embedding*: contiene mappature vettoriali se necessarie\n",
        "\n",
        "Questo oggetto viene generato automaticamente dagli annotatori dopo un processo di trasformazione. Non è richiesto alcun lavoro manuale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_GTgnAIPAaw"
      },
      "source": [
        "###*Annotators*\n",
        "Gli *Annotators* sono la punta di diamante delle funzioni NLP in Spark NLP. Esistono due forme di annotatori:\n",
        "*   *Annotator Approaches*: sono quelli che rappresentano uno Spark ML Estimator e richiedono una fase di formazione. Hanno una funzione chiamata fit(data) che addestra un modello basato su alcuni dati. Producono il secondo tipo di annotatore che è un modello di annotatore o trasformatore.\n",
        "*   *Annotator Models*: sono modelli spark o *Trasformators*, nel senso che hanno una funzione di trasformazione (dati). Questa funzione prende come input un dataframe a cui aggiunge una nuova colonna contenente il risultato dell'annotazione corrente. Tutti i *Trasformators* sono additivi, nel senso che si aggiungono ai dati correnti, non sostituiscono o eliminano mai le informazioni precedenti.\n",
        "\n",
        "Entrambe le forme di annotatori possono essere incluse in una pipeline. Tutti gli annotatori inclusi in una pipeline verranno eseguiti automaticamente nell'ordine definito e trasformeranno i dati di conseguenza. Una Pipeline viene trasformata in un *PipelineModel* dopo la fase *fit()*. La *Pipeline* può essere salvata su disco e ricaricata in qualsiasi momento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RAHti6xQC7M"
      },
      "source": [
        "###*Common Functions*\n",
        "*   *setInputCols(column_names)*: prende un elenco di nomi di colonne di annotazioni richieste da questo annotatore. Questi sono generati dagli annotatori che precedono l'annotatore corrente nella pipeline.\n",
        "*   *setOutputCol(column_name)*: definisce il nome della colonna contenente il risultato dell'annotatore corrente. Usa questo nome come input per altri annotatori lungo la pipeline che richiedono gli output generati dall'annotatore corrente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcqo86txRELh"
      },
      "source": [
        "##**Pretrained** pipeline\n",
        "**Explain Document ML** (*explain_document_ml*) è una pipeline preaddestrata che fa un po' di tutto ciò che riguarda la NLP.\n",
        "\n",
        "https://nlp.johnsnowlabs.com/docs/en/pipelines\n",
        "\n",
        "https://nlp.johnsnowlabs.com/models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Z8lAAdSOnY"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n",
        "\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEMoLsFiRsaG"
      },
      "source": [
        "# Download the pretrained pipeline from Johnsnowlab's servers\n",
        "explain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")\n",
        "\n",
        "annotations = explain_document_pipeline.annotate(\"We are very happy about SparkNLP\")\n",
        "print(annotations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbfhkH_2UvFe"
      },
      "source": [
        "Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgZN3FiUUhdw"
      },
      "source": [
        "sentences = [\n",
        "  ['Hello, this is an example sentence'],\n",
        "  ['And this is a second sentence.']\n",
        "]\n",
        "\n",
        "# spark is the Spark Session automatically started by pyspark.\n",
        "data = spark.createDataFrame(sentences).toDF(\"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35mINHmSVGn8"
      },
      "source": [
        "# Transform 'data' and store output in a new 'annotations_df' dataframe\n",
        "annotations_df = explain_document_pipeline.transform(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgbHSOYeVLFs"
      },
      "source": [
        "# Show the results\n",
        "annotations_df.printSchema()\n",
        "annotations_df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm_1ujy6Y01R"
      },
      "source": [
        "##Manipulating pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQD8cIcWY80d"
      },
      "source": [
        "annotations_df.select(\"token\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dh12R40ZJms"
      },
      "source": [
        "Per vedere solo delle annotazioni risultanti possiamo utilizzare l'annotatore *Finisher*, recuperare la pipeline *Explain Document ML* e aggiungerli insieme in una pipeline Spark ML. Le pipeline preaddestrate si aspettano che la colonna di input sia denominata \"text\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PwyEG2NZdpU"
      },
      "source": [
        "from sparknlp import Finisher\n",
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpv51pmYZjLS"
      },
      "source": [
        "explain_pipeline_model = PretrainedPipeline(\"explain_document_ml\").model\n",
        "finisher = Finisher().setInputCols([\"token\", \"lemmas\", \"pos\"])\n",
        "\n",
        "pipeline = Pipeline().setStages([\n",
        "               explain_pipeline_model,\n",
        "               finisher\n",
        "           ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFCtZDJ7Zn60"
      },
      "source": [
        "sentences = [\n",
        "    ['Hello, this is an example sentence'],\n",
        "    ['And this is a second sentence.']\n",
        "]\n",
        "data = spark.createDataFrame(sentences).toDF(\"text\")\n",
        "\n",
        "model = pipeline.fit(data)\n",
        "annotations_finished_df = model.transform(data)\n",
        "\n",
        "annotations_finished_df.select('finished_token').show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epkk3MUMmNjR"
      },
      "source": [
        "# Spark NLP and Spark ML Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouQjb9kWmNjS"
      },
      "source": [
        "## Simple Topic Modeling\n",
        "\n",
        "`Spark-NLP`\n",
        "* DocumentAssembler\n",
        "* SentenceDetector\n",
        "* Tokenizer\n",
        "* Normalizer\n",
        "* POS tagger\n",
        "* Chunker\n",
        "* Finisher\n",
        "\n",
        "`Spark ML`\n",
        "* Hashing\n",
        "* [TF-IDF](https://it.wikipedia.org/wiki/Tf-idf)\n",
        "* [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aKWnMGkmNjU"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "#Spark ML & SQL\n",
        "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.clustering import LDA, LDAModel\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "#Spark NLP\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import RegexRule\n",
        "from sparknlp.base import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3wJ02KWmNjY"
      },
      "source": [
        "### Let's create a Spark Session for our app"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irMmrC0JmNjZ"
      },
      "source": [
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8DvVMvnmNjd"
      },
      "source": [
        "Let's download some scientific sample from PubMed dataset:\n",
        "```\n",
        "wget -N \thttps://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv -P /tmp\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndXJqm2LmNje"
      },
      "source": [
        "! wget -q -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv -P /tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkrFCXC6QzJq"
      },
      "source": [
        "#!cat /tmp/pubmed-sample.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUeGQLVGmNjh"
      },
      "source": [
        "pubMedDF = spark.read.option(\"header\", \"true\").csv(\"/tmp/pubmed-sample.csv\").filter(\"AB IS NOT null\").withColumn(\"text\", col(\"AB\")).drop(\"TI\", \"AB\")\n",
        "pubMedDF.printSchema()\n",
        "pubMedDF.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P7fpHe0mNjm",
        "scrolled": true
      },
      "source": [
        "print(pubMedDF.count())\n",
        "\n",
        "pubMedDF = pubMedDF.limit(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jGB30xSmNjp"
      },
      "source": [
        "### Let's create Spark-NLP Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcltBIMymNjq"
      },
      "source": [
        "# Spark NLP Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler().setInputCol(\"text\")\n",
        "\n",
        "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
        "\n",
        "posTagger = PerceptronModel.pretrained().setInputCols([\"sentence\", \"token\"])\n",
        "\n",
        "chunker = Chunker().setInputCols([\"sentence\", \"pos\"]).setOutputCol(\"chunk\").setRegexParsers([\"<NNP>+\", \"<DT>?<JJ>*<NN>\"])\n",
        "\n",
        "finisher = Finisher().setInputCols([\"chunk\"]).setIncludeMetadata(False)\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "    document_assembler, \n",
        "    sentence_detector, \n",
        "    tokenizer,\n",
        "    posTagger,\n",
        "    chunker,\n",
        "    finisher\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPPJ0m3hmNjt"
      },
      "source": [
        "nlpPipelineDF = nlpPipeline.fit(pubMedDF).transform(pubMedDF)\n",
        "nlpPipelineDF.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgDdXnn2mNjv"
      },
      "source": [
        "### Let's create Spark ML Pipeline\n",
        "\n",
        "https://spark.apache.org/docs/latest/mllib-feature-extraction.html\n",
        "\n",
        "*   *TF*: Term Frequency\n",
        "*   *DF*: Document Frequency\n",
        "*   *IDF*: Inverse Document Frequency\n",
        "*   *LDA*: Latent Dirichlet Allocation\n",
        "*   *CountVectorizer*: Estrae un vocabolario dalle raccolte di documenti e genera un *CountVectorizerModel*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BDQ5C6hmNjw"
      },
      "source": [
        "# SPark ML Pipeline\n",
        "\n",
        "cv = CountVectorizer(inputCol=\"finished_chunk\", outputCol=\"features\", vocabSize=1000, minDF=10.0, minTF=10.0)\n",
        "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
        "lda = LDA(k=10, maxIter=5)\n",
        "\n",
        "### Let's create Spark-NLP Pipeline\n",
        "mlPipeline = Pipeline(stages=[\n",
        "    cv,\n",
        "    idf,\n",
        "    lda\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM6qSQKHmNjy"
      },
      "source": [
        "###We are going to train Spark ML Pipeline by using Spark-NLP Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXinh1fEmNjz"
      },
      "source": [
        "# Let's create Spark-NLP Pipeline\n",
        "mlModel = mlPipeline.fit(nlpPipelineDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i34QTAtdmNj2"
      },
      "source": [
        "mlPipelineDF = mlModel.transform(nlpPipelineDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTbgegKhmNj4"
      },
      "source": [
        "mlPipelineDF.printSchema()\n",
        "mlPipelineDF.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybTGr48jjD7N"
      },
      "source": [
        "https://spark.apache.org/docs/latest/ml-pipeline.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf4y90rGmNj7"
      },
      "source": [
        "ldaModel = mlModel.stages[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5oFHh8qmNj9"
      },
      "source": [
        "ll = ldaModel.logLikelihood(mlPipelineDF)\n",
        "lp = ldaModel.logPerplexity(mlPipelineDF)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
        "print(\"The upper bound on perplexity: \" + str(lp))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfYw9B1amNj_"
      },
      "source": [
        "# Describe topics.\n",
        "print(\"The topics described by their top-weighted terms (the first three):\")\n",
        "ldaModel.describeTopics(3).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6w3HawgmNkC"
      },
      "source": [
        "### Let's look at out topics\n",
        "NOTA: più operazioni di *cleaning*, *filtering*, *playing* con `CountVectorizer` e più iterazioni in `LDA` porteranno a migliori risultati di *Topic Modeling*.\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.CountVectorizer.html\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.CountVectorizerModel.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PRurlj8mNkD"
      },
      "source": [
        "# Output topics. Each is a distribution over words (matching word count vectors)\n",
        "print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize()) + \" words):\")\n",
        "\n",
        "topics = ldaModel.describeTopics(20)\n",
        "topics_rdd = topics.rdd\n",
        "\n",
        "vocab = mlModel.stages[0].vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1MlILJIqLKI"
      },
      "source": [
        "topics_words = topics_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
        "\n",
        "for idx, topic in enumerate(topics_words):\n",
        "    print(\"topic: \", idx)\n",
        "    print(\"----------------------------------------\")\n",
        "    for word in topic:\n",
        "       print(word)\n",
        "    print(\"----------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}