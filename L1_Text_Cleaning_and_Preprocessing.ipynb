{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L1_Text_Cleaning_and_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/L1_Text_Cleaning_and_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEEEM1zKQ0V1"
      },
      "source": [
        "# **Natural Language Processing - Lesson 1**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### > Schedule\n",
        "The timeframes are only estimates and may vary according to how the class is progressing.\n",
        "\n",
        "0. Strings and regex with Python\n",
        "1. **Text Cleaning and Preprocessing**\n",
        "2. Word Embedding\n",
        "3. Going towards BERT\n",
        "\n",
        "### > Why Colab? \n",
        "Colab (Google Colaboratory) is a free cloud service based on Jupyter Notebooks that supports... FREE GPU!!! <3\n",
        "\n",
        "Lectures will be held through Colab Notebooks. To download each notebook there are few and really simple steps to do:\n",
        "\n",
        "( *only one thing is required ... having Google Drive or GitHub* )\n",
        "\n",
        "1.   Click on https://bit.ly/3oktucr  \n",
        "2.   `File` > `Save a copy in Drive` / or `Save a copy on GitHub`\n",
        "3.   (Drive option) Go to your `Drive` and check if the copied version of notebook is present into `Colab Notebooks` folder \n",
        "4.   (Github option) Choose which `repository` to copy the notebook to and than `open it with Colab`\n",
        "\n",
        "## **Lesson 1 - Text Cleaning and Preprocessing**\n",
        "Data Scientist's work, as you surely know, was declared to be ***The Sexiest Job of the 21st Century***. \n",
        "\n",
        "![alt text](https://whatsthebigdata.files.wordpress.com/2016/05/time.jpg?w=768)\n",
        "\n",
        "Data cleaning is a time consuming and unenjoyable task, yet it's a very important one. Keep in mind, **\"garbage in, garbage out\"**. Feeding dirty data into a model will give us results that are meaningless.\n",
        "\n",
        "And this certainly applies to any Data Science application and project, but it is even more true for NLP projects.\n",
        "\n",
        "\n",
        "As Data Scientist, you may can use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things. Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc. From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand.\n",
        "\n",
        "Just to give you a rough idea of what we're talking about, let's look at these shots of real text: (questo era il corpus di una conversazione)\n",
        "\n",
        "\n",
        "\n",
        "> ------ Inizio chat: giovedì, giugno 01, 2017, 10:05:31 (+0200) Origine chat: Chat_Prevention Agente VIOLA ( 2s ) VIOLA: Benvenuto/a nel Servizio Clienti <tagged>, sono VIOLA. Posso chiederti il motivo per cui vuoi inviare disdetta? ( 45s ) 1498743565.6.1.fJRQ1qXM.51785098108183337..2P7WhXWCLXggWeIpnzP6q3IZyXFZi0ojXS7d576-OcE.df6e3a7f: buongiorno Viola, non ci interessa più il servizio, vorrei capire che modulo debbo scaricare, grazie mille ( 1m 15s ) VIOLA: Buongiorno ( 1m 21s ) VIOLA: ora ti fornisco tutte le info ( 1m 50s ) 1498743565.6.1.fJRQ1qXM.51785098108183337..2P7WhXWCLXggWeIpnzP6q3IZyXFZi0ojXS7d576-OcE.df6e3a7f: grazie mille per tutto, siamo soddisfatti di <tagged> e sicuramente in tempi diversi torneremo ( 2m 37s )......( 4m 30s ) VIOLA: Sei ancora in linea? ( 6m 3s ) 1498743565.6.1.fJRQ1qXM.51785098108183337..2P7WhXWCLXggWeIpnzP6q3IZyXFZi0ojXS7d576-OcE.df6e3a7f: si viola, ma noi abbiamo due gemelle nate da poco, come può immaginare, in questo momento siamo completamente assorbiti, sarò franca, stiamo pagando un servizio che non sfruttiamo più. Ma ripeto, appena le acque si saranno placate toreneremo ( 6m 23s ) \n",
        "\n",
        "\n",
        "\n",
        "> ------ Inizio chat: sabato, giugno 03, 2017, 18:08:20 (+0200) Origine chat: Chat_Amministrativa_Pulsante Agente Miriam ( 0s ) Miriam: Benvenuto, sono Miriam, in cosa posso esserti utile? ( 20s ) GIANNINO: Buonasera Miriam, ( 3m 20s ) GIANNINO: Il giorno 01/12/2016 il mio abbonamento è stato variato su mia richiesta e mi èstata inviata una email confermandomi la variazione e specificando che il nuovo importo mensile era di 48,90 Euro. Fino ad oggi ho pagato 54,90 Euro. Mi può aiutare? ...\n",
        "\n",
        "\n",
        "\n",
        "> ------ Inizio chat: sabato, giugno 03, 2017, 18:59:23 (+0200) Origine chat: Chat_Amministrativa_Pulsante Agente Marco ( 0s ) Marco: Benvenuto, sono Marco, in cosa posso esserti utile? ( 32s ) Maria Annunziata: VORREI CAMBIARE LA MODALITà DI PAGAMENTO DA ANNUALE HA MENSILE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The pre-processing steps for a problem depend mainly on the domain and the problem itself, hence, we don’t need to apply all steps to every problem.\n",
        "\n",
        "**Text Cleaning** \n",
        "\n",
        "1. Remove whitespace and unwanted text (/n)\n",
        "2. Make text all lower case\n",
        "3. Expand abbreviations\n",
        "4. Remove punctuation\n",
        "5. Remove numerical values or converting numbers into words \n",
        "6. Handling with typos (regex or spell corrector)\n",
        "\n",
        "\n",
        "**Text Pre-processing**\n",
        "\n",
        "7. Tokenize text\n",
        "8. Remove stop words (after TF-IDF)\n",
        "9. Stemming / lemmatization\n",
        "10. Parts of speech tagging\n",
        "11. NER (Named Entity Recognition)\n",
        "\n",
        "**Vectorization**\n",
        "12. Bag of Words\n",
        "13. TF-IDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Wh7nrwMynN"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this notebook, we are going to see text cleaning and preprocessing in Python. We will be using the **NLTK** (Natural Language Toolkit) library here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKIukMfCP7ic"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt0xFjmE9rD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20aae909-4b46-43a9-faee-72932fe5bc75"
      },
      "source": [
        "# Import the necessary libraries \n",
        "import nltk \n",
        "import string \n",
        "import re \n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet') \n",
        "# WordNet is a database which is built for natural language processing. It includes groups of synonyms and a brief definition.\n",
        "# WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms \n",
        "# (synsets), each expressing a distinct concept (https://wordnet.princeton.edu/)\n",
        "#nltk.download('omw')\n",
        "#\n",
        "from nltk import chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ixeogVqOeiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8996eff-c077-43d7-a4ee-b0ce6698cd00"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "syn = wordnet.synsets(\"pain\")\n",
        "#syn = wordnet.synsets(\"dolore\", lang='ita') \n",
        "print(syn[0].definition()) \n",
        "print(syn[0].examples())\n",
        "synonyms = [] \n",
        "for syn in wordnet.synsets('computer'): \n",
        "    for lemma in syn.lemmas(): \n",
        "        synonyms.append(lemma.name()) \n",
        "print(synonyms)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a symptom of some physical hurt or disorder\n",
            "['the patient developed severe pain and distension']\n",
            "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WI9nMMsW4ep"
      },
      "source": [
        "We begin to see the most common practices of text processing that aim to reduce the variance of the text: **lowercasing**, **handling stop words** and **punctuation**.\n",
        "\n",
        ">>>>> DO YOU REMEMBER WHY IT IS SO IMPORTANT?? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vt5qrf9NCuE"
      },
      "source": [
        "### Text Lowercase\n",
        "It is a very common practise. Lowercasing the text is used to reduce the size of the vocabulary of our text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gLc2TN0NN89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ab35178-b55e-494d-997b-ed9a9c79e188"
      },
      "source": [
        "def text_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = text_lowercase(input_str) \n",
        "input_str"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHgcYwYNowg"
      },
      "source": [
        "### Remove punctuation\n",
        "You should remove punctuations so that you don’t have different forms of the same word. If you don’t remove the punctuation, then \n",
        "\n",
        "> been. \n",
        "\n",
        "> been, \n",
        "\n",
        "> been! \n",
        "\n",
        "will be treated separately. \n",
        "\n",
        "One way of doing this is by looping through the Series with list comprehension and keeping everything that is not in `string.punctuation`, a list of all punctuation we imported at the beginning with `import string`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLf7TlakQHeg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2863a3e6-3afe-4101-b4dc-d231e0981f38"
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnlYBkphOnFU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a21858c-3790-4e60-dbe4-cd0cc9b4b761"
      },
      "source": [
        "def remove_punctuation(text): \n",
        "    no_punct_text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    return no_punct_text   \n",
        "input_str = remove_punctuation(input_str) \n",
        "input_str"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the 5 biggest countries by population in 2017 are china india united states indonesia and brazil'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m48HiNYeX91t"
      },
      "source": [
        "But, **be very careful**! If then in your application it will be very important to use emails, in this way you will have lost them forever. \n",
        "\n",
        "Keep in mind: always think how much a character is important to you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h8DMK5oRKBL"
      },
      "source": [
        "### Remove numbers\n",
        "Remove numbers if they are not relevant to your analyses. Usually, regular expressions are used to remove numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNC56wEBSMLe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0a85f7e-b636-4313-f122-4fad7b516c89"
      },
      "source": [
        "re.sub('\\d+', '', input_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the  biggest countries by population in  are china india united states indonesia and brazil'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1s--j9FTNB_"
      },
      "source": [
        "If instead numbers are important for the analysis, you can also convert the numbers into words. This can be done by using the `inflect` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXWdX4xMUKGx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45a71cd6-fb34-44b5-b9a5-7f812d5b835a"
      },
      "source": [
        "# inflect correctly generate plurals, singular nouns, ordinals, indefinite articles; convert numbers to words\n",
        "import inflect \n",
        "p = inflect.engine() \n",
        "\n",
        "# convert number into words \n",
        "def convert_number(text): \n",
        "\t# split string into list of words \n",
        "\ttemp_str = text.split() \n",
        "\t# initialise empty list \n",
        "\tnew_string = [] \n",
        "\n",
        "\tfor word in temp_str: \n",
        "\t\t# if word is a digit, convert the digit \n",
        "\t\t# to numbers and append into the new_string list \n",
        "\t\tif word.isdigit(): \n",
        "\t\t\ttemp = p.number_to_words(word) \n",
        "\t\t\tnew_string.append(temp) \n",
        "\n",
        "\t\t# append the word as it is \n",
        "\t\telse: \n",
        "\t\t\tnew_string.append(word) \n",
        "\n",
        "\t# join the words of new_string to form a string \n",
        "\ttemp_str = ' '.join(new_string) \n",
        "\treturn temp_str \n",
        "\n",
        "input_str = convert_number(input_str) \n",
        "input_str"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the five biggest countries by population in two thousand and seventeen are china india united states indonesia and brazil'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-blEd0OwUjsS"
      },
      "source": [
        "### Remove whitespaces \n",
        "Use the `join` and `split` function to remove all the white spaces in a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeVtfv8Ib_Nm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b981335c-a073-4deb-d89e-d7c63bef0e18"
      },
      "source": [
        "def remove_whitespace(text): \n",
        "    return  \" \".join(text.split()) \n",
        "\n",
        "remove_whitespace(\"   we don't need   the given sentence\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"we don't need the given sentence\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjFM7r7ccjwe"
      },
      "source": [
        "### Remove unwanted text\n",
        "Use `str.replace()` adn `re.sub()` methods and a laaaarge amount of regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvx7psrOcsh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ff88a6-ca31-429a-f653-149536851234"
      },
      "source": [
        "def remove_unwanted_text(text):\n",
        "  new_text = str(text)\n",
        "  new_text = re.sub(r'\\'?(\\d+)[-x,\\.:]?(\\d+)?',r'\\1',text)  # 41,5 -> 41 | 3,65 -> 3  removing decimals\n",
        "  new_text = re.sub('3+[0-9]{9}', '<mobilephone>', new_text) # adding <mobilphone> tag\n",
        "  new_text = re.sub('(\\:(-)?\\)|\\:(-)?\\(|<3|\\:(-)?\\/|\\:-\\/|\\:(-)?\\||\\:(-)?[pP]|\\s\\:+(-)?([0-9])?\\s|\\^\\^|\\s\\:+(-)?(\\D)?\\s)', '', new_text)  # removing smile with :\n",
        "  new_text = new_text.replace('1st', 'first')\n",
        "  new_text = new_text.replace('2nd', 'second')  \n",
        "  new_text = re.sub('xké|xkè|xchè|xke|xche|perche|perché', 'perchè',new_text, flags=re.IGNORECASE)\n",
        "  new_text = re.sub('xo|xò', 'però',new_text, flags=re.IGNORECASE) \n",
        "  return new_text\n",
        "\n",
        "print(remove_unwanted_text('Do the sum of 34.5 and 65,7'))\n",
        "print(remove_unwanted_text('Please call me at 3331234567!'))\n",
        "print(remove_unwanted_text('Your new car is fantastic :) :-) :P'))\n",
        "print(remove_unwanted_text('The 1st classified is Luca'))\n",
        "print(remove_unwanted_text('Ma xke dici così?'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do the sum of 34 and 65\n",
            "Please call me at <mobilephone>!\n",
            "Your new car is fantastic \n",
            "The first classified is Luca\n",
            "Ma perchè dici così?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7GhthmPkbDo"
      },
      "source": [
        "### Tokenization\n",
        "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. \n",
        "\n",
        "And now, let's start to use **nltk** library. Natural language toolkit (NLTK) is the most popular library for natural language processing (NLP) which was written in Python and has a big community behind it.\n",
        "\n",
        "NLTK also is very easy to learn, actually, it’s the easiest natural language processing (NLP) library that you’ll use.\n",
        "\n",
        "Natural Language toolkit has very important module `tokenize`.\n",
        "\n",
        "We will compare nltk in these applications with a much more complex and articulated NLP library, used for much more advanced things: **spacy**. \n",
        "\n",
        "\n",
        "#### Tokenization - Sentences \n",
        "\n",
        "> In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56_4DlZFbsQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd2b234-ce49-4a19-fb53-cca5b0c1e996"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KuRgUsrb6-g"
      },
      "source": [
        "text = \"In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoTyn_JJc1d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b576d8-4030-4211-dabf-3a2c00f9d1ab"
      },
      "source": [
        "sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In a hole in the ground there lived a hobbit.',\n",
              " 'Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6-Tp13Mc_up"
      },
      "source": [
        "And now with **spacy**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_4WE2RHd0vJ"
      },
      "source": [
        "import spacy \n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xaRyDLDf1Gv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901a8948-f885-40f3-f366-99effd91ba6a"
      },
      "source": [
        "doc = nlp(text)\n",
        "for s in doc.sents:\n",
        "  print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a hole in the ground there lived a hobbit.\n",
            "Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat\n",
            ": it was a hobbit-hole, and that means comfort.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZCpTFt0gBs_"
      },
      "source": [
        "\n",
        "#### Tokenization - Words\n",
        "\n",
        "Word tokenization: Split a text into individual words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSg9GoU2XeBn"
      },
      "source": [
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QDpyHjfgFBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01507fd7-b517-49af-adbf-5db4852c2efc"
      },
      "source": [
        "text = \"In a hole in the ground there lived a hobbit.\"\n",
        "word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'a',\n",
              " 'hole',\n",
              " 'in',\n",
              " 'the',\n",
              " 'ground',\n",
              " 'there',\n",
              " 'lived',\n",
              " 'a',\n",
              " 'hobbit',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzMkn1H8rV6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a6d0e9-a1d6-461d-e4c8-0f155c0a9337"
      },
      "source": [
        "doc = nlp(text)\n",
        "for word in doc:\n",
        "  print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In\n",
            "a\n",
            "hole\n",
            "in\n",
            "the\n",
            "ground\n",
            "there\n",
            "lived\n",
            "a\n",
            "hobbit\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8OjsfuVWsmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f994c1e8-c89a-4300-8a87-0aeea621e8e2"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+') # It removes punctuations\n",
        "tokenizer.tokenize(\"So lucky! I won a lottery. Am I the 1st?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['So', 'lucky', 'I', 'won', 'a', 'lottery', 'Am', 'I', 'the', '1st']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "764YILONqswx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7016e5-155f-4174-9dd9-ff2e2751a292"
      },
      "source": [
        "# Instantiate tokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+') \n",
        "tokenizer_1 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+') \n",
        "tokenizer_2 = RegexpTokenizer('[A-Z]\\w+') #only words that begin with a capital letter \n",
        "\n",
        "text = \"So lucky! I won a lottery. Am I the 1st?\"\n",
        "print(tokenizer.tokenize(text))\n",
        "print(tokenizer_1.tokenize(text))\n",
        "print(tokenizer_2.tokenize(text))\n",
        "print([t for t in text.split()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['So', 'lucky', 'I', 'won', 'a', 'lottery', 'Am', 'I', 'the', '1st']\n",
            "['So', 'lucky', '!', 'I', 'won', 'a', 'lottery', '.', 'Am', 'I', 'the', '1st', '?']\n",
            "['So', 'Am']\n",
            "['So', 'lucky!', 'I', 'won', 'a', 'lottery.', 'Am', 'I', 'the', '1st?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXzo97kYiG-e"
      },
      "source": [
        "Can’t simply split on spaces.\n",
        "\n",
        "“They aren’t here.”  --> \"They\" \"are\" \"n't\" \"here\"\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTHdM3UFiWWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4afa7b7-0f4c-4daf-a33a-95d3b8326bed"
      },
      "source": [
        "print(word_tokenize(\"They aren’t here.\"))\n",
        "doc = nlp(\"They aren’t here.\")\n",
        "for word in doc:\n",
        "  print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['They', 'aren', '’', 't', 'here', '.']\n",
            "They\n",
            "are\n",
            "n’t\n",
            "here\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeOU0vYritKb"
      },
      "source": [
        "### N-Gram\n",
        "\n",
        "An n-gram is a subsequence of n elements of a given sequence.\n",
        "\n",
        "> In a hole in the ground there lived a hobbit. \n",
        "\n",
        "Unigrams - 1 word: “In”, “a”, “hole”, “in”, ...\n",
        "\n",
        "Bigrams - 2 words: “In a”, “a hole”, “hole in”, ...\n",
        "\n",
        "Trigrams - 3 words: “In a hole”, “a hole in”, ...  \n",
        "Etc…\n",
        "\n",
        "Applications: **Keywords extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEFn8KtajepD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1da7fc3-fd94-4e4b-9019-1e738b7fd763"
      },
      "source": [
        "from nltk import ngrams\n",
        "\n",
        "sentence = 'In a hole in the ground there lived a hobbit.'\n",
        "\n",
        "n = 6\n",
        "sixgrams = ngrams(sentence.split(), n)\n",
        "\n",
        "for grams in sixgrams:\n",
        "  print(grams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('In', 'a', 'hole', 'in', 'the', 'ground')\n",
            "('a', 'hole', 'in', 'the', 'ground', 'there')\n",
            "('hole', 'in', 'the', 'ground', 'there', 'lived')\n",
            "('in', 'the', 'ground', 'there', 'lived', 'a')\n",
            "('the', 'ground', 'there', 'lived', 'a', 'hobbit.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEvsbSXuwZD0"
      },
      "source": [
        "### Remove stop words\n",
        "\"*Stop words*\" are the most common words in a language like \"the\", \"a\", \"on\", \"is\", \"all\". These words do not carry important meaning and are usually removed from texts\n",
        "\n",
        "We imported a list of the most frequently used words from the NL Toolkit at the beginning with `from nltk.corpus import stopwords`. You can run `stopwords.word(insert language)` to get a full list for every language. There are 179 English words, including ‘i’, ‘me’, ‘my’, ‘myself’, ‘we’, ‘you’, ‘he’, ‘his’, for example.\n",
        "\n",
        "They are **filtered** from the text for **word based approaches**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s3sy9ru0hcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2f3082-169f-4a8c-bbc2-a6e125b12d0c"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words(\"english\") \n",
        "stopwords.words(\"italian\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ad',\n",
              " 'al',\n",
              " 'allo',\n",
              " 'ai',\n",
              " 'agli',\n",
              " 'all',\n",
              " 'agl',\n",
              " 'alla',\n",
              " 'alle',\n",
              " 'con',\n",
              " 'col',\n",
              " 'coi',\n",
              " 'da',\n",
              " 'dal',\n",
              " 'dallo',\n",
              " 'dai',\n",
              " 'dagli',\n",
              " 'dall',\n",
              " 'dagl',\n",
              " 'dalla',\n",
              " 'dalle',\n",
              " 'di',\n",
              " 'del',\n",
              " 'dello',\n",
              " 'dei',\n",
              " 'degli',\n",
              " 'dell',\n",
              " 'degl',\n",
              " 'della',\n",
              " 'delle',\n",
              " 'in',\n",
              " 'nel',\n",
              " 'nello',\n",
              " 'nei',\n",
              " 'negli',\n",
              " 'nell',\n",
              " 'negl',\n",
              " 'nella',\n",
              " 'nelle',\n",
              " 'su',\n",
              " 'sul',\n",
              " 'sullo',\n",
              " 'sui',\n",
              " 'sugli',\n",
              " 'sull',\n",
              " 'sugl',\n",
              " 'sulla',\n",
              " 'sulle',\n",
              " 'per',\n",
              " 'tra',\n",
              " 'contro',\n",
              " 'io',\n",
              " 'tu',\n",
              " 'lui',\n",
              " 'lei',\n",
              " 'noi',\n",
              " 'voi',\n",
              " 'loro',\n",
              " 'mio',\n",
              " 'mia',\n",
              " 'miei',\n",
              " 'mie',\n",
              " 'tuo',\n",
              " 'tua',\n",
              " 'tuoi',\n",
              " 'tue',\n",
              " 'suo',\n",
              " 'sua',\n",
              " 'suoi',\n",
              " 'sue',\n",
              " 'nostro',\n",
              " 'nostra',\n",
              " 'nostri',\n",
              " 'nostre',\n",
              " 'vostro',\n",
              " 'vostra',\n",
              " 'vostri',\n",
              " 'vostre',\n",
              " 'mi',\n",
              " 'ti',\n",
              " 'ci',\n",
              " 'vi',\n",
              " 'lo',\n",
              " 'la',\n",
              " 'li',\n",
              " 'le',\n",
              " 'gli',\n",
              " 'ne',\n",
              " 'il',\n",
              " 'un',\n",
              " 'uno',\n",
              " 'una',\n",
              " 'ma',\n",
              " 'ed',\n",
              " 'se',\n",
              " 'perché',\n",
              " 'anche',\n",
              " 'come',\n",
              " 'dov',\n",
              " 'dove',\n",
              " 'che',\n",
              " 'chi',\n",
              " 'cui',\n",
              " 'non',\n",
              " 'più',\n",
              " 'quale',\n",
              " 'quanto',\n",
              " 'quanti',\n",
              " 'quanta',\n",
              " 'quante',\n",
              " 'quello',\n",
              " 'quelli',\n",
              " 'quella',\n",
              " 'quelle',\n",
              " 'questo',\n",
              " 'questi',\n",
              " 'questa',\n",
              " 'queste',\n",
              " 'si',\n",
              " 'tutto',\n",
              " 'tutti',\n",
              " 'a',\n",
              " 'c',\n",
              " 'e',\n",
              " 'i',\n",
              " 'l',\n",
              " 'o',\n",
              " 'ho',\n",
              " 'hai',\n",
              " 'ha',\n",
              " 'abbiamo',\n",
              " 'avete',\n",
              " 'hanno',\n",
              " 'abbia',\n",
              " 'abbiate',\n",
              " 'abbiano',\n",
              " 'avrò',\n",
              " 'avrai',\n",
              " 'avrà',\n",
              " 'avremo',\n",
              " 'avrete',\n",
              " 'avranno',\n",
              " 'avrei',\n",
              " 'avresti',\n",
              " 'avrebbe',\n",
              " 'avremmo',\n",
              " 'avreste',\n",
              " 'avrebbero',\n",
              " 'avevo',\n",
              " 'avevi',\n",
              " 'aveva',\n",
              " 'avevamo',\n",
              " 'avevate',\n",
              " 'avevano',\n",
              " 'ebbi',\n",
              " 'avesti',\n",
              " 'ebbe',\n",
              " 'avemmo',\n",
              " 'aveste',\n",
              " 'ebbero',\n",
              " 'avessi',\n",
              " 'avesse',\n",
              " 'avessimo',\n",
              " 'avessero',\n",
              " 'avendo',\n",
              " 'avuto',\n",
              " 'avuta',\n",
              " 'avuti',\n",
              " 'avute',\n",
              " 'sono',\n",
              " 'sei',\n",
              " 'è',\n",
              " 'siamo',\n",
              " 'siete',\n",
              " 'sia',\n",
              " 'siate',\n",
              " 'siano',\n",
              " 'sarò',\n",
              " 'sarai',\n",
              " 'sarà',\n",
              " 'saremo',\n",
              " 'sarete',\n",
              " 'saranno',\n",
              " 'sarei',\n",
              " 'saresti',\n",
              " 'sarebbe',\n",
              " 'saremmo',\n",
              " 'sareste',\n",
              " 'sarebbero',\n",
              " 'ero',\n",
              " 'eri',\n",
              " 'era',\n",
              " 'eravamo',\n",
              " 'eravate',\n",
              " 'erano',\n",
              " 'fui',\n",
              " 'fosti',\n",
              " 'fu',\n",
              " 'fummo',\n",
              " 'foste',\n",
              " 'furono',\n",
              " 'fossi',\n",
              " 'fosse',\n",
              " 'fossimo',\n",
              " 'fossero',\n",
              " 'essendo',\n",
              " 'faccio',\n",
              " 'fai',\n",
              " 'facciamo',\n",
              " 'fanno',\n",
              " 'faccia',\n",
              " 'facciate',\n",
              " 'facciano',\n",
              " 'farò',\n",
              " 'farai',\n",
              " 'farà',\n",
              " 'faremo',\n",
              " 'farete',\n",
              " 'faranno',\n",
              " 'farei',\n",
              " 'faresti',\n",
              " 'farebbe',\n",
              " 'faremmo',\n",
              " 'fareste',\n",
              " 'farebbero',\n",
              " 'facevo',\n",
              " 'facevi',\n",
              " 'faceva',\n",
              " 'facevamo',\n",
              " 'facevate',\n",
              " 'facevano',\n",
              " 'feci',\n",
              " 'facesti',\n",
              " 'fece',\n",
              " 'facemmo',\n",
              " 'faceste',\n",
              " 'fecero',\n",
              " 'facessi',\n",
              " 'facesse',\n",
              " 'facessimo',\n",
              " 'facessero',\n",
              " 'facendo',\n",
              " 'sto',\n",
              " 'stai',\n",
              " 'sta',\n",
              " 'stiamo',\n",
              " 'stanno',\n",
              " 'stia',\n",
              " 'stiate',\n",
              " 'stiano',\n",
              " 'starò',\n",
              " 'starai',\n",
              " 'starà',\n",
              " 'staremo',\n",
              " 'starete',\n",
              " 'staranno',\n",
              " 'starei',\n",
              " 'staresti',\n",
              " 'starebbe',\n",
              " 'staremmo',\n",
              " 'stareste',\n",
              " 'starebbero',\n",
              " 'stavo',\n",
              " 'stavi',\n",
              " 'stava',\n",
              " 'stavamo',\n",
              " 'stavate',\n",
              " 'stavano',\n",
              " 'stetti',\n",
              " 'stesti',\n",
              " 'stette',\n",
              " 'stemmo',\n",
              " 'steste',\n",
              " 'stettero',\n",
              " 'stessi',\n",
              " 'stesse',\n",
              " 'stessimo',\n",
              " 'stessero',\n",
              " 'stando']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtEefjzV1WX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6cdae6b-041d-404e-c214-4185d439e331"
      },
      "source": [
        "def remove_stopwords(text, lang): \n",
        "    stop_words = set(stopwords.words(lang)) \n",
        "    word_tokens = tokenizer.tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
        "    return filtered_text \n",
        "  \n",
        "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
        "remove_stopwords(example_text, \"english\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM4R4sXukuj9"
      },
      "source": [
        "\n",
        "But the stop-words can also be **other** than those generally considered as such, in fact the verbs for example can be carriers of much meaning for our use case.\n",
        "\n",
        "For example, in our sentence `In a hole in the ground there lived a hobbit.`.\n",
        "\n",
        "We can decide that our stopwords are\n",
        "\n",
        "> hole, ground, lived, hobbit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyBtCDPIkvDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ece2f2a-8ecb-466f-b3c1-aaebafe58f06"
      },
      "source": [
        "#stop = set(stopwords.words('english'))\n",
        "stop = set([\"hole\",\"ground\",\"lived\",\"hobbit\"]) \n",
        "tokenized = nltk.word_tokenize(\"In a hole in the ground there lived a hobbit.\")\n",
        "filtered = [w.lower() for w in tokenized \\\n",
        "if (not w.lower() in stop and w.isalnum())]\n",
        "print(tokenized)\n",
        "print(filtered)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit', '.']\n",
            "['in', 'a', 'in', 'the', 'there', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D3iTxKf31qX"
      },
      "source": [
        "### Stemming & Lemmatizing\n",
        "Both tools shorten words back to their **root form**. \n",
        "\n",
        "* **Stemming** is a little more aggressive. It cuts off prefixes and/or endings of words based on common ones. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. It can sometimes be helpful, but not always because often times the new word is so much a root that it loses its actual meaning.\n",
        "\n",
        "* **Lemmatizing**, on the other hand, maps common words into one base. Unlike stemming though, it always still returns a proper word that can be found in the dictionary.\n",
        "\n",
        "\n",
        "I am preferring lemmatizing than stemming because I could extract the word meaning from the context in the sentence (e.g. distinguish between a verb and a noun) and obtain words that exist in the language, rather than roots of those words that don't usually have a meaning.\n",
        "\n",
        "**There is no stemming in Spacy.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eY3FwZe4lzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5046183-0619-4720-aa3b-8b514d58f190"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxpIB5mWoXOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9badbcad-5b80-43b7-f484-4d14f7e6471f"
      },
      "source": [
        "porter = PorterStemmer()\n",
        "tokenized = nltk.word_tokenize(\"We don't want any adventures here, thank you!\")\n",
        "for word in tokenized:\n",
        "    print(porter.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We\n",
            "do\n",
            "n't\n",
            "want\n",
            "ani\n",
            "adventur\n",
            "here\n",
            ",\n",
            "thank\n",
            "you\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMo8YKL-464r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8cdf5a-9eb3-4402-f239-e88c48939ebe"
      },
      "source": [
        "# Instantiate lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Instantiate stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def word_lemmatizer(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  lem_text = [lemmatizer.lemmatize(i) for i in tokens]\n",
        "  return lem_text\n",
        "\n",
        "def word_stemmer(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  stem_text = [stemmer.stem(i) for i in tokens]\n",
        "  return stem_text\n",
        "\n",
        "print(word_lemmatizer(\"We don't want any adventures here, thank you!\"))\n",
        "print(word_stemmer(\"We don't want any adventures here, thank you!\"))\n",
        "\n",
        "print(word_lemmatizer('data science uses scientific methods algorithms and many types of processes'))\n",
        "print(word_stemmer('data science uses scientific methods algorithms and many types of processes'))\n",
        "\n",
        "print(word_lemmatizer('been had done languages cities mice'))\n",
        "print(word_stemmer('been had done languages cities mice'))\n",
        "\n",
        "print(word_lemmatizer('yesterday I studied the mouse cities'))\n",
        "print(word_stemmer('yesterday I studied the mouse cities'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'don', 't', 'want', 'any', 'adventure', 'here', 'thank', 'you']\n",
            "['We', 'don', 't', 'want', 'ani', 'adventur', 'here', 'thank', 'you']\n",
            "['data', 'science', 'us', 'scientific', 'method', 'algorithm', 'and', 'many', 'type', 'of', 'process']\n",
            "['data', 'scienc', 'use', 'scientif', 'method', 'algorithm', 'and', 'mani', 'type', 'of', 'process']\n",
            "['been', 'had', 'done', 'language', 'city', 'mouse']\n",
            "['been', 'had', 'done', 'languag', 'citi', 'mice']\n",
            "['yesterday', 'I', 'studied', 'the', 'mouse', 'city']\n",
            "['yesterday', 'I', 'studi', 'the', 'mous', 'citi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV9l2YVLret2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515219a9-5c2f-42e8-c148-bee36ba595c0"
      },
      "source": [
        "# Lemmatization with Spacy, we can start to see a POS-tagging action.\n",
        "nlp = en_core_web_sm.load()\n",
        "doc = nlp(\"We don't want any adventures here, thank you!\")\n",
        "for word in doc:\n",
        "    print(word.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-PRON-\n",
            "do\n",
            "not\n",
            "want\n",
            "any\n",
            "adventure\n",
            "here\n",
            ",\n",
            "thank\n",
            "-PRON-\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnU0Jhio-o2v"
      },
      "source": [
        "And for other languages? Let's try with Italian!!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVjJp2vl-sY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278019a4-b88b-4a35-eeff-d348687c09c3"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "print(\" \".join(SnowballStemmer.languages)) # See which languages are supported"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLqJ3Le0CGzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac8c223b-c4c8-472b-c5d1-925f8aa27c2d"
      },
      "source": [
        "arabic_stemmer = SnowballStemmer(language='arabic', ignore_stopwords=False)\n",
        "arabic_stemmer.stem(\"شكرا  الله يحفظك ويحميك ويحرص يرص عليك\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'شكرا  الله يحفظك ويحميك ويحرص يرص عل'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktkk2ZcYEH6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66802154-5fcd-478d-db3b-4360f009d916"
      },
      "source": [
        "ita_stemmer = nltk.stem.snowball.ItalianStemmer()\n",
        "ita_text = \"Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio la pizza con le verdure.\"\n",
        "print(\" \".join([ita_stemmer.stem(i) for i in tokenizer.tokenize(ita_text)])) \n",
        "#print(\" \".join([ita_stemmer.stem(i) for i in tokenizer_1.tokenize(ita_text)])) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ier son andat in due supermerc oggi vol andar all ippodrom staser mang la pizz con le verdur\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEbuYdPkFsBr"
      },
      "source": [
        "### POS (Part Of Speech) tagging \n",
        "\n",
        "*Ci sei alle sei?*\n",
        "\n",
        "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context.\n",
        "\n",
        "part of speech general tags: Noun (N), Verb (V), Adjective(ADJ), Adverb (ADV), Preposition (P), Conjunction (CON), Pronoun(PRO), Interjection (INT)\n",
        "\n",
        "\n",
        "![alt text](https://cdn-media-1.freecodecamp.org/images/1*f6e0uf5PX17pTceYU4rbCA.jpeg)\n",
        "\n",
        "\n",
        "POS tagging is one of the fundamental tasks of natural language processing tasks.\n",
        "Other than the usage mentioned in the other answers here, I have one important use for POS tagging **Word Sense Disambiguation**.\n",
        "\n",
        "Words often occur in different senses as different parts of speech. For eg.\n",
        "\n",
        "She saw a `bear`.\n",
        "\n",
        "Your efforts will `bear` fruit.\n",
        "\n",
        "Ci `sei` alle `sei`?\n",
        "\n",
        "The word bear in the above sentences has completely different senses, but more importantly one is a noun and other is a verb.\n",
        "A basic word sense disambiguation is possible if you can tag words with their POS.\n",
        "\n",
        "There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core.\n",
        "\n",
        "\n",
        "[SpaCy official Page](https://spacy.io/)\n",
        "\n",
        "spaCy is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7jbWC3ued45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ddbe5e-55aa-4889-a032-1a9d5bd90223"
      },
      "source": [
        "import pprint\n",
        "\n",
        "!python -m spacy download en_core_web_sm # These are the pretrained pos-tagging and ner models for english and italian\n",
        "!python -m spacy download it\n",
        "import spacy\n",
        "import it_core_news_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp_ita = spacy.load('it')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting it_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.2.5/it_core_news_sm-2.2.5.tar.gz (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from it_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2.10)\n",
            "Building wheels for collected packages: it-core-news-sm\n",
            "  Building wheel for it-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for it-core-news-sm: filename=it_core_news_sm-2.2.5-py3-none-any.whl size=14471129 sha256=8f2f5a1a4796813a99e93931797a206e838f9c2e46c935397550aed798cbdb73\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l6vvj6qu/wheels/87/88/46/36fd0cabbebd89b2ee247bf113c1ca4f2cb184f8b7a6758ba2\n",
            "Successfully built it-core-news-sm\n",
            "Installing collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/it_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/it\n",
            "You can now load the model via spacy.load('it')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kICwBP6DI3YQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc907f0-892a-4c42-8be8-74172f03e76d"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "\n",
        "tagged_sent = nltk.pos_tag(tokenizer.tokenize(\"Can you please buy me an Arizona Ice Tea, please? It's $0.99., please?\")) # See https://www.nltk.org/book/ch05.html for tags legend\n",
        "\n",
        "pp.pprint(tagged_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[ ('Can', 'MD'),\n",
            "  ('you', 'PRP'),\n",
            "  ('please', 'VB'),\n",
            "  ('buy', 'VB'),\n",
            "  ('me', 'PRP'),\n",
            "  ('an', 'DT'),\n",
            "  ('Arizona', 'NNP'),\n",
            "  ('Ice', 'NNP'),\n",
            "  ('Tea', 'NNP'),\n",
            "  ('please', 'VB'),\n",
            "  ('It', 'PRP'),\n",
            "  ('s', 'VBZ'),\n",
            "  ('0', 'CD'),\n",
            "  ('99', 'CD'),\n",
            "  ('please', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-fuVh1VxWtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5599f8-7167-40a7-e29f-7e5db048a70e"
      },
      "source": [
        "doc = nlp(\"Can you please buy me an Arizona Ice Tea? It's $0.99.\")\n",
        "for token in doc:\n",
        "   print(token.text, token.lemma_, token.pos_) #See https://spacy.io/api/annotation for tags legend"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can Can VERB\n",
            "you -PRON- PRON\n",
            "please please INTJ\n",
            "buy buy VERB\n",
            "me -PRON- PRON\n",
            "an an DET\n",
            "Arizona Arizona PROPN\n",
            "Ice Ice PROPN\n",
            "Tea Tea PROPN\n",
            "? ? PUNCT\n",
            "It -PRON- PRON\n",
            "'s be AUX\n",
            "$ $ SYM\n",
            "0.99 0.99 NUM\n",
            ". . PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDjFri7rLBvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17abbe27-cc27-4006-a60e-e41eb3cbdce6"
      },
      "source": [
        "nltk.pos_tag(tokenizer.tokenize(\"Can you please buy me an Arizona Ice Tea? It's $0.99.\".lower())) ## 'Arizona' becomes a JJ (adjective) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('can', 'MD'),\n",
              " ('you', 'PRP'),\n",
              " ('please', 'VB'),\n",
              " ('buy', 'VB'),\n",
              " ('me', 'PRP'),\n",
              " ('an', 'DT'),\n",
              " ('arizona', 'JJ'),\n",
              " ('ice', 'NN'),\n",
              " ('tea', 'NN'),\n",
              " ('it', 'PRP'),\n",
              " ('s', 'VBD'),\n",
              " ('0', 'CD'),\n",
              " ('99', 'CD')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xbJmJZRfxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7649658-1eb4-4d27-bfc7-655661e33873"
      },
      "source": [
        "#Now for the italian\n",
        "tagged_sent_ita = nltk.pos_tag(tokenizer.tokenize(\"Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo\"), lang=\"italian\") \n",
        "pp.pprint(tagged_sent_ita)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ ('Ieri', 'NNP'),\n",
            "  ('sono', 'NN'),\n",
            "  ('andato', 'NN'),\n",
            "  ('in', 'IN'),\n",
            "  ('due', 'JJ'),\n",
            "  ('supermercati', 'NNS'),\n",
            "  ('Oggi', 'NNP'),\n",
            "  ('volevo', 'NN'),\n",
            "  ('andare', 'NN'),\n",
            "  ('all', 'DT'),\n",
            "  ('ippodromo', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcPO5OkQyfbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014d587e-d7ba-47ab-a6b6-a628b7906775"
      },
      "source": [
        "doc_ita = nlp_ita(\"Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo\")\n",
        "for token in doc_ita:\n",
        "   print(token.text, token.lemma_, token.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ieri Ieri ADV\n",
            "sono essere AUX\n",
            "andato andare VERB\n",
            "in in ADP\n",
            "due due NUM\n",
            "supermercati supermercato NOUN\n",
            ". . PUNCT\n",
            "Oggi Oggi ADV\n",
            "volevo volere AUX\n",
            "andare andare VERB\n",
            "all' alla SCONJ\n",
            "ippodromo ippodromo PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv2IasgyMnZE"
      },
      "source": [
        "Seems easy?? \n",
        "\n",
        "**Algorithms for PoS tagging**\n",
        "\n",
        "*  Rule-based taggers \n",
        "   \n",
        "  \"manual\" creation of a large database of rules which\n",
        "   specify for ambiguous cases the conditions to be verified for\n",
        "  the assignment of every possible tag\n",
        "   E.g. a word is a noun if it is preceded by an article\n",
        "\n",
        "\n",
        "*  Probabilistic taggers (HMM, CRF)\n",
        " \n",
        "  They generally solve ambibuity by estimating the probability that a specific\n",
        "  word has a given tag in a given context using a dataset of reference\n",
        "\n",
        "* Other approaches\n",
        "\n",
        "  * Tagging problem as a classification problem (each tag corresponds to a    class and a classifier processes text features to describe the context)\n",
        "   \n",
        "  * Rules-based taggers learned from examples\n",
        "\n",
        "\n",
        "State of the art of POS-Tagging is the **BI-LSTM-CRF** model for sequence labeling. \n",
        "\n",
        "(Some tutorial to develop a state of the art POS-tagging with Keras)\n",
        "\n",
        "https://github.com/Hironsan/anago\n",
        "\n",
        "https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/\n",
        "\n",
        "https://nlpforhackers.io/lstm-pos-tagger-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMbqPpcVjp9y"
      },
      "source": [
        "### NER (Named Entity Recognition)\n",
        "\n",
        "<center><img src='https://miro.medium.com/max/725/1*i8IfPgFDFVAIqXBnIdg5yg.jpeg' width=\"340\" height=\"240\"></center>\n",
        "\n",
        "\n",
        "NER, short for Named Entity Recognition is probably the first step towards **Information Extraction from unstructured text**(POS Tagging is still more data augmentation than information extraction). \n",
        "It basically means extracting what is a real world entity from the text (Person,Organization, Event etc …).\n",
        "\n",
        "<center><img src='https://miro.medium.com/max/1171/1*OZaHa-z7A4Xny3dN1qbsQg.png\n",
        "' width=\"630\" height=\"210\"></center>\n",
        "\n",
        "Few Use-Cases of Named Entity Recognition:\n",
        "\n",
        "* News and Blog Post Classification \n",
        "\n",
        "* Efficient Search Algorithms ( What would happen if we were to search for a word in a blog of 10,000 articles? The system should search within each of them, and since it has no memory, repeat this operation with each new query.\n",
        "\n",
        "Such a system, in addition to being poorly scalable, proves to be inefficient.\n",
        "\n",
        "The NER solves this problem by analyzing each article only once, extracting the key words and populating a list of named entities that can be used by search queries.)\n",
        "\n",
        "\n",
        "* Customer Support (We apply the NER to the texts so as to properly sort the message to the most relevant department)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySFxJb8enN7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01a73a1-6931-438d-ba6c-a3d30d93541c"
      },
      "source": [
        "text = \"\"\"London is the capital and most populous city of England and \n",
        "the United Kingdom.  Standing on the River Thames in the south east \n",
        "of the island of Great Britain, London has been a major settlement \n",
        "for two millennia. It was founded by the Romans, who named it Londinium.\n",
        "\\n\n",
        "Bill works for Apple so he went to Boston for a conference.\n",
        "\"\"\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for entity in doc.ents:    \n",
        "    spacy_expl=spacy.explain(entity.label_)\n",
        "    print(f\"{entity.text} ({entity.label_} : {spacy_expl} )\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "London (GPE : Countries, cities, states )\n",
            "England (GPE : Countries, cities, states )\n",
            "the United Kingdom (GPE : Countries, cities, states )\n",
            "the south east (LOC : Non-GPE locations, mountain ranges, bodies of water )\n",
            "Great Britain (GPE : Countries, cities, states )\n",
            "London (GPE : Countries, cities, states )\n",
            "two (CARDINAL : Numerals that do not fall under another type )\n",
            "Romans (NORP : Nationalities or religious or political groups )\n",
            "Londinium (ORG : Companies, agencies, institutions, etc. )\n",
            "Bill (PERSON : People, including fictional )\n",
            "Apple (ORG : Companies, agencies, institutions, etc. )\n",
            "Boston (GPE : Countries, cities, states )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkmWt-lBo-vD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "09aa9d4e-e1a7-4d51-fbb9-22ccd22a9599"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    London\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " is the capital and most populous city of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    England\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and </br>\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the United Kingdom\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".  Standing on the River Thames in \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the south east\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " </br>of the island of \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Great Britain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    London\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " has been a major settlement </br>for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    two\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " millennia. It was founded by the \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Romans\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ", who named it \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Londinium\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</br></br></br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bill\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " works for \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " so he went to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Boston\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " for a conference.\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hSTP-j1puhh"
      },
      "source": [
        "* PERSON\tPeople, including fictional.\n",
        "* NORP\tNationalities or religious or political groups.\n",
        "* FAC\tBuildings, airports, highways, bridges, etc.\n",
        "* ORG\tCompanies, agencies, institutions, etc.\n",
        "* GPE\tCountries, cities, states.\n",
        "* LOC\tNon-GPE locations, mountain ranges, bodies of water.\n",
        "* PRODUCT\tObjects, vehicles, foods, etc. (Not services.)\n",
        "* EVENT\tNamed hurricanes, battles, wars, sports events, etc.\n",
        "* WORK_OF_ART\tTitles of books, songs, etc.\n",
        "* LAW\tNamed documents made into laws.\n",
        "* LANGUAGE\tAny named language.\n",
        "* DATE\tAbsolute or relative dates or periods.\n",
        "* TIME\tTimes smaller than a day.\n",
        "* PERCENT\tPercentage, including ”%“.\n",
        "* MONEY\tMonetary values, including unit.\n",
        "* QUANTITY\tMeasurements, as of weight or distance.\n",
        "* ORDINAL\t“first”, “second”, etc.\n",
        "* CARDINAL\tNumerals that do not fall under another type.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhPWDC-vrEzN"
      },
      "source": [
        "## *EXERCISE: Why don't you see how it performs with Italian language?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ne1OGVQrWf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "fb328019-de78-4761-c713-0d15f4d59ecb"
      },
      "source": [
        "text_ita = \"\"\"Mattia è un bimbo di 5 anni che passa tutte le sue giornate a disegnare. \n",
        "Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. Stasera mangio \n",
        "la pizza con le verdure.  Che tempo fara' a Bastia Umbra\"\"\" \n",
        "\n",
        "doc_ita = nlp_ita(text_ita)\n",
        "for entity in doc_ita.ents:\n",
        "    #print(spacy.explain(entity.label_))\n",
        "    spacy_expl=spacy.explain(entity.label_)\n",
        "    print(f\"{entity.text} ({entity.label_} : {spacy_expl} )\")\n",
        "  \n",
        "displacy.render(doc_ita, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mattia (MISC : Miscellaneous entities, e.g. events, nationalities, products or works of art )\n",
            "Stasera (MISC : Miscellaneous entities, e.g. events, nationalities, products or works of art )\n",
            "Bastia Umbra (LOC : Non-GPE locations, mountain ranges, bodies of water )\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mattia\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " è un bimbo di 5 anni che passa tutte le sue giornate a disegnare. </br>Ieri sono andato in due supermercati. Oggi volevo andare all'ippodromo. \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Stasera\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " mangio </br>la pizza con le verdure.  Che tempo fara' a \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bastia Umbra\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T5HoViz108T"
      },
      "source": [
        "And if we make lowercase the sentence?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edn5IdFr2BVK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkRCb433U0iu"
      },
      "source": [
        "That sounds easy too, doesn't it?\n",
        "\n",
        "* *Classical Approaches*\n",
        "\n",
        "  mostly rule-based. ([Tutorial: How use NLTK to create a rule-based NER](https://www.youtube.com/watch?v=LFXsG7fueyk))\n",
        "* *Machine Learning Approaches*\n",
        "  * *Multi-class Classification* (they are algorithms that ignore the context)\n",
        "  * *Conditional Random Field* (CRF) model \n",
        "    * they are models widely used to model sequential data, just like words in sentences\n",
        "    * the CRF model is able to capture the features of the current and previous labels in a sequence but it cannot understand the context of the forward labels (let's see [NER With CRF In Python](https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/) )\n",
        "* *Deep Learning Approaches* \n",
        "\n",
        "  Which type of neural network works best to tackle NER problem considering that the text is a sequential data format? Yeah, you guessed it right… Long short Term Memory (LSTM). But not any type of LSTM, we need to use bi-directional LSTMs because using a standard LSTM to make predictions will only take the “past” information in a sequence of the text into account. (Bi-LSTM e' la combinazione di due LSTM, una 'forward' da sx a dx e una 'backward' da dx a sx)  (vediamo alcuni tra gli approcci state-of-the-art)\n",
        "  * *Bidirectional LSTM-CRF* (More details and [implementation](https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/) in keras)\n",
        "  * *Bidirectional LSTM-CNNs* (More details and [implementation](https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/) in keras)\n",
        "  * *Bidirectional LSTM-CNNS-CRF* (Let's see the paper [here](https://arxiv.org/pdf/1603.01354.pdf))\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJgsYhTcrh-P"
      },
      "source": [
        "----------------\n",
        "### Bag of Words\n",
        "\n",
        "<center><img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRdXiRXvj1cwjs9_ewoAXEY2ex_HF2A5tG1HA&usqp=CAU\n",
        "'></center>\n",
        " \n",
        "\n",
        "Bag of Words (BoW) is an algorithm that **counts** how many times a word appears in a document. It’s a tally. \n",
        "\n",
        "Those word counts allow us to **compare documents** and gauge **their similarities** for applications like **search**, **document classification** and **topic modeling**. \n",
        "\n",
        "BoW is a also method for preparing text for input in a deep-learning net.\n",
        "\n",
        "BoW lists words paired with their word counts per document. \n",
        "\n",
        "In the table where the words and documents that effectively become vectors are stored:\n",
        "* each row is a word\n",
        "* each column is a document\n",
        "* each cell is a word count\n",
        "\n",
        "**Each of the documents in the corpus is represented by columns/vectors** of equal length. Those are wordcount vectors, an output stripped of context.\n",
        "\n",
        "With BoW, the order of words does not matter...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl143V5uuULW"
      },
      "source": [
        "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "Term-frequency-inverse document frequency (TF-IDF) is another way to judge the **topic** of an article by the words it contains. \n",
        "\n",
        "With TF-IDF, words are given weight – **TF-IDF measures relevance, not frequency**. \n",
        "\n",
        "That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\n",
        "\n",
        "Have a look to the slides..."
      ]
    }
  ]
}