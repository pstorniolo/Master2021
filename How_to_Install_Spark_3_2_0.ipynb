{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How_to_Install_Spark_3-2-0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMURNsAeAnXmp1hGaOADtpB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/How_to_Install_Spark_3_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOYPteoM9MtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4874d61-a7f0-4eef-c3e0-22bf7f1445ef"
      },
      "source": [
        "# Install Java 11\n",
        "!apt-get -q install openjdk-11-jdk-headless"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "openjdk-11-jdk-headless is already the newest version (11.0.11+9-0ubuntu2~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-jKl9HB9TwQ"
      },
      "source": [
        "#Install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWryKWH69a4M"
      },
      "source": [
        "#Unzip the spark file to the current folder\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_hQP7woKff_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49652209-a358-4e61-8685-b33aa2f44670"
      },
      "source": [
        "!rm -f spark-3.2.0-bin-hadoop3.2.tgz ; ls -la"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x  1 root root 4096 Oct 24 07:18 .\n",
            "drwxr-xr-x  1 root root 4096 Oct 24 07:13 ..\n",
            "drwxr-xr-x  4 root root 4096 Oct  8 13:44 .config\n",
            "drwxr-xr-x  1 root root 4096 Oct  8 13:45 sample_data\n",
            "drwxr-xr-x 13 1000 1000 4096 Oct  6 13:18 spark-3.2.0-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGS-_DhG-Dof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e349fd80-b4ed-4da0-c9db-ceb906c89748"
      },
      "source": [
        "!ls -la sample_data/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 55512\n",
            "drwxr-xr-x 1 root root     4096 Oct  8 13:45 .\n",
            "drwxr-xr-x 1 root root     4096 Oct 24 07:18 ..\n",
            "-rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json\n",
            "-rw-r--r-- 1 root root   301141 Oct  8 13:45 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Oct  8 13:45 california_housing_train.csv\n",
            "-rw-r--r-- 1 root root 18289443 Oct  8 13:45 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Oct  8 13:45 mnist_train_small.csv\n",
            "-rwxr-xr-x 1 root root      930 Jan  1  2000 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2n2w4af9x2n"
      },
      "source": [
        "#Set your spark folder to your system path environment.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRSQzMWz_NjM"
      },
      "source": [
        "#Install findspark using pip to make pyspark importable as regular library\n",
        "!pip -q install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#Spark for Python (pyspark)\n",
        "#!pip -q install pyspark==3.2.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqNnWi5w_jCu"
      },
      "source": [
        "#importing pyspark\n",
        "import pyspark\n",
        "\n",
        "#importing sparksession\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOLeta_zKRKH"
      },
      "source": [
        "#creating a sparksession object and providing appName \n",
        "spark=SparkSession.builder.appName(\"local\").getOrCreate()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijMVY5DRKZL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd8f1c8-ecc7-48a3-dbc0-b9d589a28b4a"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\",\"true\")\n",
        "\n",
        "#printing the version of spark\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Spark version:  3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFG18DC6LRc8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R00EsfD-Kmyu"
      },
      "source": [
        "# create a list of random numbers between 10 to 1000\n",
        "from random import randint\n",
        "my_large_list = [randint(10,1000) for x in range(0,5000000)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDBMVtxWKtlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3da3b51-3f31-402f-b2fa-305927775164"
      },
      "source": [
        "# create one partition of the list  \n",
        "my_large_list_one_partition = spark.sparkContext.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "4040494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-OhrSckKy31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3811d8c9-374e-46e9-933c-fdf93783b0cc"
      },
      "source": [
        "# create partitions of the list\n",
        "my_large_list_with_partition = spark.sparkContext.parallelize(my_large_list, numSlices=10)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_with_partition.getNumPartitions())\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_partition = my_large_list_with_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_partition.count())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "4040494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz1gUv3lLehO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c32862f-161d-4d40-bdaa-af63203bbe83"
      },
      "source": [
        "# create a sample list\n",
        "my_list = [i for i in range(1,10000000)]\n",
        "\n",
        "# parallelize the data\n",
        "rdd_0 = spark.sparkContext.parallelize(my_list,3)\n",
        "\n",
        "print(rdd_0)\n",
        "rdd_0.take(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi3LK5h8rio2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc969fa4-5877-4f7e-8166-d4a808dc4c69"
      },
      "source": [
        "# add value 4 to each number\n",
        "rdd_1 = rdd_0.map(lambda x: x + 4)\n",
        "\n",
        "# RDD object\n",
        "print(rdd_1)\n",
        "rdd_1.take(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[6] at RDD at PythonRDD.scala:53\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79BsWwxMG4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d85fa4-1109-4dd5-bb80-5e494448654a"
      },
      "source": [
        "# add value 20 each number\n",
        "rdd_2 = rdd_1.map(lambda x : x + 20)\n",
        "\n",
        "# RDD Object\n",
        "print(rdd_2)\n",
        "rdd_2.take(10)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[8] at RDD at PythonRDD.scala:53\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 26, 27, 28, 29, 30, 31, 32, 33, 34]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}