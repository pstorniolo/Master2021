{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How_to_Install_Spark_3-2-0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXyrlhRN8qRwPKcHjZ+3x0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/How_to_Install_Spark_3_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOYPteoM9MtX"
      },
      "source": [
        "# Install Java 11\n",
        "\n",
        "!apt-get -q install openjdk-11-jdk-headless"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-jKl9HB9TwQ"
      },
      "source": [
        "#Install spark (change the version number if needed)\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWryKWH69a4M"
      },
      "source": [
        "#Unzip the spark file to the current folder\n",
        "\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_hQP7woKff_"
      },
      "source": [
        "!rm -f *.tgz* ; ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGS-_DhG-Dof"
      },
      "source": [
        "!ls -la sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2n2w4af9x2n"
      },
      "source": [
        "#Set your spark folder to your system path environment.\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRSQzMWz_NjM"
      },
      "source": [
        "#Install findspark using pip to make pyspark importable as regular library\n",
        "#!pip -q install findspark\n",
        "\n",
        "#Spark for Python (pyspark)\n",
        "!pip -q install pyspark==3.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqNnWi5w_jCu"
      },
      "source": [
        "#importing pyspark\n",
        "import pyspark\n",
        "\n",
        "#importing sparksession\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOLeta_zKRKH"
      },
      "source": [
        "#creating a sparksession object and providing appName \n",
        "spark=SparkSession.builder.appName(\"local\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijMVY5DRKZL6"
      },
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\",\"true\")\n",
        "\n",
        "#printing the version of spark\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R00EsfD-Kmyu"
      },
      "source": [
        "from random import randint \n",
        "\n",
        "# create a list of random numbers between 10 to 1000\n",
        "my_large_list = [randint(10,1000) for x in range(0,5000000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDBMVtxWKtlD"
      },
      "source": [
        "# create one partition of the list  \n",
        "my_large_list_one_partition = spark.sparkContext.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-OhrSckKy31"
      },
      "source": [
        "# create five partitions of the list\n",
        "my_large_list_with_partition = spark.sparkContext.parallelize(my_large_list, numSlices=10)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_with_partition.getNumPartitions())\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_partition = my_large_list_with_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_partition.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz1gUv3lLehO"
      },
      "source": [
        "# create a sample list\n",
        "my_list = [i for i in range(1,10000000)]\n",
        "\n",
        "# parallelize the data\n",
        "rdd_0 = spark.sparkContext.parallelize(my_list,3)\n",
        "\n",
        "print(rdd_0)\n",
        "rdd_0.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi3LK5h8rio2"
      },
      "source": [
        "# add value 4 to each number\n",
        "rdd_1 = rdd_0.map(lambda x: x + 4)\n",
        "\n",
        "# RDD object\n",
        "print(rdd_1)\n",
        "rdd_1.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79BsWwxMG4R"
      },
      "source": [
        "# add value 20 each number\n",
        "rdd_2 = rdd_1.map(lambda x : x + 20)\n",
        "\n",
        "# RDD Object\n",
        "print(rdd_2)\n",
        "rdd_2.take(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}