{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-21-Hadoop-Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8u2WiSua1kQxVoshiJmXQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_21_Hadoop_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w48z9Ca2RKhQ"
      },
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0BsDb12RXGT"
      },
      "source": [
        "#Install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBjM356_RdMT"
      },
      "source": [
        "#Unzip the spark file to the current folder\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr3VSxtf0oBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1e720a-d507-4aba-8f42-2e3506e962fe"
      },
      "source": [
        "!cd spark-3.2.0-bin-hadoop3.2; ls -la"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 160\n",
            "drwxr-xr-x 13 1000 1000  4096 Oct  6 13:18 .\n",
            "drwxr-xr-x  1 root root  4096 Oct 23 15:41 ..\n",
            "drwxr-xr-x  2 1000 1000  4096 Oct  6 13:18 bin\n",
            "drwxr-xr-x  2 1000 1000  4096 Oct  6 13:18 conf\n",
            "drwxr-xr-x  5 1000 1000  4096 Oct  6 13:18 data\n",
            "drwxr-xr-x  4 1000 1000  4096 Oct  6 13:18 examples\n",
            "drwxr-xr-x  2 1000 1000 16384 Oct  6 13:18 jars\n",
            "drwxr-xr-x  4 1000 1000  4096 Oct  6 13:18 kubernetes\n",
            "-rw-r--r--  1 1000 1000 22878 Oct  6 13:18 LICENSE\n",
            "drwxr-xr-x  2 1000 1000  4096 Oct  6 13:18 licenses\n",
            "-rw-r--r--  1 1000 1000 57677 Oct  6 13:18 NOTICE\n",
            "drwxr-xr-x  9 1000 1000  4096 Oct  6 13:18 python\n",
            "drwxr-xr-x  3 1000 1000  4096 Oct  6 13:18 R\n",
            "-rw-r--r--  1 1000 1000  4512 Oct  6 13:18 README.md\n",
            "-rw-r--r--  1 1000 1000   167 Oct  6 13:18 RELEASE\n",
            "drwxr-xr-x  2 1000 1000  4096 Oct  6 13:18 sbin\n",
            "drwxr-xr-x  2 1000 1000  4096 Oct  6 13:18 yarn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRB8s_BMENqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4947b1-e13a-4f7e-b542-1124810c721b"
      },
      "source": [
        "!cd spark-3.2.0-bin-hadoop3.2/bin; ls -la"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 124\n",
            "drwxr-xr-x  2 1000 1000  4096 Oct  6 13:18 .\n",
            "drwxr-xr-x 13 1000 1000  4096 Oct  6 13:18 ..\n",
            "-rwxr-xr-x  1 1000 1000  1089 Oct  6 13:18 beeline\n",
            "-rw-r--r--  1 1000 1000  1064 Oct  6 13:18 beeline.cmd\n",
            "-rwxr-xr-x  1 1000 1000 10965 Oct  6 13:18 docker-image-tool.sh\n",
            "-rwxr-xr-x  1 1000 1000  1935 Oct  6 13:18 find-spark-home\n",
            "-rw-r--r--  1 1000 1000  2685 Oct  6 13:18 find-spark-home.cmd\n",
            "-rw-r--r--  1 1000 1000  2337 Oct  6 13:18 load-spark-env.cmd\n",
            "-rw-r--r--  1 1000 1000  2435 Oct  6 13:18 load-spark-env.sh\n",
            "-rwxr-xr-x  1 1000 1000  2636 Oct  6 13:18 pyspark\n",
            "-rw-r--r--  1 1000 1000  1542 Oct  6 13:18 pyspark2.cmd\n",
            "-rw-r--r--  1 1000 1000  1170 Oct  6 13:18 pyspark.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1030 Oct  6 13:18 run-example\n",
            "-rw-r--r--  1 1000 1000  1223 Oct  6 13:18 run-example.cmd\n",
            "-rwxr-xr-x  1 1000 1000  3539 Oct  6 13:18 spark-class\n",
            "-rwxr-xr-x  1 1000 1000  2812 Oct  6 13:18 spark-class2.cmd\n",
            "-rw-r--r--  1 1000 1000  1180 Oct  6 13:18 spark-class.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1039 Oct  6 13:18 sparkR\n",
            "-rw-r--r--  1 1000 1000  1097 Oct  6 13:18 sparkR2.cmd\n",
            "-rw-r--r--  1 1000 1000  1168 Oct  6 13:18 sparkR.cmd\n",
            "-rwxr-xr-x  1 1000 1000  3122 Oct  6 13:18 spark-shell\n",
            "-rw-r--r--  1 1000 1000  1818 Oct  6 13:18 spark-shell2.cmd\n",
            "-rw-r--r--  1 1000 1000  1178 Oct  6 13:18 spark-shell.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1065 Oct  6 13:18 spark-sql\n",
            "-rw-r--r--  1 1000 1000  1118 Oct  6 13:18 spark-sql2.cmd\n",
            "-rw-r--r--  1 1000 1000  1173 Oct  6 13:18 spark-sql.cmd\n",
            "-rwxr-xr-x  1 1000 1000  1040 Oct  6 13:18 spark-submit\n",
            "-rw-r--r--  1 1000 1000  1155 Oct  6 13:18 spark-submit2.cmd\n",
            "-rw-r--r--  1 1000 1000  1180 Oct  6 13:18 spark-submit.cmd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWT1FhhVRhJz"
      },
      "source": [
        "#Set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWuXmFDqRwdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8118b3bb-1699-4be3-bde9-664f53e69304"
      },
      "source": [
        "#Spark for Python (pyspark)\n",
        "!pip install -q pyspark==3.2.0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 13.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vUM_qYTR2vU"
      },
      "source": [
        "#importing pyspark\n",
        "import pyspark"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcR7zpmtR8nE"
      },
      "source": [
        "#importing sparksession\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0iIxwXySB3c"
      },
      "source": [
        "#creating a sparksession object and providing appName \n",
        "spark=SparkSession.builder.appName(\"local\").getOrCreate()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dta5v8CSTZxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76175b65-91a8-4d7f-c7fc-52f6f4e38688"
      },
      "source": [
        "#printing the version of spark\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Spark version:  3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwgAP3VKT0tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8accfbe6-7e40-404f-97e4-7b285a4c2edf"
      },
      "source": [
        "!rm -f *.tgz* ; ls -la"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20\n",
            "drwxr-xr-x  1 root root 4096 Oct 23 15:46 .\n",
            "drwxr-xr-x  1 root root 4096 Oct 23 15:42 ..\n",
            "drwxr-xr-x  4 root root 4096 Oct  8 13:44 .config\n",
            "drwxr-xr-x  1 root root 4096 Oct  8 13:45 sample_data\n",
            "drwxr-xr-x 13 1000 1000 4096 Oct  6 13:18 spark-3.2.0-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pTsZKZhU3GU"
      },
      "source": [
        "from random import randint \n",
        "\n",
        "# create a list of random numbers between 10 to 1000\n",
        "my_large_list = [randint(10,1000) for x in range(0,20000000)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1-75OQgVPlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6243c7e7-d4fc-42c1-d2bf-1315abe7b377"
      },
      "source": [
        "# create one partition of the list  \n",
        "my_large_list_one_partition = spark.sparkContext.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "16165378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfhyUxLNXiAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc84889-22d7-43be-98eb-70402b0e64ca"
      },
      "source": [
        "# create five partitions of the list\n",
        "my_large_list_with_five_partition = spark.sparkContext.parallelize(my_large_list, numSlices=5)\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_five_partition.count())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16165378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9DbB28Ydyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03cae0ce-f911-4e7c-d6ab-7b940985225b"
      },
      "source": [
        "# create a sample list\n",
        "my_list = [i for i in range(1,10000000)]\n",
        "\n",
        "# parallelize the data\n",
        "rdd_0 = spark.sparkContext.parallelize(my_list,3)\n",
        "\n",
        "rdd_0.take(10)\n",
        "\n",
        "# add value 4 to each number\n",
        "rdd_1 = rdd_0.map(lambda x: x + 4)\n",
        "\n",
        "# RDD object\n",
        "print(rdd_1)\n",
        "rdd_1.take(10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[9] at RDD at PythonRDD.scala:53\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}