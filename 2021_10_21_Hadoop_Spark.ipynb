{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-21-Hadoop-Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmzkQA1PQAuuYucKUdpXeL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_21_Hadoop_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w48z9Ca2RKhQ"
      },
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0BsDb12RXGT"
      },
      "source": [
        "#Install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBjM356_RdMT"
      },
      "source": [
        "#Unzip the spark file to the current folder\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr3VSxtf0oBg"
      },
      "source": [
        "!cd spark-3.1.2-bin-hadoop3.2; ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRB8s_BMENqq"
      },
      "source": [
        "!cd spark-3.1.2-bin-hadoop3.2/bin; ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWT1FhhVRhJz"
      },
      "source": [
        "#Set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmpjec4lRpr0"
      },
      "source": [
        "#Install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWuXmFDqRwdU"
      },
      "source": [
        "#Spark for Python (pyspark)\n",
        "!pip install -q pyspark==3.1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vUM_qYTR2vU"
      },
      "source": [
        "#importing pyspark\n",
        "import pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcR7zpmtR8nE"
      },
      "source": [
        "#importing sparksessio\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0iIxwXySB3c"
      },
      "source": [
        "#creating a sparksession object and providing appName \n",
        "spark=SparkSession.builder.appName(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dta5v8CSTZxv"
      },
      "source": [
        "#printing the version of spark\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwgAP3VKT0tz"
      },
      "source": [
        "!rm -f *.tgz* ; ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pTsZKZhU3GU"
      },
      "source": [
        "from random import randint \n",
        "\n",
        "# create a list of random numbers between 10 to 1000\n",
        "my_large_list = [randint(10,1000) for x in range(0,20000000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1-75OQgVPlI"
      },
      "source": [
        "# create one partition of the list  \n",
        "my_large_list_one_partition = spark.sparkContext.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "%time\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfhyUxLNXiAc"
      },
      "source": [
        "# create five partitions of the list\n",
        "my_large_list_with_five_partition = spark.sparkContext.parallelize(my_large_list, numSlices=5)\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "%time \n",
        "\n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_five_partition.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT9DbB28Ydyj"
      },
      "source": [
        "# create a sample list\n",
        "my_list = [i for i in range(1,10000000)]\n",
        "\n",
        "# parallelize the data\n",
        "rdd_0 = spark.sparkContext.parallelize(my_list,3)\n",
        "\n",
        "rdd_0\n",
        "\n",
        "# add value 4 to each number\n",
        "rdd_1 = rdd_0.map(lambda x : x+4)\n",
        "\n",
        "# RDD object\n",
        "print(rdd_1)\n",
        "\n",
        "# get the RDD Lineage \n",
        "print(rdd_1.toDebugString())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}