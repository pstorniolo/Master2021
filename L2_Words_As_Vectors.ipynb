{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L2_Words_As_Vectors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/L2_Words_As_Vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu26JPvXUKTR"
      },
      "source": [
        "# **Natural Language Processing from Scratch - Lesson 2**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### > Why Colab? \n",
        "Colab (Google Colaboratory) is a free cloud service based on Jupyter Notebooks that supports... FREE GPU!!! <3\n",
        "\n",
        "Lectures will be held through Colab Notebooks. To download each notebook there are few and really simple steps to do:\n",
        "\n",
        "( *only one thing is required ... having Google Drive or GitHub* )\n",
        "\n",
        "1.   Click on  https://bit.ly/3oCpBP9 \n",
        "2.   `File` > `Save a copy in Drive` / or `Save a copy on GitHub`\n",
        "3.   (Drive option) Go to your `Drive` and check if the copied version of notebook is present into `Colab Notebooks` folder \n",
        "4.   (Github option) Choose which `repository` to copy the notebook to and than `open it with Colab`\n",
        "\n",
        "## **Lesson 2 - Words as Vectors**\n",
        "<h4><center> Goal: Understand the meaning and the semantic relations between words </center></h4>\n",
        "\n",
        "So far we have talked about words, phrases, tokens, characters... \n",
        "\n",
        "But, as we well know, many Machine Learning algorithms and almost all Deep Learning architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms.\n",
        "\n",
        "For this reason it is necessary a transformation from text to numerical values, (why not vectors?), which can as much as possible keep the meaning of the word without losing all that which the word evokes in the human mind. \n",
        "\n",
        "This transformation is known as **Word Embedding**, and this linguistic model is called **Vector Semantics**.\n",
        "\n",
        "\n",
        "\n",
        ">  —  Word embedding is the collective name for a set of language modelling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. — (Wikipedia)\n",
        "\n",
        "\n",
        "At the root of everything there is an hypothesis that was first formulated in the 1950th by linguists like Joos, Harris and Firth.\n",
        "\n",
        "\n",
        "> **Distributional hypothesys**: words that occur in *similar context* tend to have *similar meanings*. \n",
        "\n",
        "In ohter words, this hypothesys creates a link between *similarity* in how words are distributed and similarity in what they mean.\n",
        "\n",
        "The model knows as Vector Semantics instantiates this hypothesis by learning representations of the meaning of words directly from their distributions in text. \n",
        "\n",
        "Why the word 'Vector'? \n",
        "\n",
        "Along with the hypothesis, another intuition is at the base of this linguistic model: Osgood in 1957 described the meaning of words on the scale of three values: *valence*, *arousal* and *dominance*. \n",
        "\n",
        "<center><img src='https://miro.medium.com/max/530/0*aZD7eiXzGmqsm2a2\n",
        "' width=\"430\" height=\"210\"></center>\n",
        "\n",
        "\n",
        "Thus words like *happy* or *satisfied* are high on valence, while *unhappy* or *annoyed* are low on valence.\n",
        "\n",
        "*Excited* or *frenzied* are high on arousal, while *relaxed* and *calm* are low on arousal.\n",
        "\n",
        "*Important* or *controlling* are high on dominance, while *awed* and *influenced* are low in dominance. \n",
        "\n",
        "Every words, for Osgood, is thus represented by three numbers, corrisponding to its value on each of a space of three dimensions.\n",
        "\n",
        "To conclude this introduction, we can say that Vector Semantics combines two intuitions:\n",
        "* *distributionalist intuition* (defining a word by counting what other words occur in its environment)\n",
        "* *vector intuition* (defining the meaning of a word $w$ as a vector, a point in a $N$-dim space.)\n",
        "\n",
        "\n",
        "\n",
        "Vectors for representing words ($w$) are generally called **embeddings**, because the word is  embedded in a particular vector space. \n",
        "\n",
        "<center><img src='https://miro.medium.com/max/956/0*OPhacyNlOUa8lf9m\n",
        "' width=\"580\" height=\"310\"></center>\n",
        "\n",
        "(in the graph you can see how words with similar meaning are automatically distributed in meaningful clustering in space, the space represented here is a space we can say 'emotional' of words) \n",
        "\n",
        "Vector models of meaning are now the standard way to represent the meaning of words in NLP. \n",
        "\n",
        "in this lecture we will introduce the two most commonly used models of representation:\n",
        "* Frequency based Embedding (sparse representation): \n",
        "  * Count Vector\n",
        "  * TF-IDF Vector\n",
        "* Prediction based Vector (dense representation) :\n",
        "  * Word2Vec (CBOW and skipgram)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJC1fn5hRNO8"
      },
      "source": [
        " ### How to measure similarity?\n",
        "To define similarity between two target words $v$ and $w$, we need a measure for taking two such vectors and giving a measure of vector similarity. By far the most common similarity metric is the **cosine** of the angle between the vectors. \n",
        "\n",
        "The cosine is based on the **dot-product** operator from linear algebra:\n",
        "$$\n",
        "\\underline{v} \\cdot \\underline{w} = \\sum_{i=1}^{N} v_{i}w_{i} = v_{1}w_{1}+v_{2}w_{2}+...+v_{N}w_{N}  \n",
        "$$\n",
        "where $\\underline{v}$,$\\underline{w}\\in{R^{N}}$.\n",
        "\n",
        "The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensios - orthogonal vectors - will have a dot product of 0, representing their strong dissimilarity.\n",
        "\n",
        "The raw dot-product, however, has a problem as a similarity metric: it favors **long** vectors.  The vector length is defined as:\n",
        "$$\n",
        "|\\underline{v}| = \\sqrt(\\sum_{i=1}^{N}v_{i}^{2})\n",
        "$$\n",
        "\n",
        "In NLP, if we decide to represent a word based on its frequency in a document, its enbedding will result in a vector as long as the word is frequent.\n",
        "\n",
        "The raw dot-product thus will be higher for frequent words.\n",
        "\n",
        "As usual in mathematics, we **normalize** the dot product in order to make the measure independent of the module of vectors. \n",
        "\n",
        "\n",
        "Following from the trigonometric definition of dot product\n",
        "$$\n",
        "\\underline{a}\\cdot\\underline{b} = |\\underline{a}||\\underline{b}|cos\\theta\n",
        "$$\n",
        "$$\n",
        "\\frac{\\underline{a}\\cdot\\underline{b}}{|\\underline{a}||\\underline{b}|} = cos\\theta\n",
        "$$\n",
        "\n",
        "We can define **cosine similarity** between two vectors $\\underline{v}$ and $\\underline{w}$ as\n",
        "$$\n",
        "cosine(\\underline{v},\\underline{w}) = \\frac{\\underline{v}\\cdot\\underline{w}}{|\\underline{v}||\\underline{w}|} = \\frac{\\sum_{i=1}^{N} v_{i}w_{i}}{}\n",
        "$$\n",
        "\n",
        "We know that $cosine(\\underline{v}, \\underline{w}) \\in [-1,1]$, its value tends to $1$ for vectors pointing in the same directions, it is $0$ for vectors that are orthogonal, to $-1$ for vectors pointing in opposite directions. But raw frequency values are non-negative, so the cosine for these vectors ranges from $0$ to $1$. \n",
        "\n",
        "<center><img src='https://miro.medium.com/max/701/0*wXBT2F8qHQoGnIvp\n",
        "' width=\"520\" height=\"260\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmbL3GIRedt5"
      },
      "source": [
        "##Frequency based Embedding (just a Quick Overview)#\n",
        "### Count Vector Matrix\n",
        "Consider a corpus $C$ of $D$ documents ${D_{1},D_{2},...,D_{D}}$ and $V$ unique tokens extracted out of the corpus $C$. The $V$ tokens will form our dictionary $\\mathbf{V}$ and the size of the Count Vector matrix $M$ will be given by $D$x$N$. Each row in the matrix $M$ contains the frequency of tokens in document $D_{i}$.\n",
        "\n",
        "$D_{1}$: \"He is a lazy boy. She is also lazy.\"\n",
        "\n",
        "$D_{2}$: \"Neeraj is a lazy person.\"\n",
        "\n",
        "The dictionary created may be a list of unique tokens in the corpus = $[‘He’,’She’,’lazy’,’boy’,’Neeraj’,’person’]$\n",
        "\n",
        "Here, $D=2$, $V=6$\n",
        "\n",
        "The count matrix $M$ of size $2$ x $6$ will be represented as –\n",
        "\n",
        "             He\tShe\tlazy\tboy\tNeeraj\tperson\n",
        "\n",
        "    D1  \t 1   \t1\t   2\t   1\t   0\t    0\n",
        "\n",
        "    D2\t   0\t  0\t   1\t   0\t   1\t    1\n",
        "\n",
        "\n",
        "\n",
        "<center><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04164920/count-vector.png\n",
        "' width=\"320\" height=\"260\"></center>\n",
        "\n",
        "### TF-IDF\n",
        "This is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus.\n",
        "\n",
        "Common words like 'is', 'the', 'a' etc. tend to appear quite frequently in comparison to the words which are important to a document. \n",
        "\n",
        "Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.\n",
        "\n",
        "TF-IDF stands for Term Frequency - Inverse Document Frequency adn it his computed as the product of these two terms. \n",
        "\n",
        "**In summary** the frequency models we’ve described so far represents a target\n",
        "word as a vector with dimensions corresponding to all the words in the vocabulary $\\mathbf{V}$ (length $V=|\\mathbf{V}|$, with vocabularies of $20,000$ to $50,000$), which is also sparse (most values are zero). The values in each dimension are the frequency with which the target word co-occurs with each neighboring context word, weighted by tf-idf. The model\n",
        "computes the similarity between two words $x$ and $y$ by taking the cosine of their tf-idf vectors; high cosine, high similarity. \n",
        "\n",
        "The tf-idf vector model can also be used to decide if *two documents are similar*.\n",
        "We represent a document by taking the vectors of all the words in the document, and\n",
        "centroid computing the centroid of all those vectors. The centroid is the multidimensional\n",
        "version of the mean.\n",
        "\n",
        "\n",
        "\n",
        "> I know that it may seem old, too simple and rudimentary, but in reality it is much more used than one might think (key-word for search engine, topic modelic). It is a first important step in the information extraction pipeline of a text.\n",
        "\n",
        "### Co-Occurrence Matrix \n",
        "The basic idea is, as we have just said, the distributional hypotesys: \n",
        "\n",
        "\n",
        "> Similar words tend to occur together and will have similar context, for example – Apple is a fruit. Mango is a fruit.\n",
        "Apple and mango tend to have a similar context i.e fruit.\n",
        "\n",
        "\n",
        "\n",
        "* **Co-occurrence** – For a given corpus, the co-occurrence of a pair of words say $w_{1}$ and $w_{2}$ is the number of times they have appeared together in a Context Window.\n",
        "\n",
        "* **Context Window** – Context window is specified by a number and the direction. So what does a context window of 2 (around) means?\n",
        "<p>&nbsp;</p>\n",
        "<table border=\"0\" cellspacing=\"0\">\n",
        "<colgroup span=\"8\" width=\"85\"></colgroup>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\" height=\"17\">Quick</td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\"><span style=\"color: #000000;\">Brown</span></td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#FF6600\"><span style=\"color: #000000;\">Fox</span></td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\">Jump</td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\">Over</td>\n",
        "<td style=\"text-align: center;\" align=\"left\">The</td>\n",
        "<td style=\"text-align: center;\" align=\"left\">Lazy</td>\n",
        "<td style=\"text-align: center;\" align=\"left\">Dog</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<p>&nbsp;</p>\n",
        "<table border=\"0\" cellspacing=\"0\">\n",
        "<colgroup span=\"8\" width=\"85\"></colgroup>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align: center;\" align=\"left\" height=\"17\">Quick</td>\n",
        "<td style=\"text-align: center;\" align=\"left\">Brown</td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\">Fox</td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\">Jump</td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#FF6600\">Over</td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\">The</td>\n",
        "<td style=\"text-align: center;\" align=\"left\" bgcolor=\"#66FF99\">Lazy</td>\n",
        "<td style=\"text-align: center;\" align=\"left\">Dog</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "Now, let us take an example corpus to calculate a co-occurrence matrix.\n",
        "\n",
        "> Corpus = He is not lazy. He is intelligent. He is smart.\n",
        "\n",
        "<p>&nbsp;</p>\n",
        "<table border=\"0\" cellspacing=\"0\">\n",
        "<colgroup span=\"7\" width=\"85\"></colgroup>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\" height=\"17\"><b> </b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\"><b>He</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\"><b>is</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\"><b>not</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\"><b>lazy</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\"><b>intelligent</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\"><b>smart</b></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\" height=\"17\"><b>He</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#FF3333\">4</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\" height=\"17\"><b>is</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">4</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\" height=\"17\"><b>not</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\" height=\"17\"><b>lazy</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#00CCFF\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\" height=\"17\"><b>intelligent</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">2</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td align=\"center\" bgcolor=\"#CCFF00\" height=\"17\"><b>smart</b></td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">1</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "<td align=\"center\" bgcolor=\"#CCCCCC\">0</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "Let’s say there are $V$ unique words in the corpus. So Vocabulary size = $V$. The columns of the Co-occurrence matrix form the context words. The different variations of Co-Occurrence Matrix are \n",
        "\n",
        "* A co-occurrence matrix of size $V$x$V$. Now, for even a decent corpus $V$ gets very large and difficult to handle. So generally, this architecture is never preferred in practice.\n",
        "* A co-occurrence matrix of size $V$x$N$ where $N$ is a subset of $V$ and can be obtained by removing irrelevant words like stopwords etc. for example. This is still very large and presents computational difficulties.\n",
        "\n",
        "*  A co-occurrence matrix of size $V$x$k$ where $k$ are the $k$ components of the PCA decomposition of the original co-occurrence matrix. \n",
        "\n",
        " \n",
        "What PCA does at the back is decompose Co-Occurrence matrix into three matrices,$U$,$S$ and $V$ where $U$ and $V$ are both orthogonal matrices. What is of importance is that dot product of $U$ and $S$ gives the word vector representation and $V$ gives the word context representation.\n",
        "\n",
        "<center><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04224842/svd2.png\n",
        "' width=\"720\" height=\"150\"></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_nlzmbvqVgk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "218c0551-f653-4449-a86a-f526a5953bba"
      },
      "source": [
        "import string\n",
        "import pprint\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "text = \"\"\"Mary had a little lamb, little lamb,\n",
        "    little lamb. Mary had a little lamb\n",
        "    whose fleece was white as snow.\n",
        "    And everywhere that Mary went\n",
        "    Mary went, Mary went. Everywhere\n",
        "    that Mary went,\n",
        "    The lamb was sure to go\"\"\"\n",
        "\n",
        "# Tokenization \n",
        "def extract_words(text):\n",
        "    temp = text.split() # Split the text on whitespace\n",
        "    text_words = []\n",
        "\n",
        "    for word in temp:\n",
        "        # Remove any punctuation characters present in the beginning of the word\n",
        "        while word[0] in string.punctuation:\n",
        "            word = word[1:]\n",
        "\n",
        "        # Remove any punctuation characters present in the end of the word\n",
        "        while word[-1] in string.punctuation:\n",
        "            word = word[:-1]\n",
        "\n",
        "        # Append this word into our list of words.\n",
        "        text_words.append(word.lower())\n",
        "        \n",
        "    return text_words\n",
        "\n",
        "text_words = extract_words(text)\n",
        "print(text_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mary', 'had', 'a', 'little', 'lamb', 'little', 'lamb', 'little', 'lamb', 'mary', 'had', 'a', 'little', 'lamb', 'whose', 'fleece', 'was', 'white', 'as', 'snow', 'and', 'everywhere', 'that', 'mary', 'went', 'mary', 'went', 'mary', 'went', 'everywhere', 'that', 'mary', 'went', 'the', 'lamb', 'was', 'sure', 'to', 'go']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPIsfbT1ruv2"
      },
      "source": [
        "This is a wasteful way to represent text. We can be much more efficient by representing each word by a number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShDHXsG-vGwO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "607b3c1c-c4b3-4693-e137-7a92151042ea"
      },
      "source": [
        "word_dict = {}  # Maps words to integers\n",
        "word_list = []  # Maps each integer to the corresponding word\n",
        "vocabulary_size = 0\n",
        "text_tokens = []\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "\n",
        "for word in text_words:\n",
        "    # If we are seeing this word for the first time, create an id for it and added it to our word dictionary\n",
        "    if word not in word_dict:\n",
        "        word_dict[word] = vocabulary_size\n",
        "        word_list.append(word)\n",
        "        vocabulary_size += 1\n",
        "    \n",
        "    # add the token corresponding to the current word to the tokenized text.\n",
        "    text_tokens.append(word_dict[word])\n",
        "\n",
        "print(\"Word list:\", word_list, \"\\n\\n Word dictionary:\")\n",
        "pp.pprint(word_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word list: ['mary', 'had', 'a', 'little', 'lamb', 'whose', 'fleece', 'was', 'white', 'as', 'snow', 'and', 'everywhere', 'that', 'went', 'the', 'sure', 'to', 'go'] \n",
            "\n",
            " Word dictionary:\n",
            "{ 'a': 2,\n",
            "  'and': 11,\n",
            "  'as': 9,\n",
            "  'everywhere': 12,\n",
            "  'fleece': 6,\n",
            "  'go': 18,\n",
            "  'had': 1,\n",
            "  'lamb': 4,\n",
            "  'little': 3,\n",
            "  'mary': 0,\n",
            "  'snow': 10,\n",
            "  'sure': 16,\n",
            "  'that': 13,\n",
            "  'the': 15,\n",
            "  'to': 17,\n",
            "  'was': 7,\n",
            "  'went': 14,\n",
            "  'white': 8,\n",
            "  'whose': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk5VrWaqv3vt"
      },
      "source": [
        "Unfortunately, while this representation is convenient for memory reasons it has some severe limitations. Perhaps the most important of which is the fact that computers naturally assume that numbers can be operated on mathematically (by addition, subtraction, etc) in a way that doesn't match our understanding of words.\n",
        "\n",
        "**One-hot encoding**\n",
        "\n",
        "One typical way of overcoming this difficulty is to represent each word by a one-hot encoded vector where every element is zero except the one corresponding to a specific word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M74AKmqLv9KW"
      },
      "source": [
        "def one_hot(word, word_dict):\n",
        "    \"\"\"\n",
        "        Generate a one-hot encoded vector corresponding to *word*\n",
        "    \"\"\"\n",
        "    \n",
        "    vector = np.zeros(len(word_dict))\n",
        "    vector[word_dict[word]] = 1\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG8iU07UwhXl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74cb6eba-7ac4-472f-8e6d-fdea5c37d61b"
      },
      "source": [
        "# So, for example, the word \"fleece\" would be represented by\n",
        "fleece_hot = one_hot(\"fleece\", word_dict)\n",
        "print(fleece_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLrYKKQLyZiX"
      },
      "source": [
        "**Bag of words**\n",
        "\n",
        "We can now use the one-hot encoded vector for each word to produce a vector representation of our original text, by simply adding up all the one-hot encoded vectors:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA8OYcA-ydMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b932327-7f04-416e-b65f-94e63a46be03"
      },
      "source": [
        "text_vector1 = np.zeros(vocabulary_size)\n",
        "\n",
        "for word in text_words:\n",
        "    hot_word = one_hot(word, word_dict)\n",
        "    text_vector1 += hot_word\n",
        "    \n",
        "print(text_vector1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6. 2. 2. 4. 5. 1. 1. 2. 1. 1. 1. 1. 2. 2. 4. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1RmtNggyoSZ"
      },
      "source": [
        "A more pythonic (and efficient) way of producing the same result is to use the standard `Counter` module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAAHlx-Fyt8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "c68a6249-fa71-46c8-b6e0-ef472b9e03c5"
      },
      "source": [
        "word_counts = Counter(text_words)\n",
        "pp.pprint(word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({ 'mary': 6,\n",
            "          'lamb': 5,\n",
            "          'little': 4,\n",
            "          'went': 4,\n",
            "          'had': 2,\n",
            "          'a': 2,\n",
            "          'was': 2,\n",
            "          'everywhere': 2,\n",
            "          'that': 2,\n",
            "          'whose': 1,\n",
            "          'fleece': 1,\n",
            "          'white': 1,\n",
            "          'as': 1,\n",
            "          'snow': 1,\n",
            "          'and': 1,\n",
            "          'the': 1,\n",
            "          'sure': 1,\n",
            "          'to': 1,\n",
            "          'go': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2uHjpcmztyI"
      },
      "source": [
        "**Term Frequency - Inverse Document Frequency**\n",
        "\n",
        "![alt text](https://camo.githubusercontent.com/b2802c1f94a48ea27024d862ef83587521397c50/68747470733a2f2f736561626f726e2e7079646174612e6f72672f5f696d616765732f736561626f726e2d686561746d61702d312e706e67)\n",
        "\n",
        "To measure the document frequency of a word we will need to have multiple documents. For the sake of simplicity, we will treat each sentence of our nursery rhyme as an individual document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haNtzEpzz2FE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "4871cbaf-d871-4b96-c4b6-1e38c3c902a0"
      },
      "source": [
        "corpus_text = text.split('.')\n",
        "corpus_words = []\n",
        "\n",
        "for document in corpus_text:\n",
        "    doc_words = extract_words(document)\n",
        "    corpus_words.append(doc_words)\n",
        "pp.pprint(corpus_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ ['mary', 'had', 'a', 'little', 'lamb', 'little', 'lamb', 'little', 'lamb'],\n",
            "  [ 'mary',\n",
            "    'had',\n",
            "    'a',\n",
            "    'little',\n",
            "    'lamb',\n",
            "    'whose',\n",
            "    'fleece',\n",
            "    'was',\n",
            "    'white',\n",
            "    'as',\n",
            "    'snow'],\n",
            "  ['and', 'everywhere', 'that', 'mary', 'went', 'mary', 'went', 'mary', 'went'],\n",
            "  [ 'everywhere',\n",
            "    'that',\n",
            "    'mary',\n",
            "    'went',\n",
            "    'the',\n",
            "    'lamb',\n",
            "    'was',\n",
            "    'sure',\n",
            "    'to',\n",
            "    'go']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r8VFjnN0xfX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "9c6930e6-113b-4ef4-b212-76a6893e4690"
      },
      "source": [
        "# Let us now calculate the number of documents in which each word appears\n",
        "document_count = {}\n",
        "\n",
        "for document in corpus_words:\n",
        "    word_set = set(document)\n",
        "    \n",
        "    for word in word_set:\n",
        "        document_count[word] = document_count.get(word, 0) + 1\n",
        "\n",
        "pp.pprint(document_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{ 'a': 2,\n",
            "  'and': 1,\n",
            "  'as': 1,\n",
            "  'everywhere': 2,\n",
            "  'fleece': 1,\n",
            "  'go': 1,\n",
            "  'had': 2,\n",
            "  'lamb': 3,\n",
            "  'little': 2,\n",
            "  'mary': 4,\n",
            "  'snow': 1,\n",
            "  'sure': 1,\n",
            "  'that': 2,\n",
            "  'the': 1,\n",
            "  'to': 1,\n",
            "  'was': 2,\n",
            "  'went': 2,\n",
            "  'white': 1,\n",
            "  'whose': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c564SZil07yZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "ed4de42e-cc6c-4b32-8e63-f9f2cd13b144"
      },
      "source": [
        "def inv_doc_freq(corpus_words):\n",
        "    number_docs = len(corpus_words)\n",
        "    \n",
        "    document_count = {}\n",
        "\n",
        "    for document in corpus_words:\n",
        "        word_set = set(document)\n",
        "\n",
        "        for word in word_set:\n",
        "            document_count[word] = document_count.get(word, 0) + 1\n",
        "    \n",
        "    IDF = {}\n",
        "    \n",
        "    for word in document_count:\n",
        "        IDF[word] = np.log(number_docs/document_count[word])\n",
        "        \n",
        "    \n",
        "    return IDF\n",
        "\n",
        "IDF = inv_doc_freq(corpus_words)\n",
        "\n",
        "pp.pprint(IDF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{ 'a': 0.6931471805599453,\n",
            "  'and': 1.3862943611198906,\n",
            "  'as': 1.3862943611198906,\n",
            "  'everywhere': 0.6931471805599453,\n",
            "  'fleece': 1.3862943611198906,\n",
            "  'go': 1.3862943611198906,\n",
            "  'had': 0.6931471805599453,\n",
            "  'lamb': 0.28768207245178085,\n",
            "  'little': 0.6931471805599453,\n",
            "  'mary': 0.0,\n",
            "  'snow': 1.3862943611198906,\n",
            "  'sure': 1.3862943611198906,\n",
            "  'that': 0.6931471805599453,\n",
            "  'the': 1.3862943611198906,\n",
            "  'to': 1.3862943611198906,\n",
            "  'was': 0.6931471805599453,\n",
            "  'went': 0.6931471805599453,\n",
            "  'white': 1.3862943611198906,\n",
            "  'whose': 1.3862943611198906}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srwybj5x1Kf9"
      },
      "source": [
        "As expected **Mary** has the smallest weight of all words 0, meaning that it is effectively removed from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3JuWrPV1R2X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "e77a8492-c818-492e-8a85-5defd5058c5d"
      },
      "source": [
        "def tf_idf(corpus_words):\n",
        "    IDF = inv_doc_freq(corpus_words)\n",
        "    \n",
        "    TFIDF = []\n",
        "    \n",
        "    for document in corpus_words:\n",
        "        TFIDF.append(Counter(document))\n",
        "    \n",
        "    for document in TFIDF:\n",
        "        for word in document:\n",
        "            document[word] = document[word]*IDF[word]\n",
        "            \n",
        "    return TFIDF\n",
        "\n",
        "tf_idf(corpus_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-805864b5b3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhWqMnqX1egA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Prediction based Vector \n",
        "\n",
        "So far, we have seen deterministic methods to determine word vectors. \n",
        "\n",
        "Word2Vec was introduced in 2013 by Mikolov et al. at Google on efficient vector representations of words (link at the [paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)).\n",
        "\n",
        "These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities.\n",
        "\n",
        "**Word2vec** is not a single algorithm but a combination of two techniques – **CBOW**(Continuous Bag of Words) and **Skip-gram** model. Algorithmically, these models are similar, except that CBOW predicts target words from source context words, while the skip-gram does the inverse and predicts source context-words from the target words.\n",
        "\n",
        "Suppose to have the sentence \n",
        "<center> \"The cat jumped over the puddle\" </center> \n",
        "\n",
        "and want to compute an embedding for `jumped`. \n",
        "The intuition of word2vec is that instead of counting how often each word $w$ occurs near, `jumped`, we’ll instead train a classifier on a binary prediction task: `Is word w likely to show up near jumped?` .\n",
        "\n",
        "We don’t actually care about this prediction task; instead we’ll take the learned *classifier weights* as the word embeddings. \n",
        "\n",
        "The revolutionary intuition here is that we can just use the running text as implicity supervised training data for such a classifier.\n",
        "\n",
        "The representations made with Word2Vec seem to capture many linguistic regularities. \n",
        "\n",
        "We see that vectors learned these relevant relations:\n",
        "\n",
        "* Man is to woman as king is to queen (King – Man + Woman = Queen)\n",
        "* Building is to architect as software is to programmer (Software - Building + Architect = Programmer)\n",
        "\n",
        "\n",
        "<center><img src='https://cdn.datafloq.com/cms/2019/04/30/ai8.png\n",
        "' width=\"550\" height=\"400\"></center>\n",
        "\n",
        "In similar way, these are the Country-Capital relations maintained by Word2Vec\n",
        "\n",
        "<center><img src='https://adriancolyer.files.wordpress.com/2016/04/word2vec-ee-table-8.png?w=656&zoom=2\n",
        "' width=\"570\" height=\"300\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The Word2Vec models are fast, efficient to train, and easily available online with code adn pretrained embeddings. \n",
        "\n",
        "It generates *dense embedding vectors*, dimension of these vectors is an hyper-parameter of the alghoritm, the **embedding size**. \n",
        "\n",
        "In general, it makes the size of the embedding vary between $200$ and $500$ if your domain is very large, the most used value is $300$. Otherwise between $100$ and $300$ with a less extended domain. They are obviously all rules of thumb, there is no mathematical proof that taking one dimension with respect to another gives one more performance.\n",
        "\n",
        "Word2Vec consists of 2 algorithms.\n",
        "\n",
        "* CBOW: The neural network takes a look at the surrounding words and predicts the word that comes in between.\n",
        "\n",
        "* SkipGram: The neural network takes in a word and then tries to predict the surrounding words.\n",
        "\n",
        "#### CBOW (Continuous Bag of Words) Model\n",
        "*Predicting a center word form the\n",
        "surrounding context*\n",
        "\n",
        "Mokilov's idea is simplicity.\n",
        "\n",
        "We don't talk about deep networks, we don't talk about non-linear hidden layers. \n",
        "\n",
        "We talk about networks with only one hidden layer and mostly linear. What you do inside this layer, a simple average (for contexts greater than $1$) or simply pass the cards to the output.\n",
        "\n",
        "<center><img src='https://miro.medium.com/max/670/1*vZhxrBkCz-yN_rzZBqSKiA.png'></center>\n",
        "\n",
        "As we have said, everything is based on the context, the first thing to define is in fact the context window we want to consider. \n",
        "\n",
        "Imagine a sliding window over the text, that includes the central word currently in focus, together with, in the case of having chosen a window of four,  the four words and precede it, and the four words that follow it:\n",
        "\n",
        "<center><img src='https://adriancolyer.files.wordpress.com/2016/04/word2vec-context-words.png?w=656&zoom=2\n",
        "' width=\"650\" height=\"210\"></center>\n",
        "\n",
        "The context words form the input layer. Each word is encoded in one-hot form, so if the vocabulary size is V these will be V-dimensional vectors with just one of the elements set to one, and the rest all zeros.\n",
        "\n",
        "\n",
        "Word2vec uses a single hidden layer, fully connected neural network as shown below. *The neurons in the hidden layer are all linear neurons*. The hidden layer size is set to the dimensionality of the resulting word vectors. The size of the output layer is same as the input layer.\n",
        "\n",
        "The training objective is to maximize the conditional probability of observing the actual output word (the focus word) given the input context words, with regard to the weights.\n",
        "\n",
        "<center><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04224109/Screenshot-from-2017-06-04-22-40-29.png\n",
        "' width=\"350\" height=\"210\"></center>\n",
        "\n",
        "\n",
        "<center><img src='https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04220606/Screenshot-from-2017-06-04-22-05-44.png\n",
        "' width=\"350\" height=\"350\"></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Since our input vectors are one-hot, multiplying an input vector by the weight matrix $W1$ amounts to simply selecting a row from $W1$.\n",
        "\n",
        "<center><img src='https://adriancolyer.files.wordpress.com/2016/04/word2vec-linear-activitation.png?w=656&zoom=2\n",
        "' width=\"500\" height=\"300\"></center>\n",
        "\n",
        "Given $C$ input word vectors, the activation function for the hidden layer h amounts to simply summing the corresponding ‘hot’ rows in $W1$, and dividing by $C$ to take their average.\n",
        "\n",
        ">This implies that the link (activation) function of the hidden layer units is simply linear (i.e., directly passing its weighted sum of inputs to the next layer).\n",
        "\n",
        "From the hidden layer to the output layer, the second weight matrix $W2$ can be used to compute a score for each word in the vocabulary, and $softmax$ can be used to obtain the posterior distribution of words.\n",
        "\n",
        "We breakdown the way this model works in these steps:\n",
        "\n",
        "1. We generate our one hot word vectors $(x^{(c−m)},..., x^{(c−1)}, x^{(c+1)},...,x^{(c+m)})$ for the input context of size m.\n",
        "2. We get our embedded word vectors for the context \n",
        " $(v_{c−m}= W_{1}x^{(c-m)},..., v_{c+m}= W_{1}x^{(c+m)})$\n",
        "3. Average these vectors to get $ \\hat{v}=\\frac{v_{c-m}+v_{c-m+1}+...+v_{c+m}}{2m} $\n",
        "4. Generate a score vector $z = W_{2}\\hat{v}$\n",
        "5. Turn the scores into probabilities $\\hat{y} = softmax(z)$\n",
        "6. We desire our probabilities generated, $\\hat{y}$, to match the true probabilities, $y$, which also happens to be the one hot vector of the\n",
        "actual word.\n",
        "7. Training is made of Cross-Entropy Loss function and Back Propagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJujii69Lt8y"
      },
      "source": [
        "Gensim is a Python library for topic modeling, document indexing and similarity retrieval with large corpora, contains implementations of fasttext, word2vec, LDA, tf-idf\n",
        "\n",
        "\n",
        "To implement the `word2vec model`, we’ll use the **gensim** library ([gensim wod2vec documentation](https://radimrehurek.com/gensim/models/word2vec.html)) which provides many features in the model such as finding the odd one out, most similar words etc. However, it does not lowercase/tokenize the sentences, so I do the same. The tokenized sentences are then passed to the model. I’ve set the *size of vector* to be *2*, *window* to be *3* which defines the distance upto which to look and *sg = 0* uses the CBOW model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwW5uYzuMBPn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cd6be08f-6832-4f19-b435-ade784cf3eb1"
      },
      "source": [
        "# Just a very simple example of how to create an embedding given your own corpus using word2vec model of `gensim` library.\n",
        "from gensim.models import word2vec \n",
        "\n",
        "sentences = [\n",
        "    'He is playing in the field.',\n",
        "    'He is running towards the football.',\n",
        "    'The football game ended.',\n",
        "    'It started raining while everyone was playing in the field.'\n",
        "]\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "\ttokenized= []\n",
        "\tfor word in sentence.split(' '):\n",
        "\t\tword = word.split('.')[0] \n",
        "\t\tword = word.lower()\n",
        "\t\ttokenized.append(word)\n",
        "\tsentences[i] = tokenized\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embedding_size = 6\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['he', 'is', 'playing', 'in', 'the', 'field'], ['he', 'is', 'running', 'towards', 'the', 'football'], ['the', 'football', 'game', 'ended'], ['it', 'started', 'raining', 'while', 'everyone', 'was', 'playing', 'in', 'the', 'field']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cVbiuKhiVVn"
      },
      "source": [
        "`class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)`  (https://radimrehurek.com/gensim/models/word2vec.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUGL7LlIdUIQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "04a13d5d-f425-4bd7-bb10-4a0a088de641"
      },
      "source": [
        "# alpha : float, The initial learning rate.\n",
        "# iter : int, Number of iterations (epochs) over the corpus.\n",
        "model = word2vec.Word2Vec(sentences, workers = 1, size = embedding_size, min_count = 1, window = 2, sg = 0,iter=1000, compute_loss=True, batch_words=1) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-72439ac8e58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# alpha : float, The initial learning rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# iter : int, Number of iterations (epochs) over the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'word2vec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCdishqHiFPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "e30423ce-7957-4ad8-b9d1-82f66777404b"
      },
      "source": [
        "vector = model.wv['playing']  # numpy vector of embedding of word 'raining'\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e13053b61f03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'playing'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# numpy vector of embedding of word 'raining'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fgVX2lZitzZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "cc8a84ac-247d-4922-c4ba-f1fdad6a348d"
      },
      "source": [
        "# View the Vocabulary, ATTENTION to do this with large vocabularies\n",
        "# The trained word vectors are stored in a KeyedVectors instance in model.wv:\n",
        "model.wv.vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ended': <gensim.models.keyedvectors.Vocab at 0x7f516466b2e8>,\n",
              " 'everyone': <gensim.models.keyedvectors.Vocab at 0x7f516466b320>,\n",
              " 'field': <gensim.models.keyedvectors.Vocab at 0x7f516466b0f0>,\n",
              " 'football': <gensim.models.keyedvectors.Vocab at 0x7f516466b160>,\n",
              " 'game': <gensim.models.keyedvectors.Vocab at 0x7f516466b1d0>,\n",
              " 'he': <gensim.models.keyedvectors.Vocab at 0x7f51646e9a20>,\n",
              " 'in': <gensim.models.keyedvectors.Vocab at 0x7f516466b080>,\n",
              " 'is': <gensim.models.keyedvectors.Vocab at 0x7f517259a748>,\n",
              " 'it': <gensim.models.keyedvectors.Vocab at 0x7f516466b208>,\n",
              " 'playing': <gensim.models.keyedvectors.Vocab at 0x7f517259a7b8>,\n",
              " 'raining': <gensim.models.keyedvectors.Vocab at 0x7f516466b2b0>,\n",
              " 'running': <gensim.models.keyedvectors.Vocab at 0x7f516466b198>,\n",
              " 'started': <gensim.models.keyedvectors.Vocab at 0x7f516466b278>,\n",
              " 'the': <gensim.models.keyedvectors.Vocab at 0x7f516466b0b8>,\n",
              " 'towards': <gensim.models.keyedvectors.Vocab at 0x7f516466b128>,\n",
              " 'was': <gensim.models.keyedvectors.Vocab at 0x7f516466b390>,\n",
              " 'while': <gensim.models.keyedvectors.Vocab at 0x7f516466b240>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adf0q8TvwLQm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "f275dec4-26bf-457f-e707-da9da4daad47"
      },
      "source": [
        "model.wv[model.wv.vocab] # It's a different order, here embeddings is of sorted keys"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-4.49293712e-03, -1.34685829e-01,  3.60356510e-01,\n",
              "         8.23740140e-02,  9.70428810e-02, -1.79023042e-01],\n",
              "       [ 8.91096133e-05, -5.42577133e-02,  2.35974565e-01,\n",
              "         2.85202507e-02, -5.95583860e-03, -1.39510423e-01],\n",
              "       [-9.23706815e-02, -2.51447503e-02,  4.33969557e-01,\n",
              "         1.56407550e-01,  9.53966230e-02, -2.48711437e-01],\n",
              "       [-1.57394916e-01, -8.61947909e-02,  5.06874263e-01,\n",
              "         5.27862795e-02,  5.41486740e-02, -2.83928812e-01],\n",
              "       [-2.24667162e-01, -9.04246271e-02,  4.79372174e-01,\n",
              "         1.06965557e-01,  1.17606103e-01, -2.12957919e-01],\n",
              "       [-1.66331321e-01, -7.35694021e-02,  4.83612418e-01,\n",
              "         1.97481662e-01,  9.84610692e-02, -1.80011272e-01],\n",
              "       [-1.41832232e-01, -4.78229001e-02,  2.92892724e-01,\n",
              "        -2.15399247e-02, -2.83952095e-02, -2.11588740e-01],\n",
              "       [ 2.76846369e-03,  4.71777283e-03,  2.09954336e-01,\n",
              "        -1.80740263e-02, -2.79798470e-02, -1.72566906e-01],\n",
              "       [-7.29497671e-02, -1.07343748e-01,  1.98176652e-01,\n",
              "         7.31591806e-02,  3.09855212e-02, -1.51998773e-01],\n",
              "       [-1.87820327e-02, -4.40674499e-02,  1.48539409e-01,\n",
              "         4.38676663e-02, -6.01883344e-02, -1.02810711e-01],\n",
              "       [ 1.70609467e-02,  3.87538783e-02,  8.65876228e-02,\n",
              "        -2.63950527e-02,  8.21407065e-02,  3.04946420e-03],\n",
              "       [-1.43066317e-01, -4.13220674e-02,  5.28727710e-01,\n",
              "         1.34752840e-01,  6.41197786e-02, -3.19110304e-01],\n",
              "       [-6.12017401e-02, -9.93182510e-02,  4.82779503e-01,\n",
              "         2.06712231e-01,  4.53098938e-02, -2.07865238e-01],\n",
              "       [-1.04947679e-01, -1.43609747e-01,  4.23266709e-01,\n",
              "         1.01234555e-01,  3.92338820e-02, -2.59806186e-01],\n",
              "       [-1.28151268e-01, -1.02877967e-01,  4.72220421e-01,\n",
              "         2.05045685e-01, -2.95858979e-02, -3.13710481e-01],\n",
              "       [-6.17264919e-02, -6.85901120e-02,  6.17871761e-01,\n",
              "         8.60897899e-02,  1.24855220e-01, -2.41980180e-01],\n",
              "       [-1.02732904e-01, -7.22394809e-02,  5.95619440e-01,\n",
              "         1.98921323e-01, -2.81344559e-02, -2.93178022e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhsem66Bh63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "853a3df8-ccca-47de-c679-b16d3524b78d"
      },
      "source": [
        "# And now, let's check the similarity\n",
        "print(model.wv.similarity('playing', 'football'))\n",
        "print(model.wv.similarity('playing', 'while'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.93276083\n",
            "0.96473026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlxs4lyufNen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "9306c457-9ce4-4816-9489-56fa05e50566"
      },
      "source": [
        "print(model.wv.most_similar('playing'))\n",
        "print(model.wv.most_similar('football'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('it', 0.9917640089988708), ('field', 0.9775766730308533), ('started', 0.9773678183555603), ('was', 0.9728482365608215), ('everyone', 0.9713796377182007), ('in', 0.9670824408531189), ('while', 0.9647303223609924), ('raining', 0.9646284580230713), ('the', 0.9619563817977905), ('he', 0.9461178779602051)]\n",
            "[('raining', 0.9844321012496948), ('while', 0.9605823755264282), ('in', 0.947970986366272), ('he', 0.945022463798523), ('it', 0.9431756734848022), ('the', 0.9411342144012451), ('started', 0.9393022060394287), ('playing', 0.9327608346939087), ('field', 0.9315495491027832), ('was', 0.9290846586227417)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc34Lp6Eqn9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0084eee6-d40c-4be3-e8b6-a9cf192208df"
      },
      "source": [
        "# Here, we ask our model to give us the word that does not belong to the list!\n",
        "print(model.wv.doesnt_match(['football', 'playing', 'while']))\n",
        "#print(model_fasttext.wv.doesnt_match(['football', 'playing', 'while']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "football\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iphwvywepjJO"
      },
      "source": [
        "It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. \n",
        "\n",
        "**T-SNE** is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLrZdLw67Ywt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        " \n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "from sklearn.decomposition import PCA \n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcYjmAp77fwg"
      },
      "source": [
        "To make the visualizations more relevant, we will look at the relationships between a query word (in **red**), its most similar words in the model (in **blue**), and other words from the vocabulary (in **green**). (https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial#Exploring-the-model) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4rCwmgw7iG9"
      },
      "source": [
        "def tsnescatterplot(model, word, list_names):\n",
        "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
        "    its list of most similar words, and a list of words.\n",
        "    @ word = is the target word\n",
        "    @ list_names = list of most dissimilar words to compare with the most similar words    \n",
        "    \"\"\"\n",
        "    arrays = np.empty((0, embedding_size), dtype='f')\n",
        "    word_labels = [word]\n",
        "    color_list  = ['red']\n",
        "\n",
        "    # adds the vector of the query word\n",
        "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
        "    \n",
        "    # gets list of most similar words\n",
        "    close_words = model.wv.most_similar([word], topn = 3) # It takes the 3 most similar words\n",
        "    \n",
        "    # adds the vector for each of the closest words to the array\n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('blue')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "    \n",
        "    # adds the vector for each of the words from list_names to the array\n",
        "    for wrd in list_names:\n",
        "        wrd_vector = model.wv.__getitem__([wrd])\n",
        "        word_labels.append(wrd)\n",
        "        color_list.append('green')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "        \n",
        "    # Reduces the dimensionality with PCA\n",
        "    reduc = PCA(n_components=embedding_size-1).fit_transform(arrays)\n",
        "    \n",
        "    # Finds t-SNE coordinates for 2 dimensions\n",
        "    np.set_printoptions(suppress=True)\n",
        "   \n",
        "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(arrays) # If you want to do PCA before, replace 'arrays' with 'reduc'\n",
        "    \n",
        "    # Sets everything up to plot\n",
        "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
        "                       'y': [y for y in Y[:, 1]],\n",
        "                       'words': word_labels,\n",
        "                       'color': color_list})\n",
        "    \n",
        "    fig, _ = plt.subplots()\n",
        "    fig.set_size_inches(9, 9)\n",
        "    \n",
        "    # Basic plot\n",
        "    p1 = sns.regplot(data=df,\n",
        "                     x=\"x\",\n",
        "                     y=\"y\",\n",
        "                     fit_reg=False,\n",
        "                     marker=\"o\",\n",
        "                     scatter_kws={'s': 40,\n",
        "                                  'facecolors': df['color']\n",
        "                                 }\n",
        "                    )\n",
        "    \n",
        "    # Adds annotations one by one with a loop\n",
        "    for line in range(0, df.shape[0]):\n",
        "         p1.text(df[\"x\"][line],\n",
        "                 df['y'][line],\n",
        "                 '  ' + df[\"words\"][line].title(),\n",
        "                 horizontalalignment='left',\n",
        "                 verticalalignment='bottom', size='medium',\n",
        "                 color=df['color'][line],\n",
        "                 weight='normal'\n",
        "                ).set_size(15)\n",
        "\n",
        "    \n",
        "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
        "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
        "            \n",
        "    plt.title('t-SNE visualization for {}'.format(word.title()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCKM4yo97yb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "80528f17-e0d4-4744-cfca-04a1fb351db3"
      },
      "source": [
        "# Compare the word with the 3 most similar and 3 most dissimilar words\n",
        "tsnescatterplot(model, 'playing', [i[0] for i in model.wv.most_similar(negative=[\"playing\"], topn=3)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogX7wGlmdEig",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "a52105f5-ed1d-4fa6-c94c-e997279b0a3f"
      },
      "source": [
        "tsnescatterplot(model, 'playing', [i[0] for i in model.wv.most_similar(negative=[\"playing\"], topn=3)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oyix4x8fsTi2"
      },
      "source": [
        "Gensim is a topic modelling library for Python that provides access to Word2Vec and other word embedding algorithms for training, and it also allows **pre-trained word embeddings** that you can download from the internet to be loaded.\n",
        "\n",
        "## Pre-trained Word Embeddings\n",
        "\n",
        "Pre-trained models are the simplest way to start working with word embeddings. A pre-trained model is a set of word embeddings that have been created elsewhere that you simply load onto your computer and into memory.\n",
        "\n",
        "The **advantage** of these models is that they can leverage massive datasets that you may not have access to, built using billions of different words, with a vast corpus of language that captures word meanings in a statistically robust manner. \n",
        "\n",
        "Pre-trained models are also **available in languages other than English**, opening up multi-lingual opportunities for your applications.\n",
        "\n",
        "The **disadvantage** of pre-trained word embeddings is that the words contained within may not capture the peculiarities of language in your specific application domain. For example, Wikipedia may not have great word exposure to particular aspects of legal doctrine or religious text, so if your application is specific to a domain like this, your results may not be optimal due to the generality of the downloaded model’s word embeddings.\n",
        "\n",
        "#### Pre-trained models in Gensim\n",
        "To load a pre-trained model into Gensim you first need to find and download one. \n",
        "\n",
        "A popular pre-trained option is the Google News dataset model, containing **300-dimensional embeddings for 3 millions words**.\n",
        "\n",
        "Download the binary file ‘GoogleNews-vectors-negative300.bin’ (1.3 GB compressed) from https://code.google.com/archive/p/word2vec/.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCcxAudQumzg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "f257413d-8062-4350-e4c5-f7a4e1c2c2c7"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-24 12:31:23--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.98.158\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.98.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  76.3MB/s    in 17s     \n",
            "\n",
            "2020-10-24 12:31:41 (90.9 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHGu92mSvOHH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "58ac5a65-fad7-40f3-e768-bddd65db1325"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "# Load vectors directly from the file\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5feMRDQv4Tf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58dd1c93-560a-4a1f-f0af-87ae8646a307"
      },
      "source": [
        "# Access vectors for specific words with a keyed lookup:\n",
        "vector = word2vec['easy']\n",
        "# see the shape of the vector (300,)\n",
        "vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0B2ScxQwECy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "caad9954-a761-4154-ae4e-9db68c9f371a"
      },
      "source": [
        "vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.30664062,  0.06835938, -0.16015625,  0.11962891, -0.00656128,\n",
              "        0.00439453,  0.14453125,  0.06201172,  0.07177734,  0.0267334 ,\n",
              "        0.09912109, -0.02307129,  0.05664062, -0.17480469, -0.05322266,\n",
              "        0.08984375,  0.29492188, -0.06591797,  0.13574219, -0.17382812,\n",
              "        0.07324219,  0.20800781,  0.07275391,  0.21972656, -0.05029297,\n",
              "       -0.11523438, -0.18066406, -0.00000429, -0.16992188, -0.07617188,\n",
              "       -0.00430298,  0.171875  ,  0.2578125 , -0.13378906,  0.03955078,\n",
              "        0.00424194, -0.02807617, -0.15429688,  0.17675781,  0.06689453,\n",
              "        0.27148438, -0.14355469,  0.40234375, -0.11914062, -0.02587891,\n",
              "       -0.05639648,  0.0378418 ,  0.04296875,  0.02929688, -0.02111816,\n",
              "       -0.04150391,  0.06298828, -0.01904297, -0.06103516, -0.16699219,\n",
              "        0.10546875,  0.0004406 , -0.18164062,  0.19042969,  0.06982422,\n",
              "       -0.08642578, -0.06835938, -0.30859375, -0.125     , -0.01672363,\n",
              "       -0.10351562, -0.25976562,  0.02294922, -0.10107422, -0.00253296,\n",
              "        0.02929688,  0.16796875, -0.01647949, -0.25390625, -0.18164062,\n",
              "       -0.12451172,  0.09619141,  0.09423828,  0.14648438,  0.19433594,\n",
              "       -0.26171875,  0.11914062, -0.10400391,  0.01855469,  0.03027344,\n",
              "       -0.16894531, -0.10058594, -0.14355469,  0.12451172,  0.01550293,\n",
              "       -0.16015625, -0.04638672, -0.08642578, -0.15136719, -0.21191406,\n",
              "        0.03930664, -0.00305176,  0.00582886,  0.08886719, -0.08984375,\n",
              "       -0.06689453,  0.22460938, -0.12060547,  0.03955078, -0.20703125,\n",
              "        0.17773438,  0.0045166 , -0.12255859,  0.20117188, -0.07519531,\n",
              "       -0.16308594, -0.1796875 , -0.23535156, -0.28125   ,  0.328125  ,\n",
              "        0.06591797,  0.0135498 , -0.0062561 , -0.00188446, -0.05175781,\n",
              "       -0.18066406,  0.07373047,  0.1484375 , -0.14648438, -0.19628906,\n",
              "       -0.00830078, -0.10253906, -0.12792969,  0.14453125,  0.18457031,\n",
              "       -0.234375  ,  0.01879883, -0.03808594,  0.47265625, -0.10205078,\n",
              "       -0.04931641,  0.04516602,  0.12060547,  0.0324707 ,  0.12792969,\n",
              "       -0.04760742, -0.02832031,  0.1328125 , -0.10351562, -0.11914062,\n",
              "       -0.00056076, -0.11181641, -0.13183594,  0.13867188, -0.04199219,\n",
              "        0.04003906,  0.09912109, -0.12011719, -0.04638672, -0.02624512,\n",
              "        0.07861328, -0.1640625 ,  0.14941406, -0.25585938, -0.11816406,\n",
              "       -0.04321289,  0.25585938,  0.1796875 , -0.09228516,  0.08105469,\n",
              "       -0.04052734, -0.30273438, -0.10693359,  0.0703125 , -0.26171875,\n",
              "       -0.17089844,  0.10839844, -0.27929688,  0.16503906, -0.03466797,\n",
              "        0.36328125, -0.1484375 , -0.07080078, -0.08398438,  0.05371094,\n",
              "       -0.1953125 , -0.27148438,  0.0246582 , -0.21972656, -0.02185059,\n",
              "        0.04663086, -0.10839844,  0.09130859, -0.08740234, -0.07128906,\n",
              "       -0.11328125, -0.09716797,  0.01300049,  0.04199219, -0.28125   ,\n",
              "       -0.08398438, -0.11425781, -0.16894531, -0.17675781, -0.3203125 ,\n",
              "        0.296875  ,  0.15039062, -0.02099609,  0.16503906,  0.09423828,\n",
              "        0.09375   , -0.02966309, -0.01196289,  0.02734375, -0.02819824,\n",
              "       -0.12304688,  0.375     , -0.07958984,  0.19726562,  0.17382812,\n",
              "       -0.10205078,  0.19042969,  0.25585938, -0.26171875,  0.03063965,\n",
              "       -0.05517578,  0.16699219, -0.16992188, -0.14355469, -0.03588867,\n",
              "       -0.11865234,  0.20019531, -0.27929688,  0.05786133,  0.04614258,\n",
              "        0.06542969,  0.22167969, -0.0300293 , -0.02819824,  0.17480469,\n",
              "       -0.07568359, -0.09863281, -0.06005859,  0.01019287,  0.19433594,\n",
              "        0.04101562, -0.08007812, -0.04882812, -0.265625  , -0.15429688,\n",
              "        0.07763672, -0.04199219,  0.18554688, -0.05273438, -0.1328125 ,\n",
              "       -0.00866699, -0.05029297,  0.27539062,  0.11962891,  0.25      ,\n",
              "        0.10595703,  0.02026367, -0.08984375, -0.125     , -0.1875    ,\n",
              "        0.0534668 , -0.14453125, -0.18164062, -0.07226562,  0.03540039,\n",
              "        0.12890625, -0.02856445, -0.38085938, -0.10351562, -0.06933594,\n",
              "        0.13964844,  0.11914062,  0.02270508,  0.09619141, -0.08642578,\n",
              "        0.13964844,  0.3203125 , -0.18457031,  0.0859375 , -0.03515625,\n",
              "       -0.06591797, -0.13867188, -0.2109375 , -0.07324219, -0.27148438,\n",
              "        0.18359375, -0.09814453,  0.06079102, -0.07910156,  0.17578125,\n",
              "       -0.1171875 ,  0.15332031,  0.08251953, -0.04541016, -0.21484375,\n",
              "        0.02648926,  0.13183594, -0.01599121, -0.21679688,  0.1015625 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdAZgb97wRlW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12468448-4703-49b2-dda8-6605925d1879"
      },
      "source": [
        "vectors = [word2vec[x] for x in \"This is some text I am processing with Spacy\".split(' ')]\n",
        "len(vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPEoPksQwpZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "485a736b-290b-4b10-cd1a-eca7af79040a"
      },
      "source": [
        "a = word2vec.similarity('straightforward', 'easy')\n",
        "a "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5717044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xOip6-Zw33S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "99dfe333-2a8e-4ef4-b41a-6fca9f1f9d86"
      },
      "source": [
        "word2vec.most_similar('simple')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('straightforward', 0.7460168600082397),\n",
              " ('Simple', 0.7108173966407776),\n",
              " ('uncomplicated', 0.6297484636306763),\n",
              " ('simplest', 0.6171397566795349),\n",
              " ('easy', 0.5990299582481384),\n",
              " ('fairly_straightforward', 0.5893307328224182),\n",
              " ('deceptively_simple', 0.5743066072463989),\n",
              " ('simpler', 0.5537199378013611),\n",
              " ('simplistic', 0.5516539216041565),\n",
              " ('disarmingly_simple', 0.5365327000617981)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m62TC68djNk3"
      },
      "source": [
        "### How evaluate Word2vec model (or any other embedding model)\n",
        "One way to evaluate the word2vec model is to develop a **ground truth** set of words. Ground truth will represent words that should ideally be closest together in vector space. For example if your corpus is related to customer service, perhaps the vectors for \"dissatisfied\" and \"disappointed\" will ideally have the smallest euclidean distance or largest cosine similarity.\n",
        "\n",
        "You create this table for ground truth, maybe it has 200 paired words. These 200 words are the most important paired words for your industry / topic. To assess which word2vec model is best, simply calculate the distance for each pair, do it 200 times, sum up the total distance, and the smallest total distance will be your best model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pQ-Qkiu0_-t"
      },
      "source": [
        "### Limitations of Word2Vec\n",
        "While Word2Vec was a game-changer for NLP, we will see how there was still some room for improvement:\n",
        "\n",
        "* **Out of Vocabulary(OOV) Words**\n",
        "  \n",
        "  In Word2Vec, an embedding is created for each word. As such, it can’t handle any words it has not encountered during its training.\n",
        "\n",
        "  For example, words such as “tensor” and “flow” are present in the vocabulary of Word2Vec. But if you try to get embedding for the compound word “tensorflow”, you will get an out of vocabulary error.\n",
        "\n",
        "  <center><img src=https://amitness.com/images/word2vec-oov-tensorflow.png></center>\n",
        "\n",
        "* **Morphology**\n",
        "\n",
        "  For words with same radicals such as “eat” and “eaten”, Word2Vec doesn’t do any parameter sharing. Each word is learned uniquely based on the context it appears in. Thus, there is scope for utilizing the internal structure of the word to make the process more efficient.\n",
        "\n",
        "<center><image src= \"https://amitness.com/images/word2vec-radicals.png\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5B1DWaI13Ku"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "### From Word2Vec to FastText\n",
        "\n",
        "<center><img src='https://fasttext.cc/img/fasttext-logo-color-web.png\n",
        "' width=\"300\" height=\"70\"></center>\n",
        "\n",
        "\n",
        "To solve the above challenges, Bojanowski et al. proposed a new embedding method called FastText. Their key insight was to use the internal structure of a word to improve vector representations obtained from the skip-gram method.\n",
        "\n",
        "The modification to the skip-gram method is applied as follows:\n",
        "\n",
        "* **1. Sub-word generation**\n",
        "\n",
        "  Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). \n",
        "\n",
        "For instance, the tri-grams for the word *apple* is *app*, *ppl*, and *ple* (ignoring the starting and ending of boundaries of words). The word embedding vector for apple will be the sum of all these n-grams. After training the Neural Network, we will have word embeddings for all the n-grams given the training dataset. Rare words can now be properly represented since it is highly likely that some of their n-grams also appears in other words.\n",
        "\n",
        "> Word2vec treats each word in corpus like an atomic entity and generates a vector for each word.\n",
        "\n",
        "> FastText treats each word as composed of character ngrams. So the vector for a word is made of the sum of this character n grams\n",
        "\n",
        "First, we take a word and add angular brackets to denote the beginning and end of a word\n",
        "\n",
        "<center><img src=\"https://amitness.com/images/fasttext-angular-brackets.png\">\n",
        "</center>\n",
        "\n",
        "Then, we generate character n-grams of length n. For example, for the word “eating”, character n-grams of length 3 can be generated by sliding a window of 3 characters from the start of the angular bracket till the ending angular bracket is reached. Here, we shift the window one step each time.\n",
        "\n",
        "<center><img src=\"https://amitness.com/images/fasttext-3-grams-list.png\"></center>\n",
        "\n",
        "Paper Insight:\n",
        "\n",
        "FastText improves performance on syntactic word analogy tasks significantly for morphologically rich language like Czech and German.\n",
        "\n",
        "<center><img src=\"https://amitness.com/images/fasttext-syntactic-analogy.png\"></center>\n",
        "\n",
        "Language \tword2vec-skipgram |\tword2vec-cbow\t| fasttext\n",
        "\n",
        "Czech\t52.8\t55.0\t**77.8**\n",
        "\n",
        "German\t44.5\t45.0\t**56.4**\n",
        "\n",
        "English\t70.1\t69.9\t**74.9**\n",
        "\n",
        "Italian\t51.5\t51.8\t**62.7**\n",
        "\n",
        "FastText has degraded performance on semantic analogy tasks compared to Word2Vec.\n",
        "\n",
        "<center><img src=\"https://amitness.com/images/fasttext-semantic-analogy.gif\"></center>\n",
        "\n",
        "There are **two main advantages** of fastText:\n",
        "*  Generate better word embeddings for **rare words** (parole rare ma gli ngrammi saranno gia' presenti negli embedding)\n",
        "*  **Out of vocabulary words** - they can construct the vector for a word from its character n grams even if word doesn't appear in training corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIoyx_MCNTyg"
      },
      "source": [
        "Now, in a similar way to before, we're going to use `gensim` to create embedding with **fastText** to be able to compare them with those of word2vec ([fastText gensim documentation](https://radimrehurek.com/gensim/models/fasttext.html)). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBC18j-XN6Fd"
      },
      "source": [
        "from gensim.models import FastText\n",
        "model_fasttext = FastText(sentences, size=embedding_size, window=3, min_count=1, workers=1,sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az8IHvUV6KAU"
      },
      "source": [
        "FastText offers embedding calculated with its algorithm in 157 languages, just download and use them!\n",
        "\n",
        "Have a try with Italian embedding, you only need to download it from https://fasttext.cc/docs/en/crawl-vectors.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD8ecaXxgLvt"
      },
      "source": [
        "---------------------------------\n",
        "Final Exercise:: Sentiment Classification with fastText and Keras at link --> https://bit.ly/37HnVwU "
      ]
    }
  ]
}