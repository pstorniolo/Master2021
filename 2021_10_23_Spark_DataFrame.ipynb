{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-23-Spark_DataFrame.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6zMhqLbofMeyFwW0ahY/9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_23_Spark_DataFrame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAcoCqXdg14_"
      },
      "source": [
        "#DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz8R62ztbWw8"
      },
      "source": [
        "# Install Spark 3.2.0\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f *.tgz*\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark==3.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbD0ukPubXws"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "from datetime import datetime, date\n",
        "import pandas as pd\n",
        "\n",
        "spark=SparkSession.builder.appName(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ZcPaOne3hO"
      },
      "source": [
        "##Create a PySpark DataFrame from a list of rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYXt54f2cifh"
      },
      "source": [
        "df = spark.createDataFrame([\n",
        "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
        "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
        "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
        "])\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu1cWmclDtwY"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOKAC5M-fOL7"
      },
      "source": [
        "##Create a PySpark DataFrame with an explicit schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgzh54qqfVl1"
      },
      "source": [
        "df = spark.createDataFrame([\n",
        "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
        "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
        "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
        "], schema='a bigint, b double, c string, d date, e timestamp')\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3Hfu_egYQ0"
      },
      "source": [
        "##Create a PySpark DataFrame from a pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2dprCFHchHB"
      },
      "source": [
        "pandas_df = pd.DataFrame({\n",
        "    'a': [1, 2, 3],\n",
        "    'b': [2., 3., 4.],\n",
        "    'c': ['string1', 'string2', 'string3'],\n",
        "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
        "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
        "})\n",
        "print(pandas_df)\n",
        "\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abZ0JT7zFLtd"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S_fWkJch3Ck"
      },
      "source": [
        "## Read data with Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ps9xt2GXH-"
      },
      "source": [
        "!ls -la sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKAf0702lblY"
      },
      "source": [
        "### CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp5zT0BXh8WB"
      },
      "source": [
        "pandas_df = pd.read_csv('sample_data/california_housing_train.csv')\n",
        "\n",
        "california_housing_df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "california_housing_df.show()\n",
        "california_housing_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEgShujbH6Jz"
      },
      "source": [
        "!cat sample_data/anscombe.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8Wp8r2lkAu"
      },
      "source": [
        "###JSON file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELqf6s39ljVf"
      },
      "source": [
        "pandas_df = pd.read_json('sample_data/anscombe.json')\n",
        "\n",
        "print(pandas_df)\n",
        "anscombe_df = spark.createDataFrame(pandas_df)\n",
        "anscombe_df.show()\n",
        "anscombe_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T84OLdiipxEG"
      },
      "source": [
        "anscombe_df.show(2, vertical=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB-aHwEfpzSy"
      },
      "source": [
        "anscombe_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VISOdJhGqdUN"
      },
      "source": [
        "anscombe_df.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F56PSi5q0Dr"
      },
      "source": [
        "california_housing_df.take(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtZdaBh4rAhx"
      },
      "source": [
        "##Selecting and Accessing Data\n",
        "\n",
        "PySpark DataFrame viene valutato in \"modo minimale\" e la semplice selezione di una colonna non attiva un calcolo ma restituisce un'istanza di colonna.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpGbPfkuzLlO"
      },
      "source": [
        "df.a\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt5grFbDzSaR"
      },
      "source": [
        "La maggior parte delle operazioni per colonna restituisce colonne."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z62WzxjxrOaO"
      },
      "source": [
        "from pyspark.sql import Column\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "type(df.c) == type(upper(df.c)) == type(df.c.isNull())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3TsNVSfs6In"
      },
      "source": [
        "Queste colonne possono essere utilizzate per selezionare le colonne da un DataFrame. Per esempio, **DataFrame.select()** accetta le istanze di *Column* e restituisce un altro DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiGA5IY6sVwN"
      },
      "source": [
        "df.select(df.c).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0JbM4XDtNDp"
      },
      "source": [
        "Assegna una nuova istanza di colonna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjEwuctqscJT"
      },
      "source": [
        "df.withColumn('upperC', upper(df.c)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivj9UTHJQN17"
      },
      "source": [
        "df.withColumn('C', upper(df.c)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaoGu7MaQYke"
      },
      "source": [
        "df.withColumn('B', upper(df.c)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htDsDayltRwS"
      },
      "source": [
        "Per selezionare un sottoinsieme di righe, si utilizza **DataFrame.filter()**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAQlmzmmsmm7"
      },
      "source": [
        "df.filter(df.a == 1).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiav59AgQuNE"
      },
      "source": [
        "df.filter(df.a != 1).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7AlbA1DuMsO"
      },
      "source": [
        "##Applying a Function\n",
        "\n",
        "PySpark supporta varie UDF (*user-defined function*) e API per consentire agli utenti di eseguire funzioni native di Python.L'esempio seguente consente agli utenti di utilizzare direttamente le API in una serie di panda all'interno della funzione nativa di Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GPckeYQuScj"
      },
      "source": [
        "import pandas\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf('long')\n",
        "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
        "    # Simply plus one by using pandas Series.\n",
        "    return series + 1\n",
        "\n",
        "df.select(pandas_plus_one(df.a)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmH78QGEuxEd"
      },
      "source": [
        "Un altro esempio è **DataFrame.mapInPandas** che consente agli utenti di utilizzare direttamente le API in un DataFrame panda senza alcuna restrizione come la lunghezza del risultato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3m78Pl8u1Hs"
      },
      "source": [
        "def pandas_filter_func(iterator):\n",
        "    for pandas_df in iterator:\n",
        "        yield pandas_df[pandas_df.a == 1]\n",
        "\n",
        "df.mapInPandas(pandas_filter_func, schema=df.schema).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePu_BwIRhFM"
      },
      "source": [
        "def pandas_filter_func(iterator):\n",
        "    for pandas_df in iterator:\n",
        "        yield pandas_df[pandas_df.a != 1]\n",
        "\n",
        "df.mapInPandas(pandas_filter_func, schema=df.schema).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYQh8dHBxMVl"
      },
      "source": [
        "##Grouping Data\n",
        "\n",
        "PySpark DataFrame fornisce anche un modo per gestire i dati raggruppati utilizzando l'approccio comune, la strategia *split-apply-combine*. Raggruppa i dati in base a una determinata condizione applica una funzione a ciascun gruppo e quindi li combina di nuovo al DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmH48b7IxQhR"
      },
      "source": [
        "df = spark.createDataFrame([\n",
        "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
        "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
        "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], \n",
        "    schema=['color', 'fruit', 'v1', 'v2'])\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W14mSwLc0rOw"
      },
      "source": [
        "Raggruppamento e quindi applicazione della funzione **avg()** ai gruppi risultanti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIx_17AFU6o9"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCjKL1v600XJ"
      },
      "source": [
        "df.groupby('color').avg().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6IddIvJ0_AS"
      },
      "source": [
        "df.groupby('fruit').avg().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUb60sZb1KkF"
      },
      "source": [
        "Si può applicare anche una funzione nativa Python a ciascun gruppo utilizzando l'API pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCRf4MYl1Wef"
      },
      "source": [
        "def plus_mean(pandas_df):\n",
        "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
        "\n",
        "df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiNzptNi2Ahn"
      },
      "source": [
        "Co-raggruppamento e applicazione di una funzione.\n",
        "\n",
        "---\n",
        "`pandas.merge_asof(left, right, on=None, left_on=None, right_on=None, left_index=False, right_index=False, by=None, left_by=None, right_by=None, suffixes=('_x', '_y'), tolerance=None, allow_exact_matches=True, direction='backward')`\n",
        "\n",
        "---\n",
        "L'unione asof è simile a un left-join tranne per il fatto che abbiniamo la chiave più vicina anziché le chiavi uguali.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaTuqiqm2G2K"
      },
      "source": [
        "df1 = spark.createDataFrame(\n",
        "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
        "    ('time', 'id', 'v1'))\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame(\n",
        "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
        "    ('time', 'id', 'v2'))\n",
        "df2.show()\n",
        "\n",
        "def asof_join(l, r):\n",
        "    return pd.merge_asof(l, r, on='time', by='id')\n",
        "\n",
        "df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n",
        "    asof_join, schema='time int, id int, v1 double, v2 string').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdWPzE_yHjvt"
      },
      "source": [
        "df1.printSchema()\n",
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhREYG6H6PQU"
      },
      "source": [
        "##Getting Data in/out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPqxcBdByfu"
      },
      "source": [
        "!rm -rf foo*\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
        "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
        "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], \n",
        "    schema=['color', 'fruit', 'v1', 'v2'])\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tj2KCOX7f3l"
      },
      "source": [
        "###Read & Write **CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RoBsCjC6YEb"
      },
      "source": [
        "df.write.csv('foo.csv')\n",
        "spark.read.csv('foo.csv').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-5N684udy2L"
      },
      "source": [
        "!rm -rf foo*\n",
        "df.write.csv('foo.csv', header=True)\n",
        "spark.read.csv('foo.csv', header=True).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gu8Y8fkdHam"
      },
      "source": [
        "!ls -al foo.csv/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdEGE1tbeTJz"
      },
      "source": [
        "!cat foo.csv/part-00000-bc27a3ed-ea6a-48bb-89af-e8fecbdc72a3-c000.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka2NE0rPfEk3"
      },
      "source": [
        "!cat foo.csv/part-00001-bc27a3ed-ea6a-48bb-89af-e8fecbdc72a3-c000.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUlYcS8U79nZ"
      },
      "source": [
        "###Write & Read **Parquet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6yrIsAW7ePF"
      },
      "source": [
        "df.write.parquet('foo.parquet')\n",
        "spark.read.parquet('foo.parquet').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcKPk1LfgrhR"
      },
      "source": [
        "!ls -alh foo.parquet/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ4B1oW_8fZz"
      },
      "source": [
        "###Read & Write **ORC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKxN-2WE8jCl"
      },
      "source": [
        "df.write.orc('foo.orc')\n",
        "spark.read.orc('foo.orc').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPzFvIno8zhN"
      },
      "source": [
        "!ls -la foo*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KdsBj1MJFpI"
      },
      "source": [
        "pandas_df = df.toPandas()\n",
        "print(pandas_df)\n",
        "pandas_df.to_csv('foo_pandas.csv', index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCeMjCEAhtdY"
      },
      "source": [
        "!ls -la foo_pandas.csv\n",
        "!cat foo_pandas.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgQZRkL_iMs8"
      },
      "source": [
        "pandas_df = df.toPandas()\n",
        "print(pandas_df)\n",
        "pandas_df.to_csv('foo_pandas.csv', index=True, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s06NTv_4iSL5"
      },
      "source": [
        "!cat foo_pandas.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzRrzkKFiou7"
      },
      "source": [
        "p_df=pd.read_csv('foo_pandas.csv', index_col=0)\n",
        "print(p_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykHU88sGjjVw"
      },
      "source": [
        "spark.createDataFrame(p_df).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vxu69eq_Cmx"
      },
      "source": [
        "##Working with SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_KOKUqU_QJE"
      },
      "source": [
        "DataFrame e Spark SQL condividono lo stesso motore di esecuzione in modo che possano essere utilizzati in modo intercambiabile senza problemi. Ad esempio, si può registrare un DataFrame come tabella ed eseguire facilmente una query SQL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D91M2q4xlam2"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9So5OrJF_GtT"
      },
      "source": [
        "df.createOrReplaceTempView(\"tableA\")\n",
        "df.printSchema()\n",
        "spark.sql(\"SELECT count(*) from tableA\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR9afxXZmDw8"
      },
      "source": [
        "spark.sql(\"SELECT fruit, count(*) from tableA group by fruit\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qS-UU9_kNI"
      },
      "source": [
        "Inoltre, le UDF possono essere registrate e richiamate in SQL immediatamente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atr4Or3L_MuW"
      },
      "source": [
        "@pandas_udf(\"integer\")\n",
        "def add_one(s: pd.Series) -> pd.Series:\n",
        "    return s + 1\n",
        "\n",
        "spark.udf.register(\"add_one\", add_one)\n",
        "spark.sql(\"SELECT color, fruit, v1, add_one(v1) FROM tableA\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLxXXtic_5tO"
      },
      "source": [
        "Queste espressioni SQL possono essere mescolate direttamente e utilizzate come colonne PySpark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-c9miuY_6vs"
      },
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "df.selectExpr('add_one(v1)').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzzwhocHpMn6"
      },
      "source": [
        "df.select(expr('count(*)') > 0).show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}