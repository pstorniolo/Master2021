{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-28-Spark_Examples.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODwC4QdYibrUVoukZAbYvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_28_Spark_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f6H7MzVTOgK"
      },
      "source": [
        "# Install Spark 3.2.0 - JDK11\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f *.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "#Install findspark using pip to make pyspark importable as regular library\n",
        "!pip -q install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sql = spark.sql\n",
        "\n",
        "print(\"\\nApache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSFa9ihZUdzQ"
      },
      "source": [
        "##Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MgoHBf0TgMt"
      },
      "source": [
        "import pandas as pd    \n",
        "data = [[\"James\",\"\",\"Smith\",30,\"M\",60000], \n",
        "        [\"Michael\",\"Rose\",\"\",50,\"M\",70000], \n",
        "        [\"Robert\",\"\",\"Williams\",42,\"\",400000], \n",
        "        [\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000], \n",
        "        [\"Jen\",\"Mary\",\"Brown\",45,None,0]] \n",
        "columns = ['First Name', 'Middle Name','Last Name','Age','Gender','Salary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVwszUFETliZ"
      },
      "source": [
        "# Create the pandas DataFrame \n",
        "pandasDF = pd.DataFrame(data=data, columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM5jNn_5Tpa3"
      },
      "source": [
        "# print dataframe. \n",
        "print(pandasDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBH3gW7gTseP"
      },
      "source": [
        "#Outputs below data on console\n",
        "\n",
        "pdCount=pandasDF.count()\n",
        "print(pdCount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfSiL-b7Txda"
      },
      "source": [
        "print(pandasDF.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH6oFC0v3Wv3"
      },
      "source": [
        "print(pandasDF.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQX6_baU4eD"
      },
      "source": [
        "##Convert Column Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z881TktuVxQy"
      },
      "source": [
        "###Example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x6sZXsTVBMU"
      },
      "source": [
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
        "  ]\n",
        "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "df=spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "print(df.collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfVAmDUNVCuA"
      },
      "source": [
        "states1=df.rdd.map(lambda x: x[3]).collect()\n",
        "print(states1)\n",
        "\n",
        "from collections import OrderedDict \n",
        "res = list(OrderedDict.fromkeys(states1)) \n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_17EMRrV2iL"
      },
      "source": [
        "###Example 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJqZHshuVIW5"
      },
      "source": [
        "states2=df.rdd.map(lambda x: x.state).collect()\n",
        "print(states2)\n",
        "#['CA', 'NY', 'CA', 'FL']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoH5Fla4VNK_"
      },
      "source": [
        "states3=df.select(df.state).collect()\n",
        "print(states3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8UNc-2UVRWq"
      },
      "source": [
        "states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
        "print(states4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9xs-zx-VWDJ"
      },
      "source": [
        "states5=df.select(df.state).toPandas()['state']\n",
        "print(states5)\n",
        "\n",
        "states6=list(states5)\n",
        "print(states6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WMuocqAVZN8"
      },
      "source": [
        "pandDF=df.select(df.state,df.firstname).toPandas()\n",
        "print(list(pandDF['state']))\n",
        "print(list(pandDF['firstname']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGXzdNUyWLVL"
      },
      "source": [
        "##Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILCEfkFNX0pc"
      },
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
        "\n",
        "print(data)\n",
        "\n",
        "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
        "  .select(col(\"date\"),col(\"increment\"), \\\n",
        "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec15WgoMYbAF"
      },
      "source": [
        "##Aggregate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jezzBWMcYdRS"
      },
      "source": [
        "from pyspark.sql.functions import approx_count_distinct, collect_list\n",
        "from pyspark.sql.functions import collect_set, sum, avg, max, countDistinct, count\n",
        "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
        "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
        "from pyspark.sql.functions import variance, var_samp, var_pop\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_GcP-IZYfOe"
      },
      "source": [
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkJP0_udbpUW"
      },
      "source": [
        "print(\"approx_count_distinct: \" + str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCHgh_T3YpLc"
      },
      "source": [
        "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
        "\n",
        "df.select(collect_list(\"salary\")).show(truncate=False)\n",
        "\n",
        "df.select(collect_set(\"salary\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVHT0rSAZA10"
      },
      "source": [
        "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
        "df2.show(truncate=False)\n",
        "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3858Kd6b_QSP"
      },
      "source": [
        "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JImQ4cmd_ac0"
      },
      "source": [
        "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-rQ5AstZHBX"
      },
      "source": [
        "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
        "\n",
        "df.select(first(\"salary\")).show(truncate=False)\n",
        "df.select(last(\"salary\")).show(truncate=False)\n",
        "df.select(max(\"salary\")).show(truncate=False)\n",
        "df.select(min(\"salary\")).show(truncate=False)\n",
        "df.select(mean(\"salary\")).show(truncate=False)\n",
        "df.select(sum(\"salary\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_0vOgEMB8N_"
      },
      "source": [
        "Nella teoria della probabilità e nella statistica, la distribuzione normale asimmetrica è una distribuzione di probabilità continua che generalizza la distribuzione normale per consentire l'asimmetria *(skewness)* diversa da zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN_-IINpB9M9"
      },
      "source": [
        "df.select(skewness(\"salary\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUHdQeUdBZyy"
      },
      "source": [
        "La curtosi *(kurtosis)* è principalmente una misura per descrivere la forma di una distribuzione di probabilità e in particolare la sua \"coda\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwUFK8AwBciK"
      },
      "source": [
        "df.select(kurtosis(\"salary\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvc8_4XgBWEc"
      },
      "source": [
        "Le funzioni STDDEV_POP() e STDDEV_SAMP() calcolano rispettivamente la deviazione standard della popolazione e la deviazione standard del campione dei valori di input. (STDDEV() è un alias per STDDEV_SAMP().) Entrambe le funzioni valutano tutte le righe di input corrispondenti alla query. La differenza è che STDDEV_SAMP() viene ridimensionato di 1/(N-1) mentre STDDEV_POP() viene ridimensionato di 1/N."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPNlZBr8BXgI"
      },
      "source": [
        "df.select(stddev(\"salary\"),stddev_samp(\"salary\"),stddev_pop(\"salary\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH4FglEsC1Nk"
      },
      "source": [
        "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukpfDuypaFgV"
      },
      "source": [
        "##Array & String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92KARC7yZeib"
      },
      "source": [
        "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
        "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
        "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
        "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PIB6DnBaRXc"
      },
      "source": [
        "from pyspark.sql.functions import col, concat_ws\n",
        "df2 = df.withColumn(\"languagesAtSchool\", concat_ws(\",\",col(\"languagesAtSchool\")))\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fox3IqzbaaXy"
      },
      "source": [
        "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
        "spark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,\" + \\\n",
        "    \" currentState from ARRAY_STRING\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNwxMLZ1avAE"
      },
      "source": [
        "##Array Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwCTtKpzax1x"
      },
      "source": [
        "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
        "arrayCol = ArrayType(StringType(),False)\n",
        "\n",
        "data = [\n",
        " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
        " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
        " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
        "]\n",
        "\n",
        "schema = StructType([ \n",
        "    StructField(\"name\",StringType(),True), \n",
        "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
        "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
        "    StructField(\"currentState\", StringType(), True), \n",
        "    StructField(\"previousState\", StringType(), True) \n",
        "  ])\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm2Fb74qa9wd"
      },
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df.select(df.name,explode(df.languagesAtSchool)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV_yWSblbB_H"
      },
      "source": [
        "from pyspark.sql.functions import split\n",
        "df.select(split(df.name,\",\").alias(\"nameAsArray\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDkaW-2WbFC3"
      },
      "source": [
        "from pyspark.sql.functions import array\n",
        "df.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgGzcLGMbH-g"
      },
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\").alias(\"array_contains\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAXDxmwMh4N0"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xpujW5pbUhe"
      },
      "source": [
        "##Broadcast DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LlENRpHbiUK"
      },
      "source": [
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates = spark.sparkContext.broadcast(states)\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNR_7J5hbkx6"
      },
      "source": [
        "def state_convert(code):\n",
        "    return broadcastStates.value[code]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoK8Xf21bq9H"
      },
      "source": [
        "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
        "result.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeC2vBupeq4D"
      },
      "source": [
        "##Cast Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIU1sZIPetVJ"
      },
      "source": [
        "simpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n",
        "    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n",
        "    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n",
        "  ]\n",
        "\n",
        "columns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqdGuDrhev5Z"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StringType, BooleanType, DateType\n",
        "df2 = df.withColumn(\"age\",col(\"age\").cast(StringType())) \\\n",
        "    .withColumn(\"isGraduated\",col(\"isGraduated\").cast(BooleanType())) \\\n",
        "    .withColumn(\"jobStartDate\",col(\"jobStartDate\").cast(DateType()))\n",
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmPTXKjze0vK"
      },
      "source": [
        "df3 = df2.selectExpr(\"cast(age as int) age\",\n",
        "    \"cast(isGraduated as string) isGraduated\",\n",
        "    \"cast(jobStartDate as string) jobStartDate\")\n",
        "df3.printSchema()\n",
        "df3.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWaWxJ4ae_oI"
      },
      "source": [
        "df3.createOrReplaceTempView(\"CastExample\")\n",
        "df4 = spark.sql(\"SELECT STRING(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\n",
        "df4.printSchema()\n",
        "df4.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOJFdHfThAze"
      },
      "source": [
        "##Change string --> double"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK5e64ErhYxn"
      },
      "source": [
        "from pyspark.sql.types import DoubleType, IntegerType\n",
        "\n",
        "simpleData = [(\"James\",\"34\",\"true\",\"M\",\"3000.6089\"),\n",
        "    (\"Michael\",\"33\",\"true\",\"F\",\"3300.8067\"),\n",
        "    (\"Robert\",\"37\",\"false\",\"M\",\"5000.5034\")\n",
        "  ]\n",
        "\n",
        "columns = [\"firstname\",\"age\",\"isGraduated\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGbP0i89hbNQ"
      },
      "source": [
        "from pyspark.sql.functions import col,round,expr\n",
        "df.withColumn(\"salary\",df.salary.cast('double')).printSchema()    \n",
        "df.withColumn(\"salary\",df.salary.cast(DoubleType())).printSchema()    \n",
        "df.withColumn(\"salary\",col(\"salary\").cast('double')).printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USyTvFSoiUS_"
      },
      "source": [
        "df.selectExpr(\"firstname\",\"isGraduated\",\"cast(salary as double) salary\").printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu2KDQjDiWGR"
      },
      "source": [
        "df.createOrReplaceTempView(\"CastExample\")\n",
        "spark.sql(\"SELECT firstname,isGraduated,DOUBLE(salary) as salary from CastExample\").printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}