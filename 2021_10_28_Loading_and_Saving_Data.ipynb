{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-28-Loading_and_Saving_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZD1zvV9JtRsN",
        "6l9c1Jdp5O6c"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_28_Loading_and_Saving_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD1zvV9JtRsN"
      },
      "source": [
        "# Loading and Saving Data in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXcc49lmUYgz"
      },
      "source": [
        "Collab Only code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_U5TbtAUX_r"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "#Install findspark using pip to make pyspark importable as regular library\n",
        "!pip -q install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRIu4xkU2qN"
      },
      "source": [
        "Reading a text file textFile() in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMHp9gMPUyPr"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "lines = sc.textFile(\"spark-3.2.0-bin-hadoop3.2/README.md\")\n",
        "lines.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPoJs6cguwyh"
      },
      "source": [
        "Loading all the .md files in one directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ccsKyvu0qT"
      },
      "source": [
        "input = sc.textFile(\"spark-3.2.0-bin-hadoop3.2/*.md\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f8uBm53VM1H"
      },
      "source": [
        "---\n",
        "**Load file in Google Colab:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeQp5sG3VKqR"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dvuLOn-N4xB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEm91_EgObTr"
      },
      "source": [
        "##Open**Data** Regione Siciliana\n",
        "https://dati.regione.sicilia.it/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fFed7dPoAC"
      },
      "source": [
        "####Load CSV from URL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr1eisBrGvES"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://dati.regione.sicilia.it/download/dataset/arpa-qualita-aria-2019/filesystem/arpa-qualita-aria-2019-10_csv.zip'  \n",
        "file_name = url.split('/')[-1]\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(file_name,\"wb\").write(r.content)\n",
        "print(file_name)\n",
        "os.system(\"unzip \"+file_name)\n",
        "!rm -f *.zip; ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9w4g1vh1e8j"
      },
      "source": [
        "###**Reading a CSV file into a DataFrame, filter some columns and save it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuKbsrvUVhse"
      },
      "source": [
        "# Create DataFrame\n",
        "df = spark.read.csv(\"arpa-qualita-aria-2019-10_csv.csv\",sep=\";\",header=True,inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ICHjQoP1hrz"
      },
      "source": [
        "Filter data by several columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXyJNFVWjux"
      },
      "source": [
        "dataF=df.select(\"stazione_id\",\"misura_dataora\",\"misura_valore\")\n",
        "dataF.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39l8vOAA18x9"
      },
      "source": [
        "Save only the filtered Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxlQXKjg11mM"
      },
      "source": [
        "dataF.write.csv(\"dati_filtrati.csv\",mode='overwrite',header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1cg9iM93qCg"
      },
      "source": [
        "Let's read this new file back into an RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-iSzucQ3sup"
      },
      "source": [
        "data = spark.read.csv(\"dati_filtrati.csv\",header=True,inferSchema=True)\n",
        "data.printSchema()\n",
        "data.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvVuvyJvTG2y"
      },
      "source": [
        "data.createOrReplaceTempView(\"table01\")\n",
        "spark.sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l9c1Jdp5O6c"
      },
      "source": [
        "# **Hive Example**\n",
        "\n",
        "Using Hive to create and read a table - Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0sp_TGJUCg0"
      },
      "source": [
        "from pyspark.sql import HiveContext, Row\n",
        "\n",
        "hiveCtx = HiveContext(sc)\n",
        "ex1 = hiveCtx.read.csv(\"dati_filtrati.csv\",header=True,inferSchema=True)\n",
        "\n",
        "ex1.registerTempTable(\"Table02\")\n",
        "results = hiveCtx.sql(\"SELECT * FROM table02\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMN4Tj0HtFKO"
      },
      "source": [
        "spark.sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TmSIqLotL0u"
      },
      "source": [
        "#Spark SQL\n",
        "\n",
        "(https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
        "\n",
        "**Reference**\n",
        "\n",
        "(https://spark.apache.org/docs/latest/sql-ref.html)"
      ]
    }
  ]
}