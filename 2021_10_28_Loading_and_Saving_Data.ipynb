{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-28-Loading_and_Saving_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_28_Loading_and_Saving_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD1zvV9JtRsN"
      },
      "source": [
        "# Loading and Saving Data in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXcc49lmUYgz"
      },
      "source": [
        "Collab Only code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_U5TbtAUX_r"
      },
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "#Install findspark using pip to make pyspark importable as regular library\n",
        "!pip -q install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRIu4xkU2qN"
      },
      "source": [
        "Reading a text file textFile() in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMHp9gMPUyPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c7c3bc-33cb-455a-9ff0-9d6e675f1ca0"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "lines = sc.textFile(\"spark-3.2.0-bin-hadoop3.2/README.md\")\n",
        "lines.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# Apache Spark',\n",
              " '',\n",
              " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
              " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
              " 'supports general computation graphs for data analysis. It also supports a',\n",
              " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
              " 'MLlib for machine learning, GraphX for graph processing,',\n",
              " 'and Structured Streaming for stream processing.',\n",
              " '',\n",
              " '<https://spark.apache.org/>',\n",
              " '',\n",
              " '[![GitHub Action Build](https://github.com/apache/spark/actions/workflows/build_and_test.yml/badge.svg?branch=master)](https://github.com/apache/spark/actions/workflows/build_and_test.yml?query=branch%3Amaster)',\n",
              " '[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2)',\n",
              " '[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
              " '[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
              " '',\n",
              " '',\n",
              " '## Online Documentation',\n",
              " '',\n",
              " 'You can find the latest Spark documentation, including a programming',\n",
              " 'guide, on the [project web page](https://spark.apache.org/documentation.html).',\n",
              " 'This README file only contains basic setup instructions.',\n",
              " '',\n",
              " '## Building Spark',\n",
              " '',\n",
              " 'Spark is built using [Apache Maven](https://maven.apache.org/).',\n",
              " 'To build Spark and its example programs, run:',\n",
              " '',\n",
              " '    ./build/mvn -DskipTests clean package',\n",
              " '',\n",
              " '(You do not need to do this if you downloaded a pre-built package.)',\n",
              " '',\n",
              " 'More detailed documentation is available from the project site, at',\n",
              " '[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
              " '',\n",
              " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).',\n",
              " '',\n",
              " '## Interactive Scala Shell',\n",
              " '',\n",
              " 'The easiest way to start using Spark is through the Scala shell:',\n",
              " '',\n",
              " '    ./bin/spark-shell',\n",
              " '',\n",
              " 'Try the following command, which should return 1,000,000,000:',\n",
              " '',\n",
              " '    scala> spark.range(1000 * 1000 * 1000).count()',\n",
              " '',\n",
              " '## Interactive Python Shell',\n",
              " '',\n",
              " 'Alternatively, if you prefer Python, you can use the Python shell:',\n",
              " '',\n",
              " '    ./bin/pyspark',\n",
              " '',\n",
              " 'And run the following command, which should also return 1,000,000,000:',\n",
              " '',\n",
              " '    >>> spark.range(1000 * 1000 * 1000).count()',\n",
              " '',\n",
              " '## Example Programs',\n",
              " '',\n",
              " 'Spark also comes with several sample programs in the `examples` directory.',\n",
              " 'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
              " '',\n",
              " '    ./bin/run-example SparkPi',\n",
              " '',\n",
              " 'will run the Pi example locally.',\n",
              " '',\n",
              " 'You can set the MASTER environment variable when running examples to submit',\n",
              " 'examples to a cluster. This can be a mesos:// or spark:// URL,',\n",
              " '\"yarn\" to run on YARN, and \"local\" to run',\n",
              " 'locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
              " 'can also use an abbreviated class name if the class is in the `examples`',\n",
              " 'package. For instance:',\n",
              " '',\n",
              " '    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
              " '',\n",
              " 'Many of the example programs print usage help if no params are given.',\n",
              " '',\n",
              " '## Running Tests',\n",
              " '',\n",
              " 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
              " 'can be run using:',\n",
              " '',\n",
              " '    ./dev/run-tests',\n",
              " '',\n",
              " 'Please see the guidance on how to',\n",
              " '[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
              " '',\n",
              " 'There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
              " '',\n",
              " '## A Note About Hadoop Versions',\n",
              " '',\n",
              " 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
              " 'storage systems. Because the protocols have changed in different versions of',\n",
              " 'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
              " '',\n",
              " 'Please refer to the build documentation at',\n",
              " '[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
              " 'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
              " 'building for particular Hive and Hive Thriftserver distributions.',\n",
              " '',\n",
              " '## Configuration',\n",
              " '',\n",
              " 'Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
              " 'in the online documentation for an overview on how to configure Spark.',\n",
              " '',\n",
              " '## Contributing',\n",
              " '',\n",
              " 'Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)',\n",
              " 'for information on how to get started contributing to the project.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPoJs6cguwyh"
      },
      "source": [
        "Loading all the .md files in one directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ccsKyvu0qT"
      },
      "source": [
        "input = sc.textFile(\"spark-3.2.0-bin-hadoop3.2/*.md\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f8uBm53VM1H"
      },
      "source": [
        "---\n",
        "**Load file in Google Colab:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeQp5sG3VKqR",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "0c9c41f1-34a4-4c6c-d9d4-ed59c9206060"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c4a58ce8-9169-4d35-b376-495fb67a9356\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c4a58ce8-9169-4d35-b376-495fb67a9356\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2021-10-28-Loading_and_Saving_Data.ipynb to 2021-10-28-Loading_and_Saving_Data.ipynb\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2021-10-28-Loading_and_Saving_Data.ipynb': b'{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"2021-10-28-Loading_and_Saving_Data.ipynb\",\"provenance\":[{\"file_id\":\"1BQpDSrMYqPRxWqJisQJHqCmjWzCGFg_X\",\"timestamp\":1635367621926}],\"collapsed_sections\":[]},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"}},\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"ZD1zvV9JtRsN\"},\"source\":[\"# Loading and Saving Data in Spark\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"BXcc49lmUYgz\"},\"source\":[\"Collab Only code:\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"L_U5TbtAUX_r\"},\"source\":[\"!apt-get install openjdk-11-jdk-headless -qq > /dev/null\\\\n\",\"!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\\\\n\",\"!tar xf spark-3.2.0-bin-hadoop3.2.tgz\\\\n\",\"!rm -f spark-3.2.0-bin-hadoop3.2.tgz\\\\n\",\"\\\\n\",\"import os\\\\n\",\"os.environ[\\\\\"JAVA_HOME\\\\\"] = \\\\\"/usr/lib/jvm/java-11-openjdk-amd64\\\\\"\\\\n\",\"os.environ[\\\\\"SPARK_HOME\\\\\"] = \\\\\"/content/spark-3.2.0-bin-hadoop3.2\\\\\"\\\\n\",\"\\\\n\",\"#Install findspark using pip to make pyspark importable as regular library\\\\n\",\"!pip -q install findspark\\\\n\",\"import findspark\\\\n\",\"findspark.init()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"KbRIu4xkU2qN\"},\"source\":[\"Reading a text file textFile() in Python\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"yMHp9gMPUyPr\"},\"source\":[\"from pyspark.sql import SparkSession\\\\n\",\"\\\\n\",\"spark = SparkSession.builder \\\\\\\\\\\\n\",\"    .master(\\\\\"local[*]\\\\\") \\\\\\\\\\\\n\",\"    .appName(\\\\\"Learning_Spark\\\\\") \\\\\\\\\\\\n\",\"    .enableHiveSupport() \\\\\\\\\\\\n\",\"    .getOrCreate()\\\\n\",\"\\\\n\",\"sc = spark.sparkContext\\\\n\",\"\\\\n\",\"lines = sc.textFile(\\\\\"spark-3.2.0-bin-hadoop3.2/README.md\\\\\")\\\\n\",\"lines.collect()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"uPoJs6cguwyh\"},\"source\":[\"Loading all the .md files in one directory\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"K3ccsKyvu0qT\"},\"source\":[\"input = sc.textFile(\\\\\"spark-3.2.0-bin-hadoop3.2/*.md\\\\\")\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"5f8uBm53VM1H\"},\"source\":[\"---\\\\n\",\"**Load file in Google Colab:**\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"jeQp5sG3VKqR\"},\"source\":[\"from google.colab import files\\\\n\",\"files.upload()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"9dvuLOn-N4xB\"},\"source\":[\"---\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"hEm91_EgObTr\"},\"source\":[\"##Open**Data** Regione Siciliana\\\\n\",\"https://dati.regione.sicilia.it/dataset\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"17fFed7dPoAC\"},\"source\":[\"####Load CSV from URL\\\\n\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"yr1eisBrGvES\"},\"source\":[\"import requests\\\\n\",\"\\\\n\",\"url = \\'https://dati.regione.sicilia.it/download/dataset/arpa-qualita-aria-2019/filesystem/arpa-qualita-aria-2019-10_csv.zip\\'  \\\\n\",\"file_name = url.split(\\'/\\')[-1]\\\\n\",\"r = requests.get(url, allow_redirects=True)\\\\n\",\"open(file_name,\\\\\"wb\\\\\").write(r.content)\\\\n\",\"print(file_name)\\\\n\",\"os.system(\\\\\"unzip \\\\\"+file_name)\\\\n\",\"!rm -f *.zip; ls -la\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"j9w4g1vh1e8j\"},\"source\":[\"###**Reading a CSV file into a DataFrame, filter some columns and save it**\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"GuKbsrvUVhse\"},\"source\":[\"# Create DataFrame\\\\n\",\"df = spark.read.csv(\\\\\"arpa-qualita-aria-2019-10_csv.csv\\\\\",sep=\\\\\";\\\\\",header=True,inferSchema=True)\\\\n\",\"df.printSchema()\\\\n\",\"df.show(5)\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"3ICHjQoP1hrz\"},\"source\":[\"Filter data by several columns\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"2hXyJNFVWjux\"},\"source\":[\"dataF=df.select(\\\\\"stazione_id\\\\\",\\\\\"misura_dataora\\\\\",\\\\\"misura_valore\\\\\")\\\\n\",\"dataF.show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"39l8vOAA18x9\"},\"source\":[\"Save only the filtered Data\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xxlQXKjg11mM\"},\"source\":[\"dataF.write.csv(\\\\\"dati_filtrati.csv\\\\\",mode=\\'overwrite\\',header=True)\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"i1cg9iM93qCg\"},\"source\":[\"Let\\'s read this new file back into an RDD\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"v-iSzucQ3sup\"},\"source\":[\"data = spark.read.csv(\\\\\"dati_filtrati.csv\\\\\",header=True,inferSchema=True)\\\\n\",\"data.printSchema()\\\\n\",\"data.show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"YvVuvyJvTG2y\"},\"source\":[\"data.createOrReplaceTempView(\\\\\"table01\\\\\")\\\\n\",\"spark.sql(\\\\\"show tables\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"6l9c1Jdp5O6c\"},\"source\":[\"# **Hive Example**\\\\n\",\"\\\\n\",\"Using Hive to create and read a table - Simple Example\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H0sp_TGJUCg0\"},\"source\":[\"from pyspark.sql import HiveContext, Row\\\\n\",\"\\\\n\",\"hiveCtx = HiveContext(sc)\\\\n\",\"ex1 = hiveCtx.read.csv(\\\\\"dati_filtrati.csv\\\\\",header=True,inferSchema=True)\\\\n\",\"\\\\n\",\"ex1.registerTempTable(\\\\\"Table02\\\\\")\\\\n\",\"results = hiveCtx.sql(\\\\\"SELECT * FROM table02\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"TMN4Tj0HtFKO\"},\"source\":[\"spark.sql(\\\\\"show tables\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"4_-qE8CwyFTw\"},\"source\":[\"sql = spark.sql\\\\n\",\"\\\\n\",\"sql(\\\\\"show tables\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"8TmSIqLotL0u\"},\"source\":[\"#Spark SQL\\\\n\",\"\\\\n\",\"(https://spark.apache.org/docs/latest/sql-programming-guide.html)\\\\n\",\"\\\\n\",\"**Reference**\\\\n\",\"\\\\n\",\"(https://spark.apache.org/docs/latest/sql-ref.html)\"]}]}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTM9FJ_LIU6q",
        "outputId": "65cb03cc-8243-4c54-d392-1bf4296e9028"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 28\n",
            "drwxr-xr-x  1 root root 4096 Oct 28 09:00 .\n",
            "drwxr-xr-x  1 root root 4096 Oct 28 08:51 ..\n",
            "-rw-r--r--  1 root root 5529 Oct 28 09:00 2021-10-28-Loading_and_Saving_Data.ipynb\n",
            "drwxr-xr-x  4 root root 4096 Oct 26 13:33 .config\n",
            "drwxr-xr-x  1 root root 4096 Oct 26 13:34 sample_data\n",
            "drwxr-xr-x 13 1000 1000 4096 Oct  6 13:18 spark-3.2.0-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui5NTJ3yIviy",
        "outputId": "cbf186cb-f5ca-41fd-aa18-c2d04bcec2bd"
      },
      "source": [
        "sc.textFile(\"2021-10-28-Loading_and_Saving_Data.ipynb\").collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\"nbformat\":4,\"nbformat_minor\":0,\"metadata\":{\"colab\":{\"name\":\"2021-10-28-Loading_and_Saving_Data.ipynb\",\"provenance\":[{\"file_id\":\"1BQpDSrMYqPRxWqJisQJHqCmjWzCGFg_X\",\"timestamp\":1635367621926}],\"collapsed_sections\":[]},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\"}},\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"ZD1zvV9JtRsN\"},\"source\":[\"# Loading and Saving Data in Spark\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"BXcc49lmUYgz\"},\"source\":[\"Collab Only code:\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"L_U5TbtAUX_r\"},\"source\":[\"!apt-get install openjdk-11-jdk-headless -qq > /dev/null\\\\n\",\"!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\\\\n\",\"!tar xf spark-3.2.0-bin-hadoop3.2.tgz\\\\n\",\"!rm -f spark-3.2.0-bin-hadoop3.2.tgz\\\\n\",\"\\\\n\",\"import os\\\\n\",\"os.environ[\\\\\"JAVA_HOME\\\\\"] = \\\\\"/usr/lib/jvm/java-11-openjdk-amd64\\\\\"\\\\n\",\"os.environ[\\\\\"SPARK_HOME\\\\\"] = \\\\\"/content/spark-3.2.0-bin-hadoop3.2\\\\\"\\\\n\",\"\\\\n\",\"#Install findspark using pip to make pyspark importable as regular library\\\\n\",\"!pip -q install findspark\\\\n\",\"import findspark\\\\n\",\"findspark.init()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"KbRIu4xkU2qN\"},\"source\":[\"Reading a text file textFile() in Python\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"yMHp9gMPUyPr\"},\"source\":[\"from pyspark.sql import SparkSession\\\\n\",\"\\\\n\",\"spark = SparkSession.builder \\\\\\\\\\\\n\",\"    .master(\\\\\"local[*]\\\\\") \\\\\\\\\\\\n\",\"    .appName(\\\\\"Learning_Spark\\\\\") \\\\\\\\\\\\n\",\"    .enableHiveSupport() \\\\\\\\\\\\n\",\"    .getOrCreate()\\\\n\",\"\\\\n\",\"sc = spark.sparkContext\\\\n\",\"\\\\n\",\"lines = sc.textFile(\\\\\"spark-3.2.0-bin-hadoop3.2/README.md\\\\\")\\\\n\",\"lines.collect()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"uPoJs6cguwyh\"},\"source\":[\"Loading all the .md files in one directory\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"K3ccsKyvu0qT\"},\"source\":[\"input = sc.textFile(\\\\\"spark-3.2.0-bin-hadoop3.2/*.md\\\\\")\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"5f8uBm53VM1H\"},\"source\":[\"---\\\\n\",\"**Load file in Google Colab:**\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"jeQp5sG3VKqR\"},\"source\":[\"from google.colab import files\\\\n\",\"files.upload()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"9dvuLOn-N4xB\"},\"source\":[\"---\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"hEm91_EgObTr\"},\"source\":[\"##Open**Data** Regione Siciliana\\\\n\",\"https://dati.regione.sicilia.it/dataset\"]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"17fFed7dPoAC\"},\"source\":[\"####Load CSV from URL\\\\n\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"yr1eisBrGvES\"},\"source\":[\"import requests\\\\n\",\"\\\\n\",\"url = \\'https://dati.regione.sicilia.it/download/dataset/arpa-qualita-aria-2019/filesystem/arpa-qualita-aria-2019-10_csv.zip\\'  \\\\n\",\"file_name = url.split(\\'/\\')[-1]\\\\n\",\"r = requests.get(url, allow_redirects=True)\\\\n\",\"open(file_name,\\\\\"wb\\\\\").write(r.content)\\\\n\",\"print(file_name)\\\\n\",\"os.system(\\\\\"unzip \\\\\"+file_name)\\\\n\",\"!rm -f *.zip; ls -la\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"j9w4g1vh1e8j\"},\"source\":[\"###**Reading a CSV file into a DataFrame, filter some columns and save it**\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"GuKbsrvUVhse\"},\"source\":[\"# Create DataFrame\\\\n\",\"df = spark.read.csv(\\\\\"arpa-qualita-aria-2019-10_csv.csv\\\\\",sep=\\\\\";\\\\\",header=True,inferSchema=True)\\\\n\",\"df.printSchema()\\\\n\",\"df.show(5)\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"3ICHjQoP1hrz\"},\"source\":[\"Filter data by several columns\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"2hXyJNFVWjux\"},\"source\":[\"dataF=df.select(\\\\\"stazione_id\\\\\",\\\\\"misura_dataora\\\\\",\\\\\"misura_valore\\\\\")\\\\n\",\"dataF.show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"39l8vOAA18x9\"},\"source\":[\"Save only the filtered Data\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"xxlQXKjg11mM\"},\"source\":[\"dataF.write.csv(\\\\\"dati_filtrati.csv\\\\\",mode=\\'overwrite\\',header=True)\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"i1cg9iM93qCg\"},\"source\":[\"Let\\'s read this new file back into an RDD\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"v-iSzucQ3sup\"},\"source\":[\"data = spark.read.csv(\\\\\"dati_filtrati.csv\\\\\",header=True,inferSchema=True)\\\\n\",\"data.printSchema()\\\\n\",\"data.show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"YvVuvyJvTG2y\"},\"source\":[\"data.createOrReplaceTempView(\\\\\"table01\\\\\")\\\\n\",\"spark.sql(\\\\\"show tables\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"6l9c1Jdp5O6c\"},\"source\":[\"# **Hive Example**\\\\n\",\"\\\\n\",\"Using Hive to create and read a table - Simple Example\"]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"H0sp_TGJUCg0\"},\"source\":[\"from pyspark.sql import HiveContext, Row\\\\n\",\"\\\\n\",\"hiveCtx = HiveContext(sc)\\\\n\",\"ex1 = hiveCtx.read.csv(\\\\\"dati_filtrati.csv\\\\\",header=True,inferSchema=True)\\\\n\",\"\\\\n\",\"ex1.registerTempTable(\\\\\"Table02\\\\\")\\\\n\",\"results = hiveCtx.sql(\\\\\"SELECT * FROM table02\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"TMN4Tj0HtFKO\"},\"source\":[\"spark.sql(\\\\\"show tables\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"4_-qE8CwyFTw\"},\"source\":[\"sql = spark.sql\\\\n\",\"\\\\n\",\"sql(\\\\\"show tables\\\\\").show()\"],\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"8TmSIqLotL0u\"},\"source\":[\"#Spark SQL\\\\n\",\"\\\\n\",\"(https://spark.apache.org/docs/latest/sql-programming-guide.html)\\\\n\",\"\\\\n\",\"**Reference**\\\\n\",\"\\\\n\",\"(https://spark.apache.org/docs/latest/sql-ref.html)\"]}]}']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dvuLOn-N4xB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEm91_EgObTr"
      },
      "source": [
        "##Open**Data** Regione Siciliana\n",
        "https://dati.regione.sicilia.it/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fFed7dPoAC"
      },
      "source": [
        "####Load CSV from URL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03f5yfwIKXC1",
        "outputId": "ae83b7d2-ca06-4f39-c09e-b684add1fb59"
      },
      "source": [
        "import requests\n",
        "\n",
        "url ='https://dati.regione.sicilia.it/dataset/57983370-d035-4dd2-9ea1-b9552b4c7fb7/resource/cbd34026-25a5-489f-a57d-9cfbd57b63cf/download/56.csv'\n",
        "\n",
        "file_name = url.split('/')[-1]\n",
        "print(file_name)\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(file_name,\"wb\").write(r.content)\n",
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56.csv\n",
            "total 148\n",
            "drwxr-xr-x  1 root root   4096 Oct 28 09:15 .\n",
            "drwxr-xr-x  1 root root   4096 Oct 28 08:51 ..\n",
            "-rw-r--r--  1 root root   5529 Oct 28 09:00 2021-10-28-Loading_and_Saving_Data.ipynb\n",
            "-rw-r--r--  1 root root 119493 Oct 28 09:15 56.csv\n",
            "drwxr-xr-x  4 root root   4096 Oct 26 13:33 .config\n",
            "drwxr-xr-x  1 root root   4096 Oct 26 13:34 sample_data\n",
            "drwxr-xr-x 13 1000 1000   4096 Oct  6 13:18 spark-3.2.0-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SCtFa9wLvAe"
      },
      "source": [
        "sc.textFile(\"56.csv\").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr1eisBrGvES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b8f97a-9458-4694-843a-3b7572b34451"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://dati.regione.sicilia.it/download/dataset/arpa-qualita-aria-2019/filesystem/arpa-qualita-aria-2019-10_csv.zip'  \n",
        "file_name = url.split('/')[-1]\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open(file_name,\"wb\").write(r.content)\n",
        "print(file_name)\n",
        "os.system(\"unzip \"+file_name)\n",
        "!rm -f *.zip; ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arpa-qualita-aria-2019-10_csv.zip\n",
            "total 5272\n",
            "drwxr-xr-x  1 root root    4096 Oct 28 09:16 .\n",
            "drwxr-xr-x  1 root root    4096 Oct 28 08:51 ..\n",
            "-rw-r--r--  1 root root    5529 Oct 28 09:00 2021-10-28-Loading_and_Saving_Data.ipynb\n",
            "-rw-r--r--  1 root root  119493 Oct 28 09:15 56.csv\n",
            "-rw-r--r--  1 root root 5246127 Aug 12 11:20 arpa-qualita-aria-2019-10_csv.csv\n",
            "drwxr-xr-x  4 root root    4096 Oct 26 13:33 .config\n",
            "drwxr-xr-x  1 root root    4096 Oct 26 13:34 sample_data\n",
            "drwxr-xr-x 13 1000 1000    4096 Oct  6 13:18 spark-3.2.0-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVzz6JOnM4wk"
      },
      "source": [
        "sc.textFile(\"arpa-qualita-aria-2019-10_csv.csv\").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9w4g1vh1e8j"
      },
      "source": [
        "###**Reading a CSV file into a DataFrame, filter some columns and save it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuKbsrvUVhse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceabb926-d292-4ae0-dac7-2c5c39550fb4"
      },
      "source": [
        "# Create DataFrame\n",
        "df = spark.read.csv(\"arpa-qualita-aria-2019-10_csv.csv\",sep=\";\",header=True,inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- stazione_id: integer (nullable = true)\n",
            " |-- periodo_media: string (nullable = true)\n",
            " |-- inquinante_id: integer (nullable = true)\n",
            " |-- misura_valore: double (nullable = true)\n",
            " |-- misura_dataora: timestamp (nullable = true)\n",
            " |-- misura_anno: integer (nullable = true)\n",
            "\n",
            "+-----------+-------------+-------------+-------------+-------------------+-----------+\n",
            "|stazione_id|periodo_media|inquinante_id|misura_valore|     misura_dataora|misura_anno|\n",
            "+-----------+-------------+-------------+-------------+-------------------+-----------+\n",
            "|    1908498|            h|           10|     0.152034|2019-01-01 00:00:00|       2019|\n",
            "|    1908498|            h|           10|     0.158291|2019-01-01 01:00:00|       2019|\n",
            "|    1908498|            h|           10|     0.159747|2019-01-01 02:00:00|       2019|\n",
            "|    1908498|            h|           10|     0.156639|2019-01-01 04:00:00|       2019|\n",
            "|    1908498|            h|           10|     0.140042|2019-01-01 05:00:00|       2019|\n",
            "+-----------+-------------+-------------+-------------+-------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ICHjQoP1hrz"
      },
      "source": [
        "Filter data by several columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXyJNFVWjux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef07e8db-ef15-4e46-c44b-44ce0ab7bc74"
      },
      "source": [
        "dataF=df.select(\"stazione_id\",\"misura_dataora\",\"misura_valore\")\n",
        "dataF.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------------+-------------+\n",
            "|stazione_id|     misura_dataora|misura_valore|\n",
            "+-----------+-------------------+-------------+\n",
            "|    1908498|2019-01-01 00:00:00|     0.152034|\n",
            "|    1908498|2019-01-01 01:00:00|     0.158291|\n",
            "|    1908498|2019-01-01 02:00:00|     0.159747|\n",
            "|    1908498|2019-01-01 04:00:00|     0.156639|\n",
            "|    1908498|2019-01-01 05:00:00|     0.140042|\n",
            "|    1908498|2019-01-01 06:00:00|     0.130085|\n",
            "|    1908498|2019-01-01 07:00:00|     0.131141|\n",
            "|    1908498|2019-01-01 08:00:00|     0.130212|\n",
            "|    1908498|2019-01-01 09:00:00|     0.176405|\n",
            "|    1908498|2019-01-01 10:00:00|     0.164526|\n",
            "|    1908498|2019-01-01 11:00:00|     0.168561|\n",
            "|    1908498|2019-01-01 12:00:00|     0.163831|\n",
            "|    1908498|2019-01-01 13:00:00|     0.206306|\n",
            "|    1908498|2019-01-01 14:00:00|      0.15871|\n",
            "|    1908498|2019-01-01 15:00:00|     0.129604|\n",
            "|    1908498|2019-01-01 16:00:00|     0.139725|\n",
            "|    1908498|2019-01-01 17:00:00|     0.159513|\n",
            "|    1908498|2019-01-01 18:00:00|     0.146317|\n",
            "|    1908498|2019-01-01 19:00:00|     0.146357|\n",
            "|    1908498|2019-01-01 20:00:00|     0.134128|\n",
            "+-----------+-------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39l8vOAA18x9"
      },
      "source": [
        "Save only the filtered Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxlQXKjg11mM"
      },
      "source": [
        "dataF.write.csv(\"dati_filtrati.csv\",mode='overwrite',header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa88XCDuOQNF"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNoeJWhqOeEk"
      },
      "source": [
        "sc.textFile(\"dati_filtrati.csv\").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1cg9iM93qCg"
      },
      "source": [
        "Let's read this new file back into an RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-iSzucQ3sup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e01938f-9a4d-48f6-91f8-79c2d242d2b9"
      },
      "source": [
        "data = spark.read.csv(\"dati_filtrati.csv\",header=True,inferSchema=True)\n",
        "data.printSchema()\n",
        "data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- stazione_id: integer (nullable = true)\n",
            " |-- misura_dataora: timestamp (nullable = true)\n",
            " |-- misura_valore: double (nullable = true)\n",
            "\n",
            "+-----------+-------------------+-------------+\n",
            "|stazione_id|     misura_dataora|misura_valore|\n",
            "+-----------+-------------------+-------------+\n",
            "|    1908498|2019-01-01 00:00:00|     0.152034|\n",
            "|    1908498|2019-01-01 01:00:00|     0.158291|\n",
            "|    1908498|2019-01-01 02:00:00|     0.159747|\n",
            "|    1908498|2019-01-01 04:00:00|     0.156639|\n",
            "|    1908498|2019-01-01 05:00:00|     0.140042|\n",
            "|    1908498|2019-01-01 06:00:00|     0.130085|\n",
            "|    1908498|2019-01-01 07:00:00|     0.131141|\n",
            "|    1908498|2019-01-01 08:00:00|     0.130212|\n",
            "|    1908498|2019-01-01 09:00:00|     0.176405|\n",
            "|    1908498|2019-01-01 10:00:00|     0.164526|\n",
            "|    1908498|2019-01-01 11:00:00|     0.168561|\n",
            "|    1908498|2019-01-01 12:00:00|     0.163831|\n",
            "|    1908498|2019-01-01 13:00:00|     0.206306|\n",
            "|    1908498|2019-01-01 14:00:00|      0.15871|\n",
            "|    1908498|2019-01-01 15:00:00|     0.129604|\n",
            "|    1908498|2019-01-01 16:00:00|     0.139725|\n",
            "|    1908498|2019-01-01 17:00:00|     0.159513|\n",
            "|    1908498|2019-01-01 18:00:00|     0.146317|\n",
            "|    1908498|2019-01-01 19:00:00|     0.146357|\n",
            "|    1908498|2019-01-01 20:00:00|     0.134128|\n",
            "+-----------+-------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvVuvyJvTG2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0b6ddc-b3b2-4e78-bb37-fda2a1044269"
      },
      "source": [
        "data.createOrReplaceTempView(\"table01\")\n",
        "spark.sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|         |  table01|       true|\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l9c1Jdp5O6c"
      },
      "source": [
        "# **Hive Example**\n",
        "\n",
        "Using Hive to create and read a table - Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0sp_TGJUCg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5b5ff0-a619-43ba-8628-779e8dd7b9ed"
      },
      "source": [
        "from pyspark.sql import HiveContext, Row\n",
        "\n",
        "hiveCtx = HiveContext(sc)\n",
        "ex1 = hiveCtx.read.csv(\"dati_filtrati.csv\",header=True,inferSchema=True)\n",
        "\n",
        "ex1.registerTempTable(\"Table02\")\n",
        "results = hiveCtx.sql(\"SELECT * FROM table02\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/context.py:604: FutureWarning: HiveContext is deprecated in Spark 2.0.0. Please use SparkSession.builder.enableHiveSupport().getOrCreate() instead.\n",
            "  FutureWarning\n",
            "/content/spark-3.2.0-bin-hadoop3.2/python/pyspark/sql/dataframe.py:140: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------------+-------------+\n",
            "|stazione_id|     misura_dataora|misura_valore|\n",
            "+-----------+-------------------+-------------+\n",
            "|    1908498|2019-01-01 00:00:00|     0.152034|\n",
            "|    1908498|2019-01-01 01:00:00|     0.158291|\n",
            "|    1908498|2019-01-01 02:00:00|     0.159747|\n",
            "|    1908498|2019-01-01 04:00:00|     0.156639|\n",
            "|    1908498|2019-01-01 05:00:00|     0.140042|\n",
            "|    1908498|2019-01-01 06:00:00|     0.130085|\n",
            "|    1908498|2019-01-01 07:00:00|     0.131141|\n",
            "|    1908498|2019-01-01 08:00:00|     0.130212|\n",
            "|    1908498|2019-01-01 09:00:00|     0.176405|\n",
            "|    1908498|2019-01-01 10:00:00|     0.164526|\n",
            "|    1908498|2019-01-01 11:00:00|     0.168561|\n",
            "|    1908498|2019-01-01 12:00:00|     0.163831|\n",
            "|    1908498|2019-01-01 13:00:00|     0.206306|\n",
            "|    1908498|2019-01-01 14:00:00|      0.15871|\n",
            "|    1908498|2019-01-01 15:00:00|     0.129604|\n",
            "|    1908498|2019-01-01 16:00:00|     0.139725|\n",
            "|    1908498|2019-01-01 17:00:00|     0.159513|\n",
            "|    1908498|2019-01-01 18:00:00|     0.146317|\n",
            "|    1908498|2019-01-01 19:00:00|     0.146357|\n",
            "|    1908498|2019-01-01 20:00:00|     0.134128|\n",
            "+-----------+-------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMN4Tj0HtFKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124a8d75-6eac-45db-8a05-bcabee720171"
      },
      "source": [
        "spark.sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|         |  table01|       true|\n",
            "|         |  table02|       true|\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_-qE8CwyFTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee70142-b201-402a-ce65-c9ea5f3eb8d7"
      },
      "source": [
        "sql = spark.sql\n",
        "\n",
        "sql(\"show tables\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|         |  table01|       true|\n",
            "|         |  table02|       true|\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TmSIqLotL0u"
      },
      "source": [
        "#Spark SQL\n",
        "\n",
        "(https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
        "\n",
        "**Reference**\n",
        "\n",
        "(https://spark.apache.org/docs/latest/sql-ref.html)"
      ]
    }
  ]
}