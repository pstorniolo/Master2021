{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-10-30-Spark_Examples_3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "KSFa9ihZUdzQ",
        "iG0QwcQPZ0cG",
        "WRkTSTKPaNip",
        "SrZ2tA7LaWeX",
        "Zq5EIkfMadRw",
        "-PDVIGpJl8Rd",
        "c45CPvp0mGcP",
        "TsVOo8_GmmbJ",
        "NirhwynzmwUP",
        "nvirtVR1m4tw",
        "KfaPwwUkp24L"
      ],
      "authorship_tag": "ABX9TyPcVohUZXhpfF80IXT1Sx7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstorniolo/Master2021/blob/main/2021_10_30_Spark_Examples_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f6H7MzVTOgK"
      },
      "source": [
        "# Install Spark 3.2.0 - JDK11\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!rm -f *.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "#Install findspark using pip to make pyspark importable as regular library\n",
        "!pip -q install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sql = spark.sql\n",
        "\n",
        "print(\"\\nApache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSFa9ihZUdzQ"
      },
      "source": [
        "#Column to Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MgoHBf0TgMt"
      },
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "\n",
        "data = [ (\"36636\",\"Finance\",3000,\"USA\"), \n",
        "    (\"40288\",\"Finance\",5000,\"IND\"), \n",
        "    (\"42114\",\"Sales\",3900,\"USA\"), \n",
        "    (\"39192\",\"Marketing\",2500,\"CAN\"), \n",
        "    (\"34534\",\"Sales\",6500,\"USA\") ]\n",
        "schema = StructType([\n",
        "     StructField('id', StringType(), True),\n",
        "     StructField('dept', StringType(), True),\n",
        "     StructField('salary', IntegerType(), True),\n",
        "     StructField('location', StringType(), True)\n",
        "     ])\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG0QwcQPZ0cG"
      },
      "source": [
        "##Convert columns to Map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVwszUFETliZ"
      },
      "source": [
        "from pyspark.sql.functions import col,lit,create_map\n",
        "df = df.withColumn(\"propertiesMap\",create_map(lit(\"salary\"),col(\"salary\"),lit(\"location\"),col(\"location\"))).drop(\"salary\",\"location\")\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2k45bsDaCMp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM5jNn_5Tpa3"
      },
      "source": [
        "dataDictionary = [\n",
        "        ('James',{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',{'hair':'brown','eye':None}),\n",
        "        ('Robert',{'hair':'red','eye':'black'}),\n",
        "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
        "        ('Jefferson',{'hair':'brown','eye':''})\n",
        "        ]\n",
        "\n",
        "df = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRkTSTKPaNip"
      },
      "source": [
        "##Map to columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkqMEp7xjI27"
      },
      "source": [
        "df3=df.rdd.map(lambda x: (x.name,x.properties[\"hair\"],x.properties[\"eye\"])).toDF([\"name\",\"hair\",\"eye\"])\n",
        "df3.printSchema()\n",
        "df3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBH3gW7gTseP"
      },
      "source": [
        "df.withColumn(\"hair\",df.properties.getItem(\"hair\")).withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
        "  .drop(\"properties\").show()\n",
        "\n",
        "df.withColumn(\"hair\",df.properties[\"hair\"]).withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
        "  .drop(\"properties\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNdZzTMjjpfm"
      },
      "source": [
        "*Functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfSiL-b7Txda"
      },
      "source": [
        "from pyspark.sql.functions import explode,map_keys,col\n",
        "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
        "keysList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
        "keyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\n",
        "df.select(df.name, *keyCols).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrZ2tA7LaWeX"
      },
      "source": [
        "#Dataframe Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH6oFC0v3Wv3"
      },
      "source": [
        "dataDictionary = [\n",
        "        ('James',{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',{'hair':'brown','eye':None}),\n",
        "        ('Robert',{'hair':'red','eye':'black'}),\n",
        "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
        "        ('Jefferson',{'hair':'brown','eye':''})\n",
        "        ]\n",
        "\n",
        "df = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq5EIkfMadRw"
      },
      "source": [
        "## Using StructType schema\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x6sZXsTVBMU"
      },
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, MapType,IntegerType\n",
        "schema = StructType([\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('properties', MapType(StringType(),StringType()),True)\n",
        "])\n",
        "df2 = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkSIJHTtlKSE"
      },
      "source": [
        "df3=df.rdd.map(lambda x: (x.name,x.properties[\"hair\"],x.properties[\"eye\"])).toDF([\"name\",\"hair\",\"eye\"])\n",
        "df3.printSchema()\n",
        "df3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ7LaJB6lMs0"
      },
      "source": [
        "df.withColumn(\"hair\",df.properties.getItem(\"hair\")).withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
        "  .drop(\"properties\").show()\n",
        "\n",
        "df.withColumn(\"hair\",df.properties[\"hair\"]).withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
        "  .drop(\"properties\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-ZMnEalfGm"
      },
      "source": [
        "*Functions*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU4VGzFBliIA"
      },
      "source": [
        "# Functions\n",
        "from pyspark.sql.functions import explode,map_keys,col\n",
        "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
        "keysList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
        "keyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\n",
        "df.select(df.name, *keyCols).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PDVIGpJl8Rd"
      },
      "source": [
        "#Current Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIpCfUGIl_UA"
      },
      "source": [
        "data=[[\"1\"]]\n",
        "df=spark.createDataFrame(data,[\"id\"])\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c45CPvp0mGcP"
      },
      "source": [
        "##current_date() & current_timestamp()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iqtwWNPmHDi"
      },
      "source": [
        "df.withColumn(\"current_date\",current_date()).withColumn(\"current_timestamp\",current_timestamp()).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsVOo8_GmmbJ"
      },
      "source": [
        "###SQL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dzAozv7mNch"
      },
      "source": [
        "sql(\"select current_date(), current_timestamp()\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NirhwynzmwUP"
      },
      "source": [
        "## Date & Timestamp into custom format\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG4CAbHkmuqe"
      },
      "source": [
        "df.withColumn(\"date_format\",date_format(current_date(),\"MM-dd-yyyy\")) \\\n",
        "  .withColumn(\"to_timestamp\",to_timestamp(current_timestamp(),\"MM-dd-yyyy HH mm ss SSS\")) \\\n",
        "  .show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvirtVR1m4tw"
      },
      "source": [
        "###SQL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lPxTh3Lm0oz"
      },
      "source": [
        "sql(\"select date_format(current_date(),'MM-dd-yyyy') as date_format ,to_timestamp(current_timestamp(),'MM-dd-yyyy HH mm ss SSS') as to_timestamp\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imRLN9gioKKf"
      },
      "source": [
        "#Dataframe repatition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8YUo3oRoOMe"
      },
      "source": [
        "df=spark.range(0,20)\n",
        "print(df.rdd.getNumPartitions())\n",
        "\n",
        "df.write.mode(\"overwrite\").csv(\"partition.csv\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkzDbdIXoSlU"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfaPwwUkp24L"
      },
      "source": [
        "##repartition()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UDJ72T3sop4"
      },
      "source": [
        "Il metodo Spark RDD **repartition()** viene utilizzato per aumentare o diminuire le partizioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYnWXrBooVTd"
      },
      "source": [
        "df2 = df.repartition(6)\n",
        "print(df2.rdd.getNumPartitions())\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHtuvbzp6iQ"
      },
      "source": [
        "##coalesce()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szVAO4RMr8kO"
      },
      "source": [
        "Spark RDD **coalesce()** viene utilizzato solo per ridurre il numero di partizioni."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oMWVitCoYqj"
      },
      "source": [
        "df3 = df.coalesce(2)\n",
        "print(df3.rdd.getNumPartitions())\n",
        "df3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m7eIVVxqEU0"
      },
      "source": [
        "##groupBy()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9yXUUC2ocLk"
      },
      "source": [
        "df4 = df.groupBy(\"id\").count()\n",
        "print(df4.rdd.getNumPartitions())\n",
        "df4.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}